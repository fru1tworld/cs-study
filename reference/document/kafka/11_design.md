# Kafka 설계

> 이 문서는 Apache Kafka 공식 문서의 Design 섹션을 한국어로 번역한 것입니다.
> 원본: https://kafka.apache.org/documentation/#design

---

## 목차

1. [동기(Motivation)](#동기motivation)
2. [영속성(Persistence)](#영속성persistence)
3. [효율성(Efficiency)](#효율성efficiency)
4. [프로듀서(The Producer)](#프로듀서the-producer)
5. [컨슈머(The Consumer)](#컨슈머the-consumer)
6. [메시지 전달 시맨틱스(Message Delivery Semantics)](#메시지-전달-시맨틱스message-delivery-semantics)
7. [트랜잭션 사용하기(Using Transactions)](#트랜잭션-사용하기using-transactions)
8. [공유 그룹(Share Groups)](#공유-그룹share-groups)
9. [복제(Replication)](#복제replication)
10. [로그 압축(Log Compaction)](#로그-압축log-compaction)
11. [쿼터(Quotas)](#쿼터quotas)

---

## 동기(Motivation)

Kafka는 대규모 실시간 데이터 피드를 처리하기 위한 통합 플랫폼으로 설계되었습니다. 시스템은 로그 집계와 같은 높은 처리량의 이벤트 스트림을 지원하고, 오프라인 시스템에서 발생하는 대규모 데이터 백로그를 관리하며, 저지연 메시징을 처리하고, 내결함성을 보장하는 파티션된 분산 처리를 가능하게 해야 했습니다.

---

## 영속성(Persistence)

### 파일시스템을 두려워하지 마세요

Kafka는 파일시스템 스토리지와 캐싱에 크게 의존합니다. "디스크는 느리다"는 것이 일반적인 인식이지만, 순차적 디스크 성능은 랜덤 액세스보다 훨씬 뛰어납니다. 최신 하드 드라이브는 순차 쓰기(linear writes)에서 약 600MB/sec를 달성하는 반면, 랜덤 쓰기(random writes)는 약 100k/sec에 불과합니다. 이는 6000배의 차이입니다.

운영체제는 대용량 블록으로 데이터를 미리 읽고(read-ahead) 작은 쓰기를 더 큰 물리적 쓰기로 그룹화하는(write-behind) 최적화를 제공합니다. 최신 운영체제는 여유 메모리를 디스크 캐싱에 적극적으로 사용하며, "순차적 디스크 액세스는 경우에 따라 랜덤 메모리 액세스보다 빠를 수 있습니다."

Java 애플리케이션은 메모리 오버헤드 문제에 직면합니다. 객체 오버헤드가 데이터 크기를 두 배로 늘리는 경우가 많고, 가비지 컬렉션은 대용량 힙에서 문제가 됩니다. 파일시스템과 OS 페이지캐시를 활용함으로써 Kafka는 다음을 달성합니다:

- 사용 가능한 캐시 용량이 자동으로 두 배로 증가
- 서비스 재시작 후에도 캐시가 유지됨
- 캐시 일관성을 위한 코드 단순화
- 32GB 머신에서 GC 패널티 없이 캐시 크기가 28-30GB에 도달

모든 데이터 쓰기는 반드시 디스크로 플러시하지 않고도 즉시 영구 로그에 기록됩니다. 커널의 페이지캐시가 이 전송을 처리합니다.

### 상수 시간으로 충분합니다

전통적인 메시징 시스템은 메타데이터를 위해 BTree 또는 유사한 구조를 가진 컨슈머별 큐를 사용하여 O(log N) 연산이 필요합니다. 디스크 탐색(seek)은 각각 약 10밀리초가 소요되며, 고정 캐시에서 데이터가 증가함에 따라 성능 저하가 초선형적으로 발생합니다.

Kafka는 영구 큐에 단순한 파일 추가(file appends)를 사용하여 O(1) 연산을 제공하며, 읽기가 쓰기를 차단하지 않습니다. 이 접근 방식은 데이터 크기와 성능을 분리하여 단일 서버가 탐색 성능이 좋지 않음에도 불구하고 저렴하고 대용량인 SATA 드라이브를 활용할 수 있게 합니다.

이 설계는 성능 저하 없이 관대한 메시지 보존(분 단위가 아닌 주 단위)을 가능하게 하여, 컨슈머에게 재생(replay) 및 재처리(reprocessing)의 유연성을 제공합니다.

---

## 효율성(Efficiency)

효율성은 멀티테넌트 운영에 직접적인 영향을 미칩니다. 인프라가 애플리케이션이 용량 한계에 도달하기 전에 병목 현상이 되어서는 안 됩니다.

### 배칭(Batching)

프로토콜은 메시지를 함께 그룹화하는 "메시지 세트" 추상화를 사용합니다. 배칭은 다음을 달성합니다:

- 네트워크 라운드트립 오버헤드 분산
- 더 큰 순차적 디스크 연산
- 연속적인 메모리 블록
- 수 배에서 수십 배의 속도 향상

버스트성 랜덤 메시지 쓰기가 컨슈머를 향한 선형 흐름으로 변환됩니다.

### 제로카피 최적화(Zero-Copy Optimization)

Kafka는 프로듀서, 브로커, 컨슈머 전체에서 표준화된 바이너리 메시지 포맷을 사용하여 중간 변환을 제거합니다. 브로커의 메시지 로그는 프로듀서/컨슈머 포맷의 메시지 세트로 채워진 파일들로 구성됩니다.

최신 Unix 시스템은 페이지캐시에서 소켓으로 데이터를 전송하기 위한 최적화된 코드 경로를 제공합니다. Linux의 `sendfile` 시스템 콜은 기존의 4회 복사, 2회 시스템 콜 경로를 피합니다:

전통적인 경로: OS가 디스크 데이터 읽기 -> 애플리케이션이 커널 데이터 읽기 -> 애플리케이션이 소켓 버퍼에 쓰기 -> OS가 NIC 버퍼로 복사

`sendfile`을 사용하면 OS가 페이지캐시에서 네트워크로 직접 데이터를 전송하여 NIC 버퍼로의 마지막 복사만 필요합니다.

컨슈머가 최신 상태를 유지하는 클러스터에서는 디스크가 전적으로 캐시에서 데이터를 제공하며 읽기 활동이 전혀 없습니다.

참고: TLS/SSL 라이브러리는 사용자 공간에서 작동하며, 커널 수준의 SSL_sendfile은 현재 Kafka에서 지원되지 않으므로 SSL이 활성화된 경우 `sendfile`이 사용되지 않습니다.

### 엔드투엔드 배치 압축(End-to-End Batch Compression)

네트워크 대역폭은 데이터 파이프라인, 특히 광역 네트워크에서 병목 현상이 될 수 있습니다. 메시지 수준 압축은 메시지 간에 중복성이 존재하기 때문에(필드 이름, 사용자 에이전트, 공통 문자열) 압축 비율이 좋지 않습니다.

Kafka는 메시지를 배치로 묶고, 그룹을 압축한 다음, 압축된 배치를 서버로 전송합니다. 브로커는 검증을 위해 배치를 압축 해제한 후 압축된 데이터를 디스크에 기록합니다. 배치는 로그와 컨슈머에게 전송되는 동안 압축된 상태로 유지되며, 컨슈머가 수신된 데이터를 압축 해제합니다.

지원되는 압축 프로토콜: GZIP, Snappy, LZ4, ZStandard

---

## 프로듀서(The Producer)

### 로드 밸런싱(Load Balancing)

프로듀서는 중간 라우팅 없이 파티션을 리드하는 브로커에 직접 데이터를 전송합니다. 모든 Kafka 노드는 살아있는 서버와 파티션 리더십에 대한 메타데이터 요청에 응답하여 적절한 요청 방향을 설정할 수 있게 합니다.

클라이언트는 랜덤 로드 밸런싱 또는 시맨틱 파티셔닝 함수를 통해 파티션 발행을 제어합니다. 사용자는 파티션으로 해싱하기 위한 파티션 키를 지정하여 특정 키의 모든 데이터가 동일한 파티션에 도달하도록 보장합니다. 이를 통해 컨슈머의 지역성 민감(locality-sensitive) 처리가 가능합니다.

### 비동기 전송(Asynchronous Send)

배칭이 효율성을 높입니다. 프로듀서는 메모리에 데이터를 축적하고 단일 요청으로 더 큰 배치를 전송합니다. 설정은 고정된 메시지 수 또는 지연 시간 한계(예: 64k 또는 10밀리초)를 제한합니다. 이는 처리량 향상을 위해 약간의 지연 시간 증가를 교환합니다.

---

## 컨슈머(The Consumer)

Kafka 컨슈머는 파티션을 리드하는 브로커에 fetch 요청을 발행하고 로그 오프셋을 지정합니다. 브로커는 해당 위치에서 로그 청크를 반환하여 컨슈머에게 소비 위치에 대한 상당한 제어권을 부여하고 재처리를 위한 데이터 되감기를 가능하게 합니다.

### Push vs. Pull

Kafka는 프로듀서가 브로커에 푸시하고 컨슈머가 브로커에서 풀하는 전통적인 풀 모델을 따릅니다. 푸시 기반 시스템은 브로커가 전송 속도를 제어하기 때문에 다양한 컨슈머와 함께 사용하기 어렵습니다. 소비가 생산 속도보다 느려지면 컨슈머가 압도당하게 됩니다. 이는 본질적으로 서비스 거부 공격입니다.

풀 기반 시스템은 컨슈머가 뒤처지더라도 가능할 때 따라잡을 수 있게 합니다. 또한 컨슈머가 항상 현재 위치 이후의 모든 사용 가능한 메시지를 가져오므로(설정 가능한 제한까지) 적극적인 데이터 배칭을 가능하게 합니다.

순진한 풀 시스템은 브로커에 데이터가 없을 때 타이트 루프 폴링의 위험이 있습니다. Kafka는 데이터가 도착하거나 설정 가능한 바이트 임계값이 충족될 때까지 컨슈머 요청이 "롱 폴(long poll)"에서 차단되도록 허용하는 풀 파라미터를 통해 이를 방지합니다.

### 컨슈머 위치(Consumer Position)

소비 추적은 핵심 성능 포인트입니다. 대부분의 시스템은 소비 메타데이터를 브로커에 즉시 또는 컨슈머 확인 시 저장합니다. 그러나 소비 상태에 대한 브로커-컨슈머 간 합의는 간단하지 않습니다.

즉각적인 소비 기록은 컨슈머가 전달 후 실패할 경우 처리되지 않은 메시지가 건너뛰어질 위험이 있습니다. 확인 기반 접근 방식은 손실을 방지하지만 복잡성을 만듭니다: 컨슈머가 메시지를 처리했지만 확인 전에 충돌하면 복구 시 중복 소비가 발생합니다.

Kafka의 접근 방식은 근본적으로 다릅니다: 각 파티션은 구독하는 컨슈머 그룹당 정확히 하나의 컨슈머에게 할당됩니다. 컨슈머 위치는 파티션당 단일 정수(다음 메시지 오프셋)가 됩니다. 이는 소비 상태 크기를 최소화하고 주기적인 체크포인팅만 필요합니다.

컨슈머는 재생을 위해 의도적으로 이전 오프셋으로 되감을 수 있으며, 이는 전통적인 큐 계약을 위반하지만 버그 수정 후 필수적인 재처리를 가능하게 합니다.

### 오프라인 데이터 로드(Offline Data Load)

확장 가능한 영속성은 Hadoop이나 데이터 웨어하우스로의 배치 데이터 로드와 같은 주기적 컨슈머를 가능하게 합니다. Hadoop 병렬화는 노드/토픽/파티션 조합당 맵 태스크로 로드를 분할하여 완전한 병렬성을 제공합니다. 실패한 태스크는 중복 위험 없이 원래 위치에서 재시작됩니다.

### 정적 멤버십(Static Membership)

리밸런스 프로토콜은 그룹 코디네이터가 멤버에게 임시 엔티티 ID를 할당하는 것에 의존합니다. 멤버가 재시작하고 다시 참여하면 ID가 변경되어 리밸런싱을 트리거합니다. 상태 저장 애플리케이션의 경우 이로 인해 태스크 할당이 셔플되어 긴 상태 복구 기간과 부분적 비가용성이 발생합니다.

정적 멤버십은 멤버가 영구적인 엔티티 ID를 제공할 수 있게 합니다. 이러한 ID를 기반으로 한 멤버십 안정성은 불필요한 리밸런싱을 방지합니다. 구현에는 다음이 필요합니다:

- 브로커 및 클라이언트를 버전 2.3 이상으로 업그레이드
- 브로커 `inter.broker.protocol.version`을 2.3 이상으로 설정
- 컨슈머 인스턴스당 고유한 `ConsumerConfig#GROUP_INSTANCE_ID_CONFIG` 설정
- Kafka Streams의 경우: KafkaStreams 인스턴스당 하나의 고유 ID

이전 버전의 브로커는 클라이언트가 `GROUP_INSTANCE_ID_CONFIG`를 설정하면 `UnsupportedException`을 발생시킵니다. 중복 ID는 `FencedInstanceIdException`을 트리거하여 즉각적인 종료를 강제합니다.

---

## 메시지 전달 시맨틱스(Message Delivery Semantics)

Kafka는 간단한 시맨틱스를 제공합니다:

- 최대 한 번(At most once): 메시지가 손실될 수 있지만 재전달되지 않음
- 최소 한 번(At least once): 메시지가 손실되지 않지만 재전달될 수 있음
- 정확히 한 번(Exactly once): 각 메시지가 정확히 한 번만 처리됨

이는 발행 및 소비 보장 문제로 나뉩니다. "정확히 한 번"을 주장하는 시스템은 실제 동작이 실패 시나리오, 다중 컨슈머 프로세스, 디스크 손실 가능성에 따라 달라지므로 주의 깊게 읽어야 합니다.

### Kafka의 보장

메시지는 모든 동기화 복제본(ISR) 멤버가 로그에 적용할 때 "커밋"됩니다. 커밋된 메시지는 하나의 복제 브로커가 살아있는 한 손실되지 않습니다.

0.11.0.0 이전에는 실패한 프로듀서가 네트워크 오류가 커밋 전에 발생했는지 후에 발생했는지 확인할 수 없었습니다. 이는 자동 생성 키 데이터베이스 삽입과 유사합니다. 프로듀서는 재전송만 할 수 있었고, 이는 중복 가능성이 있는 최소 한 번 시맨틱스를 제공했습니다.

0.11.0.0 이후, 멱등성(idempotent) 프로듀서 전달은 재전송이 중복을 생성하지 않음을 보장합니다. 브로커는 프로듀서 ID를 할당하고 시퀀스 번호를 사용하여 중복을 제거합니다. 프로듀서는 또한 원자적 다중 파티션 트랜잭션을 지원합니다. 모든 메시지가 성공하거나 아무것도 성공하지 않습니다.

### 프로듀서 옵션

모든 사용 사례가 강력한 보장을 필요로 하지는 않습니다. 지연 시간에 민감한 프로듀서는 내구성 수준을 지정할 수 있습니다: 커밋 대기(10밀리초), 완전 비동기 전송, 또는 리더 수신만 대기(팔로워는 불필요).

### 컨슈머 시맨틱스

동일한 로그가 동일한 오프셋과 메시지 순서로 모든 복제본에 존재합니다. 컨슈머는 로그 위치를 제어합니다. 메모리 내 위치 저장은 충돌 방지 시나리오에서 작동하지만, 실패 복구에는 대체 접근 방식이 필요합니다.

컨슈머는 처리 시 다음 옵션을 직면합니다:

1. 읽기 -> 위치 저장 -> 처리: 위치 저장 후 처리 전에 충돌하면 처리되지 않은 메시지가 건너뛰어짐 (최대 한 번)
2. 읽기 -> 처리 -> 위치 저장: 처리 후 저장 전에 충돌하면 복구 시 재처리됨 (최소 한 번)

멱등성 업데이트(기본 키)는 최소 한 번을 실용적으로 만듭니다. 중복 메시지가 동일한 데이터로 덮어씁니다.

### 정확히 한 번 처리(Exactly-Once Processing)

Kafka 토픽을 소비하고 다른 토픽으로 생성할 때(Kafka Streams 애플리케이션), 0.11.0.0 이후의 트랜잭션 프로듀서는 정확히 한 번 시맨틱스를 가능하게 합니다. 컨슈머 위치는 내부 토픽에 저장되어 단일 트랜잭션에서 오프셋 쓰기와 처리된 데이터 생성을 허용합니다.

중단된 트랜잭션은 컨슈머 위치를 되돌립니다(컨슈머가 자동으로 되감기보다 커밋된 오프셋을 다시 가져와야 함). 처리된 데이터는 "read_committed" 격리 수준을 가진 다른 컨슈머에게 보이지 않습니다(기본 "read_uncommitted"는 중단된 트랜잭션 데이터를 포함한 모든 메시지를 표시).

외부 시스템 쓰기는 대부분의 시스템이 2단계 커밋을 지원하지 않으므로 다른 접근 방식이 필요합니다. 솔루션에는 대상 시스템에 출력과 함께 오프셋을 저장하여 원자적 출력-오프셋 업데이트를 보장하는 것이 포함됩니다. Kafka Connect 커넥터가 이 패턴을 예시합니다.

Kafka는 Kafka Streams에서 정확히 한 번을 지원하고 일반 사용을 위해 트랜잭션 프로듀서와 read-committed 컨슈머 격리를 통해 이를 가능하게 합니다. 외부 시스템의 정확히 한 번은 일반적으로 시스템 협력이 필요합니다. 그렇지 않으면 Kafka는 기본적으로 최소 한 번을 보장하며, 비활성화된 프로듀서 재시도와 처리 전 오프셋 커밋을 통해 최대 한 번을 허용합니다.

---

## 트랜잭션 사용하기(Using Transactions)

Kafka 트랜잭션은 다른 메시징 시스템과 다릅니다. 컨슈머와 프로듀서가 분리되어 있으며 프로듀서만 트랜잭션을 수행합니다. 하지만 프로듀서는 전체적인 정확히 한 번 동작을 위해 트랜잭션 컨슈머 위치 업데이트를 만들 수 있습니다.

### 세 가지 핵심 측면

1. 컨슈머는 그룹당 각 파티션의 단독 처리를 보장하는 파티션 할당을 사용함
2. 프로듀서는 모든 레코드 생성과 오프셋 업데이트를 원자적으로 만드는 트랜잭션을 사용함
3. 컨슈머 인스턴스당 하나의 프로듀서 인스턴스가 리밸런싱과 함께 트랜잭션을 처리함

read-committed 격리 수준 사용이 권장됩니다(엄격하게 필수는 아니지만).

### 설정

컨슈머: `isolation.level=read_committed` 및 `enable.auto.commit=false`

프로듀서: `transactional.id`를 트랜잭션 ID 이름으로 설정

### 예외 처리

예외 카테고리는 이제 다음을 포함합니다:

- RetriableException: 자동 클라이언트 재시도가 있는 임시 예외, 애플리케이션에 도달하지 않음
- RefreshRetriableException: 재시도 전 메타데이터 새로 고침 필요, 내부적으로 처리됨
- AbortableException: 트랜잭션 중단 필요, 처리를 위해 애플리케이션에 버블업
- ApplicationRecoverableException: 사용자 정의 복구가 필요한 애플리케이션 처리 예외
- InvalidConfigurationException: 애플리케이션 처리가 필요한 설정 문제
- KafkaException: 애플리케이션 처리를 위한 일반 Kafka 예외

간단한 예외 처리: Kafka 객체를 폐기하고 다시 생성하여 새로 시작합니다. 컨슈머 그룹 리밸런싱은 마지막으로 커밋된 오프셋을 가져와 중단 전 상태로 효과적으로 되감습니다.

정교한 애플리케이션은 자동 되감기에 의존하는 대신 `KafkaConsumer.seek`를 사용하여 수동으로 되감을 수 있습니다.

---

## 공유 그룹(Share Groups)

공유 그룹은 Apache Kafka 4.1의 협력적 레코드 소비를 위한 프리뷰 기능입니다. 전통적인 컨슈머 그룹과 달리 공유 그룹은 다음을 가능하게 합니다:

- 여러 컨슈머가 레코드를 협력적으로 소비
- 파티션을 여러 컨슈머에게 할당
- 컨슈머 수가 파티션 수를 초과할 수 있음
- 개별 레코드 확인
- 처리할 수 없는 레코드에 대한 전달 시도 횟수 계산

### 메커니즘

동일한 토픽을 구독한 모든 공유 그룹 컨슈머는 해당 레코드를 협력적으로 소비합니다. 토픽에 액세스하는 여러 공유 그룹은 독립적으로 소비합니다.

컨슈머는 구독된 토픽을 동적으로 설정합니다. 페칭 시 컨슈머는 일치하는 토픽-파티션에서 레코드를 수신합니다. 레코드는 시간 제한된 획득 잠금(기본 30초, `share.record.lock.duration.ms`로 제어)을 획득합니다. 잠긴 레코드는 다른 컨슈머에게 사용할 수 없게 됩니다.

잠금을 보유한 컨슈머는 다음을 할 수 있습니다:
- 성공적인 처리 확인
- 다른 전달 시도를 위해 레코드 해제
- 처리할 수 없는 레코드 거부
- 잠금 기간 만료 시 자동 해제 허용

브로커는 `group.share.partition.max.record.locks`를 통해 공유 그룹당 토픽-파티션당 획득된 레코드를 제한합니다. 이 제한에 도달하면 획득된 레코드가 타임아웃을 통해 감소할 때까지 페칭이 일시적으로 중단됩니다.

---

## 복제(Replication)

Kafka는 설정 가능한 서버 수에 걸쳐 토픽 파티션 로그를 복제하며 토픽별 복제 팩터를 가집니다. 이를 통해 서버 장애 시 복제본으로의 자동 페일오버가 가능하면서 메시지 가용성을 유지합니다.

복제를 선택 사항으로 취급하는 일부 시스템과 달리 Kafka는 복제를 기본값으로 구현합니다. 단일 복제본 토픽도 복제 팩터가 1인 복제된 토픽입니다.

복제 단위는 토픽 파티션입니다. 정상 조건에서 각 파티션은 하나의 리더와 0개 이상의 팔로워를 가집니다. 리더를 포함한 모든 복제본이 복제 팩터를 구성합니다. 모든 쓰기는 파티션 리더로 이동하고, 읽기는 리더 또는 팔로워로 이동합니다. 파티션은 일반적으로 브로커보다 많으며 리더는 고르게 분산됩니다. 팔로워 로그는 리더 로그와 동일합니다. 동일한 오프셋, 메시지, 순서를 가집니다(리더가 일시적으로 복제되지 않은 메시지를 가질 수 있지만).

팔로워는 일반 Kafka 컨슈머처럼 리더로부터 소비하며 자연스럽게 적용된 로그 항목을 배치로 처리합니다.

### 노드 활성 상태(Node Liveness)

분산 시스템 장애 처리에는 정확한 "활성" 정의가 필요합니다. 특별한 "컨트롤러" 노드가 브로커 등록 및 장애 감지를 관리합니다. 브로커 활성 상태에는 다음이 필요합니다:

1. 정기적인 메타데이터 업데이트를 수신하는 컨트롤러와의 활성 세션 유지
2. 팔로워가 "너무 뒤처지지" 않고 리더 쓰기를 복제

활성 세션은 클러스터 설정에 따라 달라집니다. KRaft 클러스터는 주기적인 컨트롤러 하트비트를 통해 세션을 유지합니다. 세션 타임아웃은 `broker.session.timeout.ms`로 설정됩니다. 타임아웃 전에 하트비트가 누락되면 노드가 오프라인으로 표시됩니다.

두 조건을 모두 만족하는 노드는 "동기화 중(in sync)"이며 리더가 동기화 복제본(ISR) 세트로 추적합니다. ISR 세트는 클러스터 메타데이터에 유지되며 변경 시 업데이트됩니다. 조건 실패는 ISR에서 브로커를 제거합니다. 죽은 팔로워는 손실된 세션을 통해 종료되고, 지연된 팔로워는 `replica.lag.time.max.ms`로 결정되는 복제 지연을 통해 종료됩니다.

Kafka는 노드가 작동을 멈췄다가 나중에 복구되는 "fail/recover" 장애 모델을 처리하며, 임의적이거나 악의적인 응답을 생성하는 "비잔틴(Byzantine)" 장애는 처리하지 않습니다.

커밋된 메시지만 컨슈머에게 전달되어 리더 장애 시 손실 우려를 방지합니다. 프로듀서는 `acks` 설정을 통해 커밋 대기 여부를 선택하여 지연과 내구성을 교환합니다. 토픽은 프로듀서가 전체 ISR 확인을 요청할 때 확인되는 최소 동기화 복제본(`min.insync.replicas`)을 설정합니다. 덜 엄격한 확인(acks=0)은 ISR 간에 비동기적으로 커밋하고, acks=1은 리더에서만 동기화합니다.

`acks` 설정에 관계없이 컨슈머 가시성에는 다음이 필요합니다:

1. 모든 동기화 복제본에 복제
2. 동기화 복제본 수가 `min.insync.replicas` 설정을 초과

Kafka의 보장: 커밋된 메시지는 최소 하나의 살아있는 동기화 복제본이 있는 한 손실되지 않습니다.

노드 장애 시 짧은 페일오버 후 가용성이 계속되지만 네트워크 파티션에서는 그렇지 않을 수 있습니다.

### 복제된 로그: 쿼럼, ISR, 상태 머신

Kafka 파티션은 근본적으로 복제된 로그입니다. 이는 분산 데이터 시스템의 가장 기본적인 프리미티브 중 하나입니다. 복제된 로그는 순서화된 값 시퀀스에 대한 합의를 모델링합니다.

가장 간단한 구현은 리더가 선택한 순서를 사용합니다. 리더가 살아있는 한 팔로워는 값과 순서만 복사하면 됩니다.

리더 실패 시 팔로워 중에서 새 리더를 선출해야 합니다. 그러나 팔로워는 지연되거나 충돌할 수 있으므로 최신 팔로워를 선택해야 합니다. 로그 복제 알고리즘은 다음을 보장해야 합니다: 클라이언트가 메시지가 커밋되었음을 알게 되고 리더가 실패하면 새로 선출된 리더는 해당 메시지를 가지고 있어야 합니다. 이는 트레이드오프를 만듭니다: 리더가 커밋 전에 더 많은 팔로워 확인을 기다릴수록 잠재적으로 선출 가능한 리더가 더 많아집니다.

쿼럼 접근 방식은 커밋에 필요한 확인 수와 리더 선출 로그 비교 수 사이에 보장된 중복을 만듭니다.

일반적인 접근 방식은 커밋과 선출 모두에 과반수 투표를 사용합니다: 2f+1 복제본에서 커밋 전에 f+1 복제본이 메시지를 수신하도록 요구하고 가장 완전한 로그를 가진 f+1 복제본에서 새 리더를 선출하면 f개 이하의 장애 시 리더가 커밋된 메시지를 손실하지 않음을 보장합니다. f+1 복제본 중 적어도 하나는 모든 커밋된 메시지를 포함하고, 해당 복제본의 가장 완전한 로그가 새 리더가 됩니다. 많은 알고리즘이 존재합니다(ZooKeeper의 Zab, Raft, Viewstamped Replication, Microsoft의 PacificA). 이 개요는 세부 사항을 단순화합니다.

과반수 투표의 장점: 지연 시간은 가장 빠른 서버에만 의존합니다. 3개의 복제본에서 지연 시간은 느린 팔로워가 아닌 빠른 팔로워를 따릅니다.

과반수 투표의 단점: 장애가 빠르게 선출 가능한 리더를 제거합니다. 하나의 장애 허용에는 3개의 데이터 복사본이 필요하고, 두 개의 장애 허용에는 5개의 복사본이 필요합니다. 단일 장애 중복은 실용적인 시스템에 충분하지 않고, 5개 복사본 오버헤드는 대용량 데이터에 실용적이지 않습니다.

### Kafka의 ISR 접근 방식

Kafka는 과반수 투표 대신 동적 동기화 복제본(ISR) 세트를 사용합니다. ISR 멤버만 리더십에 적합합니다. 파티션 쓰기는 모든 ISR 멤버가 쓰기를 수신할 때만 커밋됩니다. ISR 세트는 변경 사항과 함께 클러스터 메타데이터에 유지됩니다.

f+1 복제본과 ISR 모델에서 Kafka는 커밋된 메시지 손실 없이 f개의 장애를 허용합니다.

대부분의 사용 사례에서 이 트레이드오프는 합리적입니다. 실제로 두 접근 방식 모두 장애 허용을 위해 동일한 복제본 확인 수를 기다립니다(과반수 쿼럼은 3개의 복제본과 1개의 확인이 필요하고, ISR은 2개와 1개가 필요). 과반수 투표의 장점(가장 느린 서버 없이 커밋)은 클라이언트가 커밋 차단 여부를 선택할 수 있게 하여 완화됩니다. 낮은 복제 팩터 요구 사항은 처리량과 디스크 공간 이점을 제공합니다.

중요한 구분: Kafka는 충돌한 노드가 데이터가 온전한 상태로 복구될 것을 요구하지 않습니다. 복제 알고리즘은 일반적으로 일관성 위반 없이 장애-복구 시나리오를 통해 결코 손실되지 않는 "안정적인 저장소"에 의존합니다. 두 가지 문제가 존재합니다: 디스크 오류(가장 흔한 운영 문제)는 종종 데이터를 온전하게 두지 않으며, 모든 쓰기에 fsync(일관성 보장을 가능하게 함)는 성능을 2-3배 감소시킵니다. Kafka의 복제본 재참여 프로토콜은 플러시되지 않은 데이터가 손실된 충돌 후에도 완전한 재동기화를 요구합니다.

### 비정상 리더 선출(Unclean Leader Election)

Kafka의 데이터 손실 보장은 최소 하나의 동기화 복제본이 남아있음을 가정합니다. 모든 복제 노드가 죽으면 이 보장이 제거됩니다.

실용적인 시스템은 모든 복제본 죽음을 처리해야 합니다. 두 가지 가능한 구현:

1. ISR 복제본이 살아나기를 기다리고 부활을 리더로 선택(데이터가 살아있기를 희망)
2. 첫 번째 부활하는 복제본(반드시 ISR이 아닌)을 리더로 선택

이는 가용성과 일관성을 교환합니다. ISR 복제본을 기다리면 해당 복제본이 돌아올 때까지 비가용성을 의미합니다. 파괴되거나 데이터가 손실된 복제본은 영구적인 다운타임을 의미합니다. 비ISR 복제본 리더십은 커밋된 메시지 보장이 없음에도 불구하고 해당 로그가 진실이 됨을 의미합니다.

0.11.0.0 이후 Kafka는 일관성을 선호하여 일관된 복제본을 기다립니다. `unclean.leader.election.enable` 설정 속성은 가용성을 선호하는 사용 사례를 지원합니다.

이 딜레마는 Kafka에만 국한되지 않습니다. 모든 쿼럼 방식이 이에 직면합니다. 과반수 서버 영구 장애 시 과반수 투표는 100% 데이터 손실 또는 일관성 위반 중 선택을 강제합니다.

### 가용성과 내구성 보장

프로듀서는 확인을 선택합니다: 0, 1 또는 모든(-1) 복제본. "모든 복제본" 확인은 할당된 전체 복제본 수신을 보장하지 않습니다. acks=all에서 확인은 현재 동기화 복제본이 메시지를 수신할 때 발생합니다. 하나의 실패한 복제본이 있는 2개 복제본 토픽에서 acks=all이 성공합니다. 나머지 복제본도 실패하면 이러한 쓰기가 손실될 수 있습니다.

이는 최대 가용성을 보장하지만 내구성 선호를 희생할 수 있습니다. 두 가지 토픽 설정이 내구성을 선호합니다:

1. 비정상 리더 선출 비활성화: 비가용 파티션은 최근 리더가 돌아올 때까지 비가용 상태로 유지되어 손실보다 비가용성을 선호
2. 최소 ISR 크기 지정: 파티션은 ISR이 최소값을 초과할 때만 쓰기를 수락하여 단일 복제본 손실을 방지. acks=all에서만 작동하며 최소 동기화 복제본 확인을 보장. 높은 최소값은 일관성을 향상시키고(더 많은 복제본이 더 낮은 손실 확률을 보장) 가용성을 감소시킴(더 적은 동기화 복제본이 쓰기 비가용성을 유발)

### 복제본 관리

복제된 로그 논의는 단일 로그/파티션을 다룹니다. Kafka 클러스터는 수백 또는 수천 개를 관리합니다. 파티션은 클러스터 전체에 라운드 로빈으로 균형을 맞추어 적은 노드에 대용량 토픽이 클러스터링되는 것을 방지합니다. 리더십도 균형을 맞추어 노드당 비례적인 리더 분배를 보장합니다.

리더 선출 효율성은 비가용성 윈도우로서 매우 중요합니다. 순진한 구현은 실패한 노드의 호스팅된 파티션에 대해 파티션별 선출을 실행합니다. "컨트롤러" 노드는 브로커 등록 및 장애 감지를 관리하고 남은 ISR 멤버를 리더로 선출합니다. 이는 필요한 리더십 알림을 배치 처리하여 많은 파티션에 대해 선출을 더 저렴하고 빠르게 만듭니다. 컨트롤러 장애는 새 컨트롤러 선출을 트리거합니다.

---

## 로그 압축(Log Compaction)

로그 압축은 Kafka가 단일 토픽 파티션 로그 내에서 각 메시지 키에 대해 마지막으로 알려진 값을 보존하도록 보장합니다. 이는 애플리케이션 충돌, 시스템 장애, 유지보수 중 캐시 리로드 후 상태 복원을 처리합니다.

### 사용 사례

전통적인 보존은 고정된 기간 후 또는 미리 결정된 크기에 도달하면 오래된 로그 데이터를 삭제합니다. 이는 레코드가 독립적인 시간적 이벤트 데이터에 적합합니다. 그러나 중요한 데이터 스트림은 가변적인 키 데이터 변경(예: 데이터베이스 테이블 변경)을 추적합니다.

예시: 사용자 이메일 토픽은 사용자 ID를 기본 키로 사용하여 이메일 주소 변경 시 메시지를 수신합니다:

```
123 => bill@microsoft.com
...
123 => bill@gatesfoundation.org
...
123 => bill@gmail.com
```

로그 압축은 각 기본 키에 대한 마지막 업데이트가 유지되도록 보장하는 세분화된 보존을 제공합니다. 이를 통해 로그가 최근 변경된 키뿐만 아니라 모든 키의 최종 상태를 포함하도록 보장합니다. 다운스트림 컨슈머는 전체 변경 이력 없이 토픽에서 상태를 복원할 수 있습니다.

유용한 시나리오:

1. 데이터베이스 변경 구독: 여러 시스템(RDBMS, 캐시, 검색 클러스터, Hadoop)에 필요한 데이터셋. 실시간 변경에는 최근 로그가 필요하고, 캐시/검색 리로드 또는 장애 복구에는 완전한 데이터셋이 필요함
2. 이벤트 소싱: 변경 로그를 기본 저장소로 하는 쿼리 처리를 공존시키는 애플리케이션 설계
3. 고가용성 저널링: 변경 로깅을 통한 내결함성 로컬 연산으로 장애 시 로그 재생을 통해 다른 프로세스가 복구할 수 있음. 스트림 시스템의 카운트, 집계, group-by 처리에 유용. Samza가 이를 상태 관리에 사용

이러한 사례는 실시간 변경 피드가 필요하지만 머신 충돌, 데이터 리로드, 재처리 시 전체 로드가 가끔 필요합니다.

### 로그 압축 기본 사항

압축된 테일을 가진 Kafka 로그는 메시지당 오프셋이 있는 논리적 구조를 보여줍니다. 로그 헤드는 모든 메시지를 보존하는 조밀하고 순차적인 오프셋을 가진 전통적인 Kafka 로그와 유사합니다. 로그 압축은 테일 처리 옵션을 추가합니다.

압축된 테일 메시지는 원래 오프셋을 유지합니다. 오프셋은 절대 변경되지 않습니다. 모든 위치는 유효한 로그 위치로 유지되며, 압축된 메시지는 다음으로 높은 나타나는 오프셋과 구별할 수 없게 됩니다. 오프셋 36, 37, 38은 모두 동일한 위치를 나타내며, 어느 것에서든 읽으면 38부터 시작하는 메시지 세트를 반환합니다.

압축은 삭제를 허용합니다: 키가 있는 null 페이로드 메시지는 삭제 레코드(톰스톤)가 됩니다. 삭제 마커는 이전의 일치하는 키 메시지를 제거하고 공간을 확보하기 위해 시간이 지나면 자체적으로 사라집니다. "삭제 보존 지점"은 이 타임라인을 표시합니다.

백그라운드 압축은 읽기를 차단하지 않고 주기적으로 로그 세그먼트를 다시 복사합니다. 스로틀링은 프로듀서/컨슈머에게 영향을 미치는 과도한 I/O를 방지합니다. 세그먼트 압축은 다음과 유사합니다:

1. 헤드 대 테일 비율이 가장 높은 로그 선택
2. 키당 로그 헤드 오프셋 요약을 공간 압축으로 생성
3. 시작부터 끝까지 로그를 다시 복사하고 나중에 발생하는 키 제거
4. 새로운 깨끗한 세그먼트를 로그로 교체

### 로그 압축 보장

로그 압축은 다음을 제공합니다:

1. 따라잡은 컨슈머: 로그 헤드 내에 있는 컨슈머는 순차적 오프셋으로 모든 기록된 메시지를 봅니다. 토픽 `min.compaction.lag.ms`는 메시지에 대한 최소 압축 전 시간을 보장합니다. 토픽 `max.compaction.lag.ms`는 최대 압축 적격 지연을 보장합니다
2. 메시지 순서: 압축은 순서를 유지합니다. 메시지만 제거합니다
3. 영구적 오프셋: 메시지 오프셋은 절대 변경되지 않습니다
4. 완전한 최종 상태: 로그 시작부터 진행하는 컨슈머는 쓰기 순서로 모든 레코드의 최종 상태를 봅니다. 삭제 마커는 컨슈머가 `delete.retention.ms` 설정(기본 24시간) 내에 헤드에 도달하면 나타납니다. 삭제 마커 동시 제거는 `delete.retention.ms`를 초과하여 지연될 때 컨슈머가 마커를 놓칠 수 있음을 의미합니다

### 로그 압축 세부 사항

로그 클리너(백그라운드 스레드 풀)는 헤드 중복 키를 제거하면서 세그먼트를 다시 복사합니다. 각 스레드:

1. 헤드 대 테일 비율이 가장 높은 로그 선택
2. 헤드에 대한 마지막 키 오프셋 요약 생성
3. 시작부터 끝까지 로그를 다시 복사하고 나중에 발생하는 키 제거
4. 깨끗한 세그먼트를 로그로 교체

헤드 요약은 항목당 24바이트를 사용하는 공간 압축 해시 테이블입니다. 8GB 클리너 버퍼로 반복은 약 366GB 헤드 로그(1k 메시지 평균)를 정리합니다.

### 로그 클리너 설정

로그 클리너는 기본적으로 활성화되어 클리너 스레드 풀을 시작합니다. 특정 토픽에서 압축을 활성화:

```
log.cleanup.policy=compact
```

이 브로커 설정은 `server.properties`에서 재정의 없이 모든 클러스터 토픽에 영향을 미칩니다. 클리너는 압축 시간 지연을 통해 최소한의 압축되지 않은 "헤드" 양을 유지할 수 있습니다:

```
log.cleaner.min.compaction.lag.ms
```

이는 최소값보다 새로운 메시지가 압축되지 않도록 방지합니다. 설정 없이 현재 쓰기 중인 세그먼트를 제외한 모든 세그먼트가 적격이 됩니다. 활성 세그먼트는 나이에 관계없이 압축되지 않습니다.

최대 압축 지연은 낮은 생산 속도 로그가 무한히 부적격 상태로 남는 것을 방지합니다:

```
log.cleaner.max.compaction.lag.ms
```

이는 무한한 부적격성을 방지합니다. 설정 없이 `min.cleanable.dirty.ratio`를 초과하지 않는 로그는 압축되지 않습니다. 압축 마감일에는 하드 보장이 없으며 가용성과 실제 시간이 적용됩니다.

`uncleanable-partitions-count`, `max-clean-time-secs`, `max-compaction-delay-secs` 메트릭을 모니터링하세요.

---

## 쿼터(Quotas)

Kafka 클러스터는 클라이언트 그룹에 의한 브로커 리소스 사용을 제어하는 요청 쿼터를 적용합니다. 두 가지 쿼터 유형이 그룹별로 적용됩니다:

1. 네트워크 대역폭 쿼터: 바이트 속도 임계값 (0.9 이후)
2. 요청 속도 쿼터: 네트워크 및 I/O 스레드의 CPU 사용률 백분율 (0.11 이후)

### 쿼터가 중요한 이유

높은 볼륨을 생산/소비하거나 높은 속도의 요청을 생성하는 프로듀서/컨슈머는 브로커 리소스를 독점하고, 네트워크 포화를 유발하며, 다른 클라이언트와 브로커에 대해 DOS 공격을 할 수 있습니다. 쿼터는 이러한 문제로부터 보호합니다. 이는 멀티테넌트 클러스터에서 잘못 동작하는 클라이언트 세트가 잘 동작하는 사용자 경험을 저하시키는 것을 방지하는 데 중요합니다. 서비스 운영 Kafka는 계약에 따른 API 제한을 적용합니다.

### 클라이언트 그룹

Kafka 클라이언트 ID는 사용자 주체(보안 클러스터에서 인증된 사용자)입니다. 인증되지 않은 클러스터는 설정 가능한 `PrincipalBuilder`를 통해 사용자를 그룹화합니다. client-id는 의미 있는 애플리케이션 이름을 가진 논리적 클라이언트 그룹화입니다. (user, client-id) 튜플은 주체와 ID를 모두 공유하는 보안 클라이언트 그룹을 정의합니다.

쿼터 적용 대상은 (user, client-id), user 또는 client-id 그룹입니다. 연결당 가장 구체적인 일치 쿼터가 적용됩니다. 그룹은 설정된 쿼터를 공유합니다. 예를 들어 (user="test-user", client-id="test-client")에 10MB/sec 생산 쿼터가 있으면 "test-client" client-id를 가진 모든 "test-user" 프로듀서 인스턴스에서 공유됩니다.

### 쿼터 설정

설정은 (user, client-id), user, client-id 그룹에 대한 쿼터를 정의합니다. 재정의를 통해 필요한 수준에서 더 높은(또는 더 낮은) 쿼터를 허용합니다. 토픽별 로그 설정 재정의와 유사하게 작동합니다. 사용자 및 (user, client-id) 쿼터 재정의는 메타데이터 로그에 기록됩니다. 모든 브로커는 이를 읽고 롤링 재시작 없이 즉시 적용합니다.

우선순위 순서:

1. 일치하는 user 및 client-id 쿼터
2. 일치하는 user 및 기본 client-id 쿼터
3. 일치하는 user 쿼터
4. 기본 user 및 일치하는 client-id 쿼터
5. 기본 user 및 기본 client-id 쿼터
6. 기본 user 쿼터
7. 일치하는 client-id 쿼터
8. 기본 client-id 쿼터

### 네트워크 대역폭 쿼터

네트워크 대역폭 쿼터는 클라이언트 그룹당 바이트 속도 임계값을 정의합니다. 기본적으로 고유 그룹은 브로커별로 클러스터 전체에 설정된 고정 바이트/초 쿼터를 받습니다. 각 그룹은 스로틀링 전에 브로커당 최대 X 바이트/초를 발행/페치합니다.

### 요청 속도 쿼터

요청 속도 쿼터는 클라이언트가 쿼터 윈도우에서 브로커당 요청 핸들러 I/O 스레드와 네트워크 스레드를 활용할 수 있는 시간의 백분율을 정의합니다. n% 쿼터는 하나의 스레드의 n%를 나타내며 총 ((num.io.threads + num.network.threads) * 100)%입니다. 그룹은 스로틀링 전에 모든 I/O 및 네트워크 스레드에서 최대 n%를 사용합니다.

I/O 및 네트워크 스레드 할당은 코어 수에 기반하므로 요청 속도 쿼터는 클라이언트가 사용할 수 있는 총 CPU 백분율을 나타냅니다.

### 적용

기본적으로 고유 클라이언트 그룹은 클러스터 전체에 설정된 고정 쿼터를 받으며 브로커별로 정의됩니다. 브로커별 정의는 브로커 간 클라이언트 쿼터 사용 공유가 필요한 고정 클러스터 전체 접근 방식보다 우수합니다. 올바르게 구현하기 더 어렵습니다.

브로커 쿼터 위반 응답은 위반 클라이언트를 쿼터 이하로 가져오는 데 필요한 지연을 계산하고 응답을 즉시 반환합니다. 페치 요청에는 데이터가 포함되지 않습니다. 브로커는 클라이언트로의 채널을 음소거하여 지연이 경과할 때까지 추가 처리를 방지합니다. 응답 수신 시 Kafka 클라이언트는 지연 동안 추가 요청을 자제합니다. 스로틀된 클라이언트 요청은 양쪽 모두에서 효과적으로 차단됩니다.

브로커 지연 응답을 존중하지 않는 이전 클라이언트 구현은 브로커 소켓 채널 음소거를 통해 여전히 스로틀됩니다. 추가 스로틀된 채널 요청은 지연 후에만 응답을 받습니다.

바이트 속도 및 스레드 사용률은 빠른 쿼터 위반 감지를 위해 여러 작은 윈도우(예: 30개의 1초 윈도우)에서 측정됩니다. 큰 윈도우(예: 10개의 30초 윈도우)는 큰 트래픽 버스트와 긴 지연을 유발합니다. 이는 좋지 않은 사용자 경험입니다.

---

## 참고 자료

- [Apache Kafka 공식 문서](https://kafka.apache.org/documentation/)
- [Apache Kafka Design](https://kafka.apache.org/documentation/#design)
