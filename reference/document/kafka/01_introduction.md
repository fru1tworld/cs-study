# Kafka 소개

> 이 문서는 Apache Kafka 공식 문서의 Introduction 섹션을 한국어로 번역한 것입니다.
> 원본: https://kafka.apache.org/documentation/#introduction

---

## 목차

1. [이벤트 스트리밍이란?](#이벤트-스트리밍이란)
2. [Apache Kafka는 이벤트 스트리밍 플랫폼입니다](#apache-kafka는-이벤트-스트리밍-플랫폼입니다)
3. [Kafka는 어떻게 동작하나요?](#kafka는-어떻게-동작하나요)
4. [주요 개념과 용어](#주요-개념과-용어)
5. [Kafka API](#kafka-api)
6. [다음 단계](#다음-단계)

---

## 이벤트 스트리밍이란?

이벤트 스트리밍(Event Streaming)은 인체의 중추신경계에 해당하는 디지털 기술 입니다. 이는 '상시 가동(always-on)' 세계의 기술적 기반으로, 비즈니스가 점점 더 소프트웨어에 의해 정의되고 자동화되며, 소프트웨어 사용자 자체도 소프트웨어인 세상을 지원합니다.

기술적으로, 이벤트 스트리밍은 데이터베이스, 센서, 모바일 기기, 클라우드 서비스, 소프트웨어 애플리케이션 등의 이벤트 소스 에서 이벤트 스트림 형태로 실시간 데이터를 캡처하는 것입니다. 이렇게 캡처된 이벤트 스트림을 나중에 검색할 수 있도록 영구적으로 저장하고, 이벤트 스트림을 실시간으로 처리하거나 소급하여 조작 및 처리하며, 필요에 따라 이벤트 스트림을 다른 목적지 기술로 라우팅합니다. 따라서 이벤트 스트리밍은 데이터의 지속적인 흐름과 해석을 보장하여 올바른 정보가 적시에 적절한 위치에 있도록 합니다.

### 이벤트 스트리밍의 활용 사례

이벤트 스트리밍은 다양한 산업과 조직의 광범위한 사용 사례에 적용됩니다. 대표적인 활용 사례는 다음과 같습니다:

- 금융 서비스: 은행, 주식 거래소, 보험사 등에서 실시간으로 결제 및 금융 거래를 처리
- 물류 및 운송: 차량, 트럭, 선박, 화물 등을 실시간으로 추적하고 모니터링
- 사물인터넷(IoT): 공장 및 풍력 발전 단지와 같은 IoT 기기에서 센서 데이터를 수집하고 분석
- 소매 및 서비스: 소매, 호텔 및 여행 산업, 모바일 애플리케이션에서 고객 상호작용 및 주문을 실시간 추적
- 헬스케어: 환자 모니터링 및 건강 상태 변화 예측을 통한 응급 상황 대응
- 데이터 플랫폼: 기업의 여러 부서 간 데이터 연결 및 실시간 공유
- 마이크로서비스 아키텍처: 데이터 플랫폼, 이벤트 기반 아키텍처, 마이크로서비스의 기반 기술로 활용

---

## Apache Kafka는 이벤트 스트리밍 플랫폼입니다

Kafka는 세 가지 핵심 기능을 결합하여 엔드투엔드(end-to-end) 이벤트 스트리밍 을 단일 검증된 솔루션으로 구현할 수 있게 해줍니다:

1. 이벤트 스트림 발행(publish) 및 구독(subscribe): 다른 시스템에서 데이터를 지속적으로 가져오거나(import) 내보내기(export)
2. 이벤트 스트림의 영구 저장: 원하는 기간 동안 이벤트 스트림을 안정적이고 신뢰성 있게 저장
3. 이벤트 스트림 처리: 이벤트가 발생하는 시점 또는 소급하여 이벤트 스트림을 처리

이러한 모든 기능은 분산, 고확장성(highly scalable), 탄력성(elastic), 내결함성(fault-tolerant), 보안 방식으로 제공됩니다. Kafka는 베어메탈 하드웨어, 가상 머신, 컨테이너에 배포할 수 있으며, 온프레미스 환경과 클라우드 환경 모두에서 운영할 수 있습니다. 또한 Kafka 환경을 직접 관리하거나 다양한 벤더가 제공하는 완전 관리형 서비스를 사용할 수 있습니다.

---

## Kafka는 어떻게 동작하나요?

Kafka는 서버 와 클라이언트 로 구성된 분산 시스템으로, 고성능 TCP 네트워크 프로토콜을 통해 통신합니다. 온프레미스 및 클라우드 환경의 베어메탈 하드웨어, 가상 머신, 컨테이너에 배포할 수 있습니다.

### 서버 (Servers)

Kafka는 하나 이상의 서버로 구성된 클러스터로 실행되며, 여러 데이터센터나 클라우드 리전에 걸쳐 있을 수 있습니다.

- 브로커(Broker): 일부 서버는 스토리지 레이어를 형성하며, 이를 브로커라고 합니다.
- Kafka Connect: 다른 서버들은 Kafka Connect를 실행하여 데이터를 이벤트 스트림으로 지속적으로 가져오고(import) 내보내기(export)하며, 관계형 데이터베이스나 다른 Kafka 클러스터와 같은 기존 시스템과 Kafka를 통합합니다.

미션 크리티컬한 사용 사례를 충족하기 위해, Kafka 클러스터는 고확장성 과 내결함성 을 갖추고 있습니다. 서버 중 하나에 장애가 발생하면 다른 서버들이 작업을 인계받아 데이터 손실 없이 지속적인 운영을 보장합니다.

### 클라이언트 (Clients)

클라이언트를 사용하면 네트워크 문제나 머신 장애가 발생하더라도 내결함성을 갖춘 방식으로 이벤트 스트림을 병렬로 대규모(at scale)로 읽고, 쓰고, 처리하는 분산 애플리케이션과 마이크로서비스를 작성할 수 있습니다.

Kafka에는 커뮤니티에서 제공하는 수십 개의 클라이언트와 함께 여러 클라이언트가 포함되어 있습니다. Java, Scala 등의 고수준 언어와 Go, Python, C/C++ 등 다양한 프로그래밍 언어용 클라이언트, 그리고 REST API를 사용할 수 있습니다.

---

## 주요 개념과 용어

### 이벤트 (Event)

이벤트 는 세상에서 "무언가가 일어났다"는 사실을 기록합니다. 문서에서는 레코드(record) 또는 메시지(message)라고도 합니다. Kafka에서 데이터를 읽거나 쓸 때, 이벤트 형태로 수행합니다.

개념적으로 이벤트는 키(key), 값(value), 타임스탬프(timestamp), 그리고 선택적 메타데이터 헤더(metadata headers) 를 가집니다.

다음은 이벤트의 예시입니다:

- 이벤트 키: "Alice"
- 이벤트 값: "Bob에게 $200를 지불함"
- 이벤트 타임스탬프: "Jun. 25, 2020 at 2:06 p.m."

### 프로듀서와 컨슈머 (Producers and Consumers)

프로듀서(Producer) 는 Kafka에 이벤트를 발행(write)하는 클라이언트 애플리케이션이고, 컨슈머(Consumer) 는 이러한 이벤트를 구독(read 및 처리)하는 클라이언트 애플리케이션입니다.

Kafka에서 프로듀서와 컨슈머는 완전히 분리(decoupled) 되어 있으며 서로에 대해 알지 못합니다. 이는 Kafka가 잘 알려진 높은 확장성을 달성하기 위한 핵심 설계 요소입니다. 예를 들어, 프로듀서는 컨슈머를 기다릴 필요가 없습니다. Kafka는 이벤트를 정확히 한 번(exactly-once) 처리하는 기능과 같은 다양한 보장(guarantees)을 제공합니다.

### 토픽 (Topics)

이벤트는 토픽(Topic) 으로 구성되고 영구적으로 저장됩니다. 매우 단순하게 설명하면, 토픽은 파일 시스템의 폴더와 유사하고, 이벤트는 해당 폴더 안의 파일과 같습니다.

예를 들어, 토픽 이름은 "payments"일 수 있습니다. Kafka의 토픽은 항상 다중 프로듀서(multi-producer) 및 다중 구독자(multi-subscriber) 입니다. 즉, 토픽에는 0개, 하나, 또는 여러 프로듀서가 이벤트를 쓸 수 있고, 0개, 하나, 또는 여러 컨슈머가 이러한 이벤트를 구독할 수 있습니다.

토픽의 이벤트는 필요한 만큼 여러 번 읽을 수 있습니다. 전통적인 메시징 시스템과 달리, 이벤트는 소비 후에도 삭제되지 않습니다. 대신 토픽별 설정을 통해 Kafka가 이벤트를 얼마나 오래 보관할지 정의하며, 이후 오래된 이벤트가 삭제됩니다. Kafka의 성능은 데이터 크기와 관계없이 일정하므로, 데이터를 장기간 저장해도 문제가 없습니다.

### 파티션 (Partitions)

토픽은 파티션(Partition) 됩니다. 즉, 토픽은 서로 다른 Kafka 브로커에 위치한 여러 "버킷"에 분산됩니다. 데이터의 이러한 분산 배치는 확장성에 매우 중요합니다. 클라이언트 애플리케이션이 여러 브로커에서 동시에 데이터를 읽고 쓸 수 있기 때문입니다.

새 이벤트가 토픽에 발행되면, 실제로는 토픽의 파티션 중 하나에 추가됩니다. 동일한 이벤트 키(예: 고객 ID 또는 차량 ID)를 가진 이벤트는 같은 파티션에 기록되며, Kafka는 특정 토픽 파티션의 모든 컨슈머가 항상 해당 파티션의 이벤트를 기록된 순서와 정확히 동일한 순서로 읽는 것을 보장합니다.

### 복제 (Replication)

내결함성과 고가용성을 보장하기 위해 모든 토픽은 복제(replicate) 될 수 있습니다. 복제는 지리적 리전이나 데이터센터 간에도 가능하므로, 문제가 발생하거나 유지보수가 필요하거나 기타 이유로 항상 데이터 복사본을 가진 여러 브로커가 존재합니다.

일반적인 프로덕션 설정은 복제 팩터(replication factor) 3 입니다. 즉, 데이터의 복사본이 항상 3개 존재합니다. 이 복제는 토픽 파티션 수준에서 수행됩니다.

---

## Kafka API

Kafka에는 관리 및 운영 작업을 위한 Admin API 외에 다음과 같은 다섯 가지 핵심 API가 있습니다:

### Producer API

Producer API 를 사용하면 애플리케이션이 하나 이상의 Kafka 토픽에 이벤트 스트림을 발행(write)할 수 있습니다.

### Consumer API

Consumer API 를 사용하면 애플리케이션이 하나 이상의 토픽을 구독하고 해당 토픽에서 생성된 이벤트 스트림을 처리할 수 있습니다.

### Kafka Streams API

Kafka Streams API 를 사용하면 스트림 처리 애플리케이션 과 마이크로서비스를 구현할 수 있습니다. 입력 스트림을 출력 스트림으로 변환하는 기능을 포함하여 이벤트를 처리하는 고급 함수를 제공합니다.

주요 기능:
- 변환(Transformations)
- 상태 저장 연산(Stateful Operations): 집계(aggregations), 조인(joins)
- 윈도잉(Windowing)
- 스트림과 테이블 처리

### Kafka Connect API

Kafka Connect API 는 Kafka 토픽에서 데이터를 읽거나 쓰는 재사용 가능한 커넥터(Connectors) 를 구축하고 실행합니다.

예를 들어, PostgreSQL과 같은 관계형 데이터베이스에 연결하는 커넥터는 테이블의 모든 변경 사항을 캡처할 수 있습니다. Kafka Connect 생태계에는 이미 수백 개의 즉시 사용 가능한 커넥터가 있습니다.

### Admin API

Admin API 를 사용하면 토픽, 브로커 및 기타 Kafka 객체를 관리하고 검사할 수 있습니다.

---

## 다음 단계

Kafka를 시작하기 위한 다음 단계를 안내합니다:

- [빠른 시작 가이드](https://kafka.apache.org/quickstart): 첫 번째 Kafka 클러스터를 설정하고 실행해 보세요.
- [문서](https://kafka.apache.org/documentation/): 상세한 Kafka 문서를 참조하세요.
- [사용 사례](https://kafka.apache.org/powered-by): 커뮤니티에서 Kafka를 어떻게 활용하고 있는지 확인하세요.
- [Kafka Summit](https://kafka.apache.org/events): Kafka Summit 및 지역 커뮤니티 이벤트에 참가하세요.

---

## 참고 자료

- [Apache Kafka 공식 문서](https://kafka.apache.org/documentation/)
- [Apache Kafka 소개 페이지](https://kafka.apache.org/intro)
- [Kafka Quickstart](https://kafka.apache.org/quickstart)
- [Kafka APIs](https://kafka.apache.org/documentation/#api)
- [Kafka Design](https://kafka.apache.org/documentation/#design)
