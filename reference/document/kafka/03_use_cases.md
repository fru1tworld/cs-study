# Kafka 사용 사례

> 이 문서는 Apache Kafka 공식 문서의 "Use Cases" 섹션을 한국어로 번역한 것입니다.
> 원본: https://kafka.apache.org/documentation/#uses

## 개요

Apache Kafka는 다양한 사용 사례에 적용할 수 있는 범용 분산 스트리밍 플랫폼입니다. 다음은 Kafka의 주요 사용 사례들입니다.

---

## 1. 메시징 (Messaging)

Kafka는 전통적인 메시지 브로커(Message Broker)의 대체제로 잘 작동합니다. 메시지 브로커는 다양한 이유로 사용됩니다 (데이터 생산자와 처리를 분리하고, 처리되지 않은 메시지를 버퍼링하는 등). 대부분의 메시징 시스템과 비교했을 때, Kafka는 더 나은 처리량(Throughput), 내장된 파티셔닝(Partitioning), 복제(Replication), 그리고 내결함성(Fault-tolerance)을 제공하며, 이는 대규모 메시지 처리 애플리케이션에 좋은 솔루션이 됩니다.

메시징 사용 사례에서 처리량은 비교적 낮지만 종단 간 지연 시간(End-to-end Latency)이 낮아야 하며, Kafka가 제공하는 강력한 내구성(Durability) 보장이 필요한 경우가 많습니다.

이 영역에서 Kafka는 [ActiveMQ](http://activemq.apache.org/)나 [RabbitMQ](https://www.rabbitmq.com/)와 같은 전통적인 메시징 시스템과 비교할 수 있습니다.

---

## 2. 웹사이트 활동 추적 (Website Activity Tracking)

Kafka의 원래 사용 사례는 사용자 활동 추적 파이프라인을 실시간 발행-구독(Publish-Subscribe) 피드 세트로 재구축하는 것이었습니다. 이는 사이트 활동(페이지 조회, 검색, 또는 사용자가 수행할 수 있는 기타 작업)이 활동 유형별로 하나의 토픽에 게시됨을 의미합니다.

이러한 피드는 실시간 처리, 실시간 모니터링, 오프라인 처리 및 보고를 위한 Hadoop 또는 오프라인 데이터 웨어하우스 시스템으로의 로딩을 포함한 다양한 사용 사례에 대한 구독에 사용할 수 있습니다.

활동 추적은 각 사용자 페이지 조회에 대해 많은 활동 메시지가 생성되므로 매우 높은 볼륨을 처리해야 하는 경우가 많습니다.

---

## 3. 메트릭 (Metrics)

Kafka는 운영 모니터링 데이터에 자주 사용됩니다. 이는 분산 애플리케이션의 통계를 집계하여 운영 데이터의 중앙 집중식 피드를 생성하는 것을 포함합니다.

---

## 4. 로그 집계 (Log Aggregation)

많은 사람들이 Kafka를 로그 집계 솔루션의 대체제로 사용합니다. 로그 집계는 일반적으로 서버에서 물리적 로그 파일을 수집하여 처리를 위해 중앙 위치(파일 서버 또는 HDFS 등)에 배치합니다.

Kafka는 파일의 세부 사항을 추상화하고 로그 또는 이벤트 데이터를 메시지 스트림으로 더 깔끔하게 추상화합니다. 이를 통해 더 낮은 지연 시간 처리가 가능하고 여러 데이터 소스와 분산 데이터 소비를 더 쉽게 지원할 수 있습니다.

Scribe나 Flume과 같은 로그 중심 시스템과 비교했을 때, Kafka는 동등하게 좋은 성능, 복제로 인한 더 강력한 내구성 보장, 그리고 훨씬 낮은 종단 간 지연 시간을 제공합니다.

---

## 5. 스트림 처리 (Stream Processing)

많은 Kafka 사용자들은 여러 단계로 구성된 처리 파이프라인에서 데이터를 처리합니다. 이러한 파이프라인에서 원시 입력 데이터는 Kafka 토픽에서 소비된 후 집계, 보강 또는 다른 방식으로 변환되어 추가 소비 또는 후속 처리를 위해 새로운 토픽에 게시됩니다.

예를 들어, 뉴스 기사 추천을 위한 처리 파이프라인은 RSS 피드에서 기사 콘텐츠를 크롤링하여 "articles" 토픽에 게시할 수 있습니다. 추가 처리에서 이 콘텐츠를 정규화하거나 중복을 제거하여 정리된 기사 콘텐츠를 새 토픽에 게시할 수 있습니다. 최종 처리 단계에서는 이 콘텐츠를 사용자에게 추천하려고 시도할 수 있습니다.

이러한 처리 파이프라인은 개별 토픽을 기반으로 실시간 데이터 흐름의 그래프를 생성합니다. 0.10.0.0 버전부터 Apache Kafka에서 사용할 수 있는 경량이지만 강력한 스트림 처리 라이브러리인 [Kafka Streams](https://kafka.apache.org/documentation/streams)가 이러한 데이터 처리를 수행하는 데 사용 가능합니다.

Kafka Streams 외에도 [Apache Storm](https://storm.apache.org/)과 [Apache Samza](http://samza.apache.org/)와 같은 대안적인 오픈 소스 스트림 처리 도구가 있습니다.

---

## 6. 이벤트 소싱 (Event Sourcing)

이벤트 소싱은 상태 변경이 시간 순서대로 정렬된 레코드 시퀀스로 기록되는 애플리케이션 설계 스타일입니다. Kafka의 매우 큰 저장된 로그 데이터에 대한 지원은 이 스타일로 구축된 애플리케이션을 위한 훌륭한 백엔드가 됩니다.

---

## 7. 커밋 로그 (Commit Log)

Kafka는 분산 시스템을 위한 외부 커밋 로그(Commit-log) 역할을 할 수 있습니다. 로그는 노드 간에 데이터를 복제하는 데 도움이 되며, 실패한 노드가 데이터를 복원하기 위한 재동기화 메커니즘으로 작동합니다.

Kafka의 [로그 컴팩션(Log Compaction)](https://kafka.apache.org/documentation.html#compaction) 기능은 이 사용을 지원하는 데 도움이 됩니다. 이 사용 사례에서 Kafka는 [Apache BookKeeper](https://bookkeeper.apache.org/) 프로젝트와 유사합니다.

---

## 요약

| 사용 사례 | 설명 | 특징 |
|-----------|------|------|
| 메시징 | 전통적인 메시지 브로커 대체 | 높은 처리량, 파티셔닝, 복제, 내결함성 |
| 웹사이트 활동 추적 | 사용자 활동의 실시간 추적 | 대용량 처리, 실시간 피드 |
| 메트릭 | 운영 모니터링 데이터 집계 | 분산 애플리케이션 통계 중앙 집중화 |
| 로그 집계 | 서버 로그의 중앙 집중식 수집 | 낮은 지연 시간, 분산 소비 지원 |
| 스트림 처리 | 다단계 데이터 처리 파이프라인 | 실시간 데이터 흐름, Kafka Streams |
| 이벤트 소싱 | 상태 변경의 시간순 기록 | 대용량 로그 데이터 저장 |
| 커밋 로그 | 분산 시스템의 외부 커밋 로그 | 로그 컴팩션, 데이터 복제 및 복원 |

---

## 참고 자료

- [Apache Kafka 공식 문서](https://kafka.apache.org/documentation/)
- [Kafka Streams](https://kafka.apache.org/documentation/streams)
- [Kafka 로그 컴팩션](https://kafka.apache.org/documentation.html#compaction)
