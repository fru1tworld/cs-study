# 4. 다양한 분류 알고리즘

## 4.1 로지스틱 회귀

### 로지스틱 회귀

**로지스틱 회귀(logistic regression)**는 이름은 회귀이지만 분류 모델이다.

선형 방정식을 사용한 분류 알고리즘이다. 이 알고리즘은 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며, 이를 통해 분류를 수행한다.

### 시그모이드 함수

**시그모이드 함수(sigmoid function)** 또는 **로지스틱 함수(logistic function)**는 선형 방정식의 출력을 0과 1 사이의 값으로 압축한다.

z = a × (Weight) + b × (Length) + c × (Diagonal) + d × (Height) + e × (Width) + f

sigmoid(z) = 1 / (1 + e^(-z))

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.predict(train_scaled[:5]))
print(lr.predict_proba(train_scaled[:5]))
```

### 다중 분류

**다중 분류(multi-class classification)**는 타깃 클래스가 2개 이상인 분류 문제다.

LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용한다. max_iter 매개변수에서 반복 횟수를 지정하며 기본값은 100이다.

```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```

### 소프트맥스 함수

**소프트맥스 함수(softmax function)**는 여러 개의 선형 방정식 출력값을 0과 1 사이로 압축하고 전체 합이 1이 되도록 만든다.

이를 통해 다중 분류에서 각 클래스에 대한 확률을 계산한다.

## 4.2 확률적 경사 하강법

### 점진적 학습

**점진적 학습(incremental learning)** 또는 **온라인 학습(online learning)**은 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식이다.

### 확률적 경사 하강법

**확률적 경사 하강법(stochastic gradient descent)**은 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 손실 함수의 경사를 따라 최적값을 찾아가는 방법이다.

**경사 하강법(gradient descent)**은 경사를 따라 원하는 지점에 도달하는 것이 목표다.

**미니배치 경사 하강법(minibatch gradient descent)**은 여러 개의 샘플을 사용해 경사 하강법을 수행한다.

**배치 경사 하강법(batch gradient descent)**은 전체 샘플을 사용한다.

```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
```

### 손실 함수

**손실 함수(loss function)**는 확률적 경사 하강법이 최적화할 대상이다.

**로지스틱 손실 함수(logistic loss function)** 또는 **이진 크로스엔트로피 손실 함수(binary cross-entropy loss function)**는 이진 분류를 위한 손실 함수다.

**크로스엔트로피 손실 함수(cross-entropy loss function)**는 다중 분류를 위한 손실 함수다.

### 에포크

**에포크(epoch)**는 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번의 반복을 의미한다.

일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복한다.

```python
sc.partial_fit(train_scaled, train_target)
```

partial_fit() 메서드를 사용하면 추가 학습이 가능하다.

## 핵심 키워드

- **로지스틱 회귀**: 선형 방정식을 사용한 분류 알고리즘
- **시그모이드 함수**: 선형 방정식의 출력을 0과 1 사이의 값으로 압축
- **소프트맥스 함수**: 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다
- **확률적 경사 하강법**: 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘
- **손실 함수**: 확률적 경사 하강법이 최적화할 대상
- **에포크**: 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번의 반복
