# 2. 데이터 다루기

## 2.1 훈련 세트와 테스트 세트

### 훈련 세트와 테스트 세트

머신러닝 모델을 훈련시킬 때 사용하는 데이터를 **훈련 데이터(training data)** 또는 **훈련 세트(training set)**라고 한다.

평가에 사용하는 데이터를 **테스트 데이터(test data)** 또는 **테스트 세트(test set)**라고 부른다.

### 샘플링 편향

훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않으면 **샘플링 편향(sampling bias)**이라고 한다.

특정 종류의 샘플이 과도하게 많으면, 제대로 훈련되지 못하고 일부 샘플만 잘 예측하는 모델이 된다.

### numpy

numpy는 파이썬의 대표적인 배열 라이브러리다.

```python
import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
```

배열 인덱싱:

```python
print(input_arr[[1,3]])  # 2번째와 4번째 행 선택
```

### 훈련 세트와 테스트 세트 만들기

```python
# 랜덤 시드 설정
np.random.seed(42)

# 인덱스 섞기
index = np.arange(49)
np.random.shuffle(index)

# 훈련 세트와 테스트 세트 나누기
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]
```

## 2.2 데이터 전처리

### 넘파이로 데이터 준비하기

column_stack() 함수는 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결한다.

```python
fish_data = np.column_stack((fish_length, fish_weight))
```

### scikit-learn으로 훈련 세트와 테스트 세트 나누기

사이킷런의 train_test_split() 함수를 사용하면 훈련 세트와 테스트 세트를 쉽게 나눌 수 있다.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, random_state=42)
```

stratify 매개변수에 타깃 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눈다. 이를 **층화 샘플링(stratified sampling)**이라고 한다.

```python
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify=fish_target, random_state=42)
```

### 수상한 도미 한 마리

```python
kn = kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```

테스트 세트의 정확도가 훈련 세트보다 낮다. 이는 모델이 훈련 세트에 **과대적합(overfitting)**되었다는 신호다.

### 기준을 맞춰라

각 특성(feature)의 스케일이 다르면 거리 기반 알고리즘에서 문제가 발생한다.

예를 들어, 길이는 10~40 범위인데 무게는 0~1000 범위라면, 무게에 의해 거리가 쉽게 좌우된다.

### 표준점수

**표준점수(z-score)** 또는 **z-점수**는 각 데이터가 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다.

분산은 데이터에서 평균을 뺀 값을 모두 제곱한 다음 평균을 내어 구한다. 표준편차는 분산의 제곱근이다.

```python
# 표준점수 구하기
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

train_scaled = (train_input - mean) / std
```

훈련 세트의 평균과 표준편차로 테스트 세트도 변환해야 한다.

```python
test_scaled = (test_input - mean) / std
```

## 2.3 KNN 회귀 (있다면)

**k-최근접 이웃 회귀(k-Nearest Neighbors Regression)**

분류 문제는 클래스 중 하나로 분류하는 것이지만, 회귀 문제는 임의의 어떤 숫자를 예측하는 문제다.

KNN 회귀는 예측하려는 샘플에 가장 가까운 샘플 k개를 선택한 다음, 이 샘플들의 타깃값을 평균하여 예측값으로 사용한다.

```python
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)

# 예측
print(knr.predict([[50]]))
```

### 결정계수 (R²)

회귀 모델의 성능을 측정하는 지표로 **결정계수(coefficient of determination)** 또는 **R²**가 있다.

R²는 1에 가까울수록 좋고, 0에 가까울수록 성능이 나쁘다.

```python
print(knr.score(test_input, test_target))
```

### 과대적합 vs 과소적합

- **과대적합(overfitting)**: 모델이 훈련 세트에서는 높은 점수를 내지만 테스트 세트에서는 낮은 점수를 낸다
- **과소적합(underfitting)**: 모델이 너무 단순하여 훈련 세트와 테스트 세트 모두에서 낮은 점수를 낸다

훈련 세트보다 테스트 세트의 점수가 높거나, 둘 다 낮으면 과소적합이다.
