# 5. 트리 알고리즘

## 5.1 결정 트리

### 결정 트리

**결정 트리(decision tree)**는 예/아니오에 대한 질문을 이어 나가면서 학습한다.

질문이나 네모 상자를 **노드(node)**라고 부른다. 맨 위의 노드를 **루트 노드(root node)**라고 하고, 맨 아래 끝에 달린 노드를 **리프 노드(leaf node)**라고 한다.

```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
```

### 불순도

**불순도(impurity)**는 결정 트리가 최적의 질문을 찾기 위한 기준이다.

**지니 불순도(Gini impurity)**는 다음과 같이 계산한다:

지니 불순도 = 1 - (음성 클래스 비율² + 양성 클래스 비율²)

**엔트로피 불순도(entropy impurity)**는 다음과 같이 계산한다:

엔트로피 불순도 = -(음성 클래스 비율 × log₂(음성 클래스 비율) + 양성 클래스 비율 × log₂(양성 클래스 비율))

```python
dt = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)
```

### 가지치기

결정 트리에서 가지치기를 하지 않으면 훈련 세트를 완벽하게 학습하여 과대적합된다.

가지치기 관련 매개변수:

- max_depth: 트리가 자랄 수 있는 최대 깊이
- min_samples_split: 노드를 나누기 위한 최소 샘플 수
- min_samples_leaf: 리프 노드가 되기 위한 최소 샘플 수

### 특성 중요도

**특성 중요도(feature importance)**는 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타낸 값이다.

```python
print(dt.feature_importances_)
```

## 5.2 교차 검증과 그리드 서치

### 검증 세트

**검증 세트(validation set)**는 하이퍼파라미터 튜닝을 위해 모델을 평가할 때 사용하는 데이터 세트다.

테스트 세트는 마지막에 최종적으로 모델을 평가할 때만 사용한다.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

# 훈련 세트를 다시 나눔
sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42)
```

### 교차 검증

**교차 검증(cross validation)**은 검증 세트를 떼어 내어 평가하는 과정을 여러 번 반복한다. 그런 다음 이 점수를 평균하여 최종 검증 점수를 얻는다.

**k-폴드 교차 검증(k-fold cross validation)**은 훈련 세트를 k개로 나누어 교차 검증을 수행하는 방법이다.

```python
from sklearn.model_selection import cross_validate

scores = cross_validate(dt, train_input, train_target)
print(np.mean(scores['test_score']))
```

### 그리드 서치

**그리드 서치(grid search)**는 하이퍼파라미터 탐색을 자동화해 주는 도구다.

탐색할 매개변수를 나열하면 교차 검증을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택한다.

```python
from sklearn.model_selection import GridSearchCV

params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

# 최상의 모델
dt = gs.best_estimator_
print(dt.score(train_input, train_target))
```

### 랜덤 서치

**랜덤 서치(random search)**는 매개변수 값의 목록을 전달하는 것이 아니라 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달한다.

```python
from scipy.stats import uniform, randint
from sklearn.model_selection import RandomizedSearchCV

params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25)}

rs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,
                        n_iter=100, n_jobs=-1, random_state=42)
rs.fit(train_input, train_target)
```

## 5.3 트리의 앙상블

### 정형 데이터와 비정형 데이터

**정형 데이터(structured data)**는 어떤 구조로 되어 있는 데이터다. CSV나 데이터베이스, 엑셀에 저장하기 쉽다.

**비정형 데이터(unstructured data)**는 데이터베이스나 엑셀로 표현하기 어려운 데이터다. 책의 글과 디지털 카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이다.

정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 **앙상블 학습(ensemble learning)**이다.

### 랜덤 포레스트

**랜덤 포레스트(random forest)**는 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 만든다. 그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.

**부트스트랩 샘플(bootstrap sample)**은 훈련 세트에서 랜덤하게 샘플을 추출하여 만든 훈련 세트다. 중복을 허용하여 샘플링한다.

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)

print(rf.score(train_input, train_target))
print(rf.score(test_input, test_target))
```

### 엑스트라 트리

**엑스트라 트리(extra trees)**는 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만, 부트스트랩 샘플을 사용하지 않는다.

대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다.

```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
et.fit(train_input, train_target)
```

### 그레이디언트 부스팅

**그레이디언트 부스팅(gradient boosting)**은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
gb.fit(train_input, train_target)
```

그레이디언트 부스팅의 속도를 개선한 **히스토그램 기반 그레이디언트 부스팅(histogram-based gradient boosting)**도 있다.

```python
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
hgb.fit(train_input, train_target)
```

## 핵심 키워드

- **결정 트리**: 예/아니오 질문을 이어 나가며 학습하는 알고리즘
- **불순도**: 결정 트리가 최적의 질문을 찾기 위한 기준
- **교차 검증**: 훈련 세트를 여러 폴드로 나누어 검증하는 방법
- **그리드 서치**: 하이퍼파라미터 탐색을 자동화해 주는 도구
- **랜덤 포레스트**: 랜덤하게 만든 결정 트리를 사용하는 앙상블 학습
- **그레이디언트 부스팅**: 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법
