# 10. 언어 모델을 위한 신경망

## 10.1 시퀀스-투-시퀀스 모델

### 시퀀스-투-시퀀스

**시퀀스-투-시퀀스(sequence-to-sequence)**는 입력 시퀀스를 출력 시퀀스로 변환하는 모델이다.

기계 번역, 챗봇, 요약 등에 사용된다.

### 인코더-디코더

시퀀스-투-시퀀스 모델은 **인코더(encoder)**와 **디코더(decoder)** 두 부분으로 구성된다.

- **인코더**: 입력 시퀀스를 처리하여 컨텍스트 벡터를 생성
- **디코더**: 컨텍스트 벡터를 받아 출력 시퀀스를 생성

```python
from tensorflow import keras

# 인코더
encoder_input = keras.layers.Input(shape=(None,))
encoder_embedding = keras.layers.Embedding(src_vocab_size, 256)(encoder_input)
encoder_lstm = keras.layers.LSTM(256, return_state=True)
encoder_output, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# 디코더
decoder_input = keras.layers.Input(shape=(None,))
decoder_embedding = keras.layers.Embedding(tar_vocab_size, 256)(decoder_input)
decoder_lstm = keras.layers.LSTM(256, return_sequences=True, return_state=True)
decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = keras.layers.Dense(tar_vocab_size, activation='softmax')
decoder_output = decoder_dense(decoder_output)

model = keras.Model([encoder_input, decoder_input], decoder_output)
```

## 10.2 어텐션 메커니즘

### 어텐션

**어텐션(attention)**은 디코더가 출력을 생성할 때 인코더의 모든 은닉 상태를 참고하도록 하는 메커니즘이다.

시퀀스-투-시퀀스 모델의 성능을 크게 향상시킨다.

### 어텐션 스코어

**어텐션 스코어(attention score)**는 디코더의 현재 은닉 상태와 인코더의 각 은닉 상태 사이의 유사도를 계산한 값이다.

**어텐션 가중치(attention weight)**는 어텐션 스코어를 소프트맥스 함수에 통과시켜 얻은 값이다.

### 컨텍스트 벡터

어텐션 가중치와 인코더의 은닉 상태를 가중 평균하여 **컨텍스트 벡터(context vector)**를 만든다.

## 10.3 트랜스포머

### 트랜스포머

**트랜스포머(Transformer)**는 순환층 없이 어텐션만으로 입력과 출력의 관계를 학습하는 모델이다.

2017년 구글에서 발표한 "Attention Is All You Need" 논문에서 제안되었다.

트랜스포머의 장점:

- 병렬 처리가 가능하여 훈련 속도가 빠르다
- 장거리 의존성을 잘 학습한다

### 셀프 어텐션

**셀프 어텐션(self-attention)**은 입력 시퀀스 내의 각 요소가 서로에게 얼마나 관련이 있는지를 계산한다.

### 멀티 헤드 어텐션

**멀티 헤드 어텐션(multi-head attention)**은 여러 개의 어텐션을 병렬로 수행하여 다양한 관점에서 정보를 추출한다.

### 포지셔널 인코딩

트랜스포머는 순환층을 사용하지 않기 때문에 위치 정보를 별도로 제공해야 한다.

**포지셔널 인코딩(positional encoding)**은 각 위치에 고유한 벡터를 더하여 위치 정보를 제공한다.

## 10.4 사전 훈련된 언어 모델

### 전이 학습

**전이 학습(transfer learning)**은 한 작업에서 학습한 지식을 다른 작업에 적용하는 방법이다.

### BERT

**BERT(Bidirectional Encoder Representations from Transformers)**는 양방향 트랜스포머 인코더를 사용한 사전 훈련 모델이다.

BERT는 마스크 언어 모델(MLM)과 다음 문장 예측(NSP) 두 가지 작업으로 사전 훈련된다.

### GPT

**GPT(Generative Pre-trained Transformer)**는 트랜스포머 디코더를 사용한 자동 회귀 언어 모델이다.

GPT-2, GPT-3 등의 후속 모델이 개발되어 언어 생성 작업에서 뛰어난 성능을 보인다.

### 파인튜닝

**파인튜닝(fine-tuning)**은 사전 훈련된 모델을 특정 작업에 맞게 추가로 훈련하는 과정이다.

```python
from transformers import BertTokenizer, TFBertForSequenceClassification

# 사전 훈련된 BERT 모델 로드
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

# 파인튜닝
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(train_dataset, epochs=3, validation_data=val_dataset)
```

## 핵심 키워드

- **시퀀스-투-시퀀스**: 입력 시퀀스를 출력 시퀀스로 변환하는 모델
- **인코더-디코더**: 시퀀스-투-시퀀스 모델의 구조
- **어텐션**: 디코더가 인코더의 모든 은닉 상태를 참고하도록 하는 메커니즘
- **트랜스포머**: 순환층 없이 어텐션만으로 학습하는 모델
- **셀프 어텐션**: 입력 시퀀스 내의 관계를 학습하는 어텐션
- **BERT**: 양방향 트랜스포머 인코더를 사용한 사전 훈련 모델
- **GPT**: 트랜스포머 디코더를 사용한 자동 회귀 언어 모델
- **파인튜닝**: 사전 훈련된 모델을 특정 작업에 맞게 추가 훈련하는 과정
