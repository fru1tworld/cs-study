# 9장 웹 크롤러 설계

웹 크롤러는 로봇 또는 스파이더라고도 부른다. 검색 엔진에서 널리 쓰이는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.

## 1단계: 문제 이해 및 설계 범위 확정

### 기본 알고리즘

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다
2. 다운받은 웹 페이지에서 URL들을 추출한다
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다

### 크롤러의 용도

- 검색 엔진 인덱싱: 크롤러의 가장 보편적인 용도다. 웹 페이지를 수집하여 검색 엔진을 위한 로컬 인덱스를 만든다
- 웹 아카이빙: 나중에 사용할 목적으로 정보를 수집하는 절차다
- 웹 마이닝: 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낸다
- 웹 모니터링: 저작권이나 상표권이 침해되는 사례를 모니터링한다

### 크롤러의 복잡도

크롤러가 처리해야 하는 데이터의 규모와 크롤링 방법의 세밀함, 데이터 추출 방법에 따라 복잡도가 결정된다.

### 요구사항

- 확장성: 웹은 거대하므로, 병행성을 활용하면 효과적으로 크롤링할 수 있어야 한다
- 안정성: 웹은 함정으로 가득하다. 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크 등을 대응해야 한다
- 예절: 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다
- 확장성: 새로운 형태의 콘텐츠를 지원하기가 쉬워야 한다

### 개략적 규모 추정

- 매달 10억 개의 웹 페이지를 다운로드한다
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400페이지/초
- 최대(peek) QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 X 500k = 500TB/월
- 1개월치 데이터를 보관한다고 하면 500TB의 저장 용량이 필요하다
- 5년간 보관한다면 500TB X 12개월 X 5년 = 30PB의 저장 용량이 필요하다

## 2단계: 개략적 설계안 제시 및 동의 구하기

### 크롤러 동작 절차

#### 시작 URL 집합

웹 크롤러가 크롤링을 시작하는 출발점이다. 전체 웹을 두 개 공간으로 나눈다고 생각하자. 크롤링이 미처 안 된 공간, 크롤링이 끝난 공간이다.

#### 미수집 URL 저장소

대부분의 현대적 크롤러는 크롤링 상태를 두 가지로 나누어 관리한다. 다운로드할 URL을 저장 관리하는 컴포넌트를 미수집 URL 저장소라고 부른다. (URL frontier)

#### HTML 다운로더

HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트다. 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공한다.

#### 도메인 이름 변환기

웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요하다. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP 주소를 알아낸다.

#### 콘텐츠 파서

웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 한다. 이상한 웹 페이지는 문제를 일으킬 수 있고 저장 공간만 낭비하게 될 것이기 때문이다.

#### 중복 콘텐츠인가?

연구 결과에 따르면 29%에 달하는 웹 페이지 콘텐츠가 중복이다. 같은 콘텐츠를 여러 번 저장하는 일을 방지하기 위해, 이미 저장소에 보관된 콘텐츠인지 확인하는 자료 구조를 도입한다.

#### 콘텐츠 저장소

HTML 문서를 보관하는 시스템이다. 저장소를 구현하는 데 쓰일 기술을 고를 때는 데이터 유형, 데이터 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 한다.

#### URL 추출기

HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.

#### URL 필터

특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

#### 이미 방문한 URL?

이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료 구조를 사용한다. 블룸 필터나 해시 테이블이 널리 쓰인다.

#### URL 저장소

이미 방문한 URL을 보관하는 저장소다.

## 3단계: 상세 설계

### DFS vs BFS

웹은 유향 그래프와 같다. 페이지는 노드, 하이퍼링크는 에지다.

크롤링 프로세스는 이 유향 그래프를 에지를 따라 탐색하는 과정이다.

DFS는 좋은 선택이 아니다. 그래프 크기가 클 경우 어느 정도 깊숙이 가게 될지 가늠하기 어렵기 때문이다.

BFS가 일반적으로 쓰인다. BFS는 FIFO 큐를 사용하는데, 다운로드할 URL을 큐에 넣고 한 번에 하나씩 꺼내 처리한다.

하지만 BFS에도 두 가지 문제가 있다:

1. 한 페이지에서 나온 링크의 상당수는 같은 서버로 되돌아간다. 크롤러는 병렬적으로 동작하므로, 같은 호스트에 속한 많은 링크를 다운로드하느라 서버는 과부하 상태가 될 것이다
2. 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않는다. 그러나 모든 웹 페이지가 같은 수준의 품질, 중요성을 갖지는 않다

### 미수집 URL 저장소

미수집 URL 저장소를 잘 구현하면 예의(politeness), 우선순위, 신선도(freshness)를 보장할 수 있다.

#### 예의

웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내서는 안 된다. 무례한(impolite) 크롤러로 간주될 수 있기 때문이다.

일반적으로 채택하는 규칙은, 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것이다.

같은 웹 사이트의 페이지를 다운받는 태스크는 시간차를 두고 실행하도록 한다.

#### 우선순위

페이지 순위(PageRank), 트래픽 양, 갱신 빈도 등 다양한 척도에 따라 URL의 우선순위를 나눌 수 있다.

우선순위를 나누는 데는 순위결정장치(prioritizer)를 사용한다.

#### 신선도

웹 페이지는 수시로 추가되고, 삭제되고, 변경된다. 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집할 필요가 있다.

### HTML 다운로더

robots.txt는 웹사이트가 크롤러와 소통하는 표준적 방법이다. 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어 있다.

### 안정성

- 안정 해시: 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술이다
- 크롤링 상태 및 수집 데이터 저장: 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해 두는 것이 바람직하다
- 예외 처리: 예외가 발생해도 전체 시스템이 중단되는 일 없이 그 작업을 우아하게 이어나갈 수 있어야 한다
- 데이터 검증: 시스템 오류를 방지하기 위한 중요 수단 가운데 하나다

### 확장성

새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써서 설계해야 한다.

### 문제 있는 콘텐츠 감지 및 회피

- 중복 콘텐츠
- 거미 덫(spider trap): 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
- 데이터 노이즈: 광고나 스크립트 코드, 스팸 URL 등

## 4단계: 마무리

추가로 논의할 만한 사항들:

- 서버 측 렌더링: 동적으로 생성되는 링크를 처리하기 위해 서버 측 렌더링을 적용하기 전에 페이지를 파싱한다
- 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
