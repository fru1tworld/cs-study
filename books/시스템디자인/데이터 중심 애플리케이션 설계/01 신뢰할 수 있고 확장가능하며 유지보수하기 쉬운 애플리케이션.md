머리말

- 데이터 양, 데이터 복잡성, 데이터가 변하는 속도 등 데이터가 주요 도전 과제인 애플리케이션을 **데이터 중심적(data intensive)**이라고 말한다.
- 반대로 CPU 사이클이 병목인 경우 **계산 중심적(CPU intensive)**이라고 한다.

# 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션

일반적으로 데이터 중심 애플리케이션은 공통으로 필요로 하는 기능을 제공하는 표준 구성 요소로 만든다.
예를 들어 많은 애플리케이션은 다음을 필요로 한다.

- 구동 애플리케이션이나 다른 애플리케이션에서 나중에 다시 데이터를 찾을 수 있게 데이터를 저장(**데이터베이스**)
- 읽기 속도 향상을 위해 값비싼 수행 결과를 기억(**캐시**)
- 사용자가 키워드로 데이터를 검색하거나 다양한 방법으로 필터링할 수 있게 제공(**검색 색인**)
- 비동기 처리를 위해 다른 프로세스로 메시지 보내기(**스트림 처리**)
- 주기적으로 대량의 누적 데이터를 분석(**일괄 처리**)

너무 뻔한 말처럼 들린다면 **데이터 시스템**이 성공적으로 추상화됐기 때문이다.

그러나 현실은 그리 간단하지 않다 애플리케이션마다 요구사항이 다르기 때문에 데이터베이스 시스템 또한 저마다 다양한 특성을 가지고 있다.

캐싱을 위한 다양한 접근 방식과 검색 색인을 구축하는 여러 가지 방법 등이 있다.

애플리케이션을 만들 때 어떤 도구와 어떤 접근 방식이 수행 중인 작업에 가장 적합한지 생각해야 한다.

단 하나의 도구만으로 할 수 없는 것을 해야 하는 경우 도구들을 결합하기 어려울 수 있다.

이 책은 데이터 시스템의 원칙과 실용성 그리고 이를 활용한 데이터 중심 애플리케이션을 개발하는 방법을 모두 담고 있다.

이 책에서 소개된 다양한 도구가 공통으로 지닌 것은 무엇이고 서로 구별되는 것은 무엇인지, 그리고 어떻게 그러한 특성을 구현해냈는지 알아본다.

신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 데이터 시스템을 구축하기 위한 기초적인 노력을 살펴보는 것으로 이번 장을 시작한다.

신뢰성, 확장성, 유지보수성의 의미를 명확히 하고 이를 고려하는 몇 가지 방법을 개략적으로 설명하고 이후 장에서 필요한 기본 사항을 거듭 검토한다.

## 1. 데이터 시스템에 대한 생각

일반적으로 데이터베이스, 큐, 캐시 등을 매우 다른 범주에 속하는 도구로 생각한다.

데이터베이스와 메시지 큐는 표면적으로 비슷하더라도(둘 다 얼마 동안 데이터를 저장) 매우 다른 접근 패턴을 갖고 있어 서로 다른 성능 특성이 있기 때문에 구현 방식이 매우 다르다.

그러면 모든 것을 왜 데이터 시스템이라는 포괄적 용어로 묶어야 할까 ?

데이터 저장과 처리를 위한 여러 새로운 도구는 최근에 만들어졌다.

새로운 도구들은 다양한 사례(use case)에 최적화됐기 때문에 더 이상 전통적인 분류에 딱 들어맞지 않는다.

에를 들어 메시지 큐로 사용하는 데이터스토어(datastore)인 레디스(Redis)가 있고 데이터베이스처럼 지속성(durability)을 보장하는 메시지 큐인 아파치 카프카도 있다.
분류 간 경계가 흐려지고 있다.

두 번째로 점점 더 많은 애플리케이션이 단일 도구로는 더 이상 데이터 처리와 저장 모두를 만족시킬 수 없는 과도하고 광범위한 요구사항을 갖고 있다.

대신 작업(work)은 단일 도구에서 효율적으로 수행할 수 있는 태스크(task)로 나누고 다양한 도구들은 애플리케이션 코드를 이용해 서로 연결한다.

예를 들어 메인 데이터베이스와 분리된 애플리케이션 관리 캐시 계층(맴캐시디(Memcached)나 유사한 도구 사용)이나엘라스틱서치나 솔라같은 full-text 검색 버의 경우 메인 데이터베이스와 동기화된 캐시나 색인을 유지하는 것은 보통 애플리케이션 코드의 책임이다.

서비스 제공을 위해 각 도구를 결합할 때 서비스 인터페이스나 애플리케이션 프로그래밍 인터페이스는 보통 클라이언트가 모르게 구현 세부 사항을 숨긴다.

기본적으로 좀 더 작은 범용 구성 요소들로 새롭고 특수한 목적의 데이터 시스템을 만든다.

복합 데이터 시스템은 외부 클라이언트가 일관된 결과를 볼 수 있게끔 쓰기에서 캐시를 올바르게 무효화하거나 업데이트하는 등의 특정 보장 기능을 제공할 수 있다.

이제부터 개발자는 애플리케이션 개발자뿐만 아니라 데이터 시스템 설계자이기도 하다.

데이터 시스템이나 서비스를 설계할 때 까다로운 문제가 많다.

이 책에서는 대부분의 소프트웨어 시스템에서 중요하게 여기는 세 가지 관심사에 중점을 둔다.

**신뢰성**:
하드웨어나 소프트웨어 결함 심지어 인적 오류같은 역경에 직면하더라도 시스템은 지속적으로 올바르게 동작해야 한다.
**확장성**:
시스템의 데이터 양, 트래픽 양, 복잡도가 증가하면서 이를 처리할 수 있는 적절한 방법이 있어야 한다.
**유지보수성**:
시간이 지남에 따라 여러 다양한 사람들이 시스템 상에서 작업할 것이기 때문에 모든 사용자가 시스템 상에서 생산적으로 작업할 수 있어야 한다.

# 2. 신뢰성

소프트웨어의 경우 신뢰성에 대한 기대치는 다음과 같다.

- 애플리케이션은 사용자가 기대한 기능을 수행한다.
- 시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.
- 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.
- 시스템은 허가되지 않은 접근과 오남용을 방지한다.

이 모든 것이 올바르게 동작함을 의미하는 경우, 대략 무언가가 잘못되더라도 지속적으로 올바르게 동작함을 신뢰성의 의미로 이해할 수 있다.

잘못될 수 있는 일을 결함(fault)이라 부른다.

그리고 결함을 예측하고 대처할 수 있는 시스템을 내결함성(fault-tolerant)또는 탄련성(resilient)을 지녔다고 말한다.

내결함성이라는 용어는 약간 오해의 소지가 있는데 모든 종류의 결함을 견딜 수 있는 시스템을 만들 수 있음을 시시하지만, 실제로는 실현 가능하지 않다.

따라서 **특정 유형**의 결함 내성에 대해서만 이야기하는 것이 타당하다.

결함은 장애와 동일하지 않다. 일반적으로 결함은 사양에서 벗어난 시스템의 한 구성요소로 정의되지만

**장애**는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우다.

결함 확률은 0으로 줄이는 것은 불가능하다. 따라서 대개 결함으로 인해 장애가 발생하지 않게끔 내결함성 구조를 설계하는 것이 좋다.

반직관적이지만 이러한 내결함성 시스템에서 경고 없이 개별 프로세스를 무작위로 죽이는 것과 같이 고의로 결함을 일으켜 결함률을 증가시키는 방법은 납득할 만하다.

실제로 많은 중대한 버그는 미흡한 오류 처리에 기인한다.

고의적으로 결함을 유도함으로써 내결함성 시스템을 지속적으로 훈련하고 테스트해서 결함이 자연적으로 발생했을 때 올바르게 처리할 수 있다는 자신감을 높인다.

넷플릭스의 **카오스 몽키**가 이런 접근 방식의 한 예다.

일반적으로 결함 예방을 넘어 내결함성을 갖기 선호하지만 예방책이 해결책보다 더 좋은 경우가 있다.

바로 보안 문제다. 예를 들어 손상시키고 민감한 데이터에 대한 접근 권한을 얻는다면 이를 되돌릴 수 없다.

### 2.1. 하드웨어 결함

시스템 장애의 원인을 생각할 때 하드웨어 결함이 바로 떠오른다.

규모가 큰 데이터센터에서 일하는 사람은 많은 장비를 다룰 경우 이 같은 일은 늘상 일어난다고 말한다.

하드디스크의 평균 장애시간은 약 10~50년으로 보고됐다. 따라서 10,000개의 디스크로 구성된 저장 클러스터는 평균적으로 하루에 한 개의 디스크가 죽는다고 예상해야 한다.

시스템 장애율을 줄이기 위한 첫 번째 대응으로 각 하드웨어 구성 요소에 중복(redundancy)를 추가하는 방법이 일반적이다.

디스크는 RAID 구성으로 설치할 수 있고 서버는 이중 전원 디바이스와 핫 스왑 가능한 CPU를 데이터 센터는 건전지와 예비 전원 용 디젤 발전기를 갖출 수 있다.

구성 요소 하나가 죽으면 고장 난 구성 요소가 교체되는 동안 중복된 구성 요소를 대신 적용할 수 있다.

이러한 접근 방식은 하드웨어 문제로 장애가 발생하는 것을 완전히 막을 수는 없지만 이해하기 쉽고 보통 수년간 장비가 중단되지 않고 계속 동작할 수 있게 한다.

최근까지 단일 장비의 전체 장애는 매우 드물기 때문에 대부분의 애플리케이션은 하드웨어 구성 요소의 중복으로 충분했다.

새 장비에 백업을 매우 빠르게 복원할 수 있는 한 장애 발생 시 중단 시간은 대부분의 애플리케이션에 치명적이지 않다 따라서 다중 장비 중복은 고가용성이 절대적으로 필수적인 소수 애플리케이션만 필요했다.

하지만 데이터 양과 애플리케이션의 계싼 요구가 늘어나면서 더 많은 애플리케이션이 많은 수의 장비를 사용하게 됐고 이와 비례해 하드웨어 결함율도 증가했다.

또한 아마존 웹 서비스같은 일부 클라우드 플랫폼은 가상 장비 인스턴스가 별도의 경고 없이 사용할 수 없게 되는 상황이 상당히 일반적이다.

이런 플랫폼은 단일 장비 신뢰성보다 유연성과 탄력성을 우선적으로 처리하게끔 설계됐기 때문이다.

따라서 소프트웨어 내결함성 기술을 사용하거나 하드웨어 중복성을 추가해 전체 장비의 손실을 견딜 수 있는 시스템으로 점점 옮겨가고 있다.

단일 서버 시스템은 계획된 중단 시간이 필요하지만 장비 장애를 견딜 수 있는 시스템은 전체 시스템의 중단시간 없이 한 번에 한 노드씩 패치할 수 있다.

### 2.2. 소프트웨어 오류

보통 하드웨어 결함을 무작위적이고 서로 독립적이라 생각한다.

즉 한 장비의 디스크에 장애가 있다고 해서 다른 장비의 디스크에 정애가 발생하지 않는다는 뜻이다.

약하게 상관관계(예를 들어 서버 랙의 온도 같은 공통 원인)가 있을 수도 있다.

그렇지 않다면 다수의 하드웨어 구성 요소에 동시에 장애가 발생하지 않는다.

또 다른 부류의 결함으로 시스템 내 체계적 오류(systematic error)가 있다.

이 결함은 예상하기가 더 어렵고 노드 간 상관관계 때문에 상관관계 없는 하드웨어 결함보다 오히려 시스템 오류를 더욱 많이 유발하는 경향이 있다. 예를 들어 다음과 같다.

- 잘못된 특정 입력이 있을 때 모든 애플리케이션 서버 인스턴스가 죽는 소프트웨어 버그, 예를 들어 리눅스 커널의 버그로 인해 많은 애플리케이션이 일제히 멈춰버린 원인이 된 2012년 6월 30일 윤초를 생각해보자.
- CPU 시간, 메모리, 디스크 공간, 네트워크 대역폭처럼 공유 자원을 과도하게 사용하는 일부 프로세스
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 한 구성 요소의 작은 결함이 다른 구성 요소의 결함을 야기하고 차례차례 더 많은 결함이 발생하는 연쇄 장애

이 같은 소프트웨어 결함을 유발하는 버그는 특정 상황에 의해 발생하기 전까지 오랫동안 나타나지 않는다.

이런 상황으로 볼 때 소프트웨어에는 환경에 대한 일종의 가정이 있다는 사실을 알 수 있다.

소프트웨어의 체계적 오류 문제는 신속한 해결이 없다.

시스템의 가저오가 상호작용에 대해 주의 깊게 생각하기, 빈틈없는 테스트, 프로세스 격리, 죽은 프로세스의 재시작 허용, 프로덕션 환경에서 시스템 동작의 측정, 모니터링, 분석하기와 같은 여러 작은 인들이 문제해결에 도움을 줄 수 있다.

시스템이 뭔가를 보장하길 기대한다면(예를 들어 메시지 큐에 수신된 메시지 수와 송신된 메시지 수가 같다.) 수행 중에 이를 지속적으로 확인해 차이가 생기는 경우 경고를 발생시킬 수 있다.

### 2.3. 인적 오류

사람은 소프트웨어 시스템을 설계하고 구축하며, 운영자로서 시스템을 계속 운영한다.

이들이 최선의 의도를 갖고 있어도 사람은 미덥지 않다고 알려져 있다.

예를 들어 대규모 인터넷 서비스에 대한 연구에 따르면 운영자의 설정 오류가 중단의 주요 원인인 반면 하드웨어 결함은 중단 원인의 10~25% 정도에 그친다.

사람이 미덥지 않음에도 시스템을 어떻게 신뢰성 있게 만들까 ? 최고의 시스템은 다양한 접근 방식을 결합한다.

- 오류의 가능성을 최소화하는 방향으로 시스템을 설계하라. 예를 들어 잘 설계된 추상화 API, 관리 인터페이스를 사용하면 "옳은 일"은 쉽게하고 "잘못된 일"은 막을 수 있다. 하지만 인터페이스가 지나치게 제한적이면 사람들은 좋은 점을 잊은 채 제한된 인터페이스를 피해 작업한다. 따라서 이런 시스템 설계는 올바르게 작동하게끔 균형을 맞추기가 어렵다.
- 사람이 가장 많이 실수하는 장소에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리하다. 특히 실제 데이터를 사용해 안전하게 살펴보고 실험할 수 있지만 실제 사용자에게는 영향이 없는 비 프로덕션 샌드박스를 제공하라.
- 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트하라, 자동 테스트는 널리 사용되며 잘 알려져 있다. 특히 정상적인 동작에서는 거의 발생하지 않는 코너 케이스를 다루는 데 유용하다.
- 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게하라. 예를 들어 설정 변경 내역을 빠르게 롤백하고 새로운 코드를 서서히 롤 아웃하게 만들고 이전 계산이 잘못된 경우를 대비해 데이터 재계산 도구를 제공하라.
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련하라. 모니터링을 다른 엔지니어링 분야에서 원격 측정이라고 부른다. 모니터링은 조기에 경고 신호를 보내줄 수 있고 특정 가정이나 제한을 벗어나는지 확인할 수 있게 한다. 문제가 발생했을 때 지표는 문제를 분석하는 데 매우 중요하다.
- 조작 교육과 실습을 시행하라. 까다롭지만 매우 중요한 측면이다. 책의 범위를 벗어나므로 여기서 다루지는 않는다.

### 2.4. 신뢰성은 얼마나 중요할까 ?

신뢰성은 단순히 원자력 발전소와 항공 교통 관제 소프트웨어만을 위한 것이 아니다.

더 많은 수의 일상적인 애플리케이션도 안정적으로 작동해야 한다.

비즈니스 애플리케이션에서 버그는 생산성 저하의 원인이고 전자 상거래 사이트의 중단은 매출에 손실이 발생하고 명성에 타격을 준다는 면에서 많은 비용이 든다.

중요하지 않은 애플리케이션도 사용자에 대한 책임이 있다. 증명되지 않은 시장을 위해 시제품을 개발하는 비용이나 매우 작은 이익률의 서비스를 운영하는 비용을 줄이려 신뢰성을 희생해야 하는 상황이 있다.

하지만 이 경우에는 비용을 줄여야 하는 시점을 매우 잘 알고 있어야 한다.

## 3. 확장성

현재 안정적으로 동작한다고 해서 미래에도 안정적으로 동작한다는 보장은 없다.

성능 저하를 유발하는 흔한 이유 중 하나는 부하 증가다.

어쩌면 시스템의 동시 사용자 수가 1만 명에서 100만 명으로 증가했을 수도 있다.

확장성은 일차원적인 표식이 아님을 주의하자.

즉 X는 확장 가능하다 Y는 불가능하다는 의미가 없다.

오히려 확장성을 논한다는 것은 시스템으 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇인가?와 같은 질문을 고려한다는 의미다.

### 3.1. 부하 기술하기

무엇보다 시스템의 현재 부하를 간결하게 기술해야한다.

그래야 부하 성장 질문(두 배로 되면 어떻게 될까?)를 논의할 수 있다.

부하는 부하 매개변수라 부르는 몇 개의 숫자로 나타낼 수 있다.

(부하 매개변수는 시스템 설계에 따라 달라진다.)

단순히 초당 12,000건의 쓰기 처리는 쉽다.

하지만 확장성 문제는 주로 트윗 양이 아닌 팬 아웃 때문이다.

1. 팔로우하는 모든 사람을 찾고, 이 사람들의 모든 트윗을 찾아 시간순으로 정렬해서 합친다.

```SQL
SELECT tweets.*, users.* FROM tweets
    JOIN users ON tweets.sender_id = users.id
    JOIN fllows ON follows.followw_id = users.id
    WHERE follows.follower_id = current_user
```

2. 각 수신 사용자용 트윗 우편함처럼 개별 사용자의 홈 타임라인 캐시를 유지한다. 사용자가 트윗을 작성하면 해당 사용자를 팔로우하는 사람을 모두 찾고 팔로워 각자의 홈 타임라인 캐시에 새로운 트윗을 삽입한다. 그러면 홈 타임라인의 읽기 요청은 요청 결과를 미리 계산했기 때문에 비용이 저렴하다.

방식 2의 문제는 트윗 작성이 많은 부가 작업을 필요로 하고, 평균적으로 트윗이 약 75명의 팔로워에게 전달되므로 초당 4.6k 트윗은 홈 타임라인 캐시에 초당 345k 건의 쓰기가 된다.

그러나 일부 사용자는 팔로워가 3천만명이 넘는다.

그래서 단일 트윗이 홈 타임라인에 3천만 건 이상의 쓰기 요청이 될지도 모른다는 의미다.

결국 최종 진화는 두 접근 방식의 혼합형으로 바꾸고 있다.

## 3.2. 성능 기술하기

- 부하 매개변수를 증가시키고 시스템 자원은 변경하지 않고 유지하면 시스템 성능은 어떻게 영향을 받을까?
- 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까 ?

하둡 같은 일괄 처리 시스템은 보통 처리량이고,
온라인 시스템에서 더 중요한 사항은 서비스 응답 시간이다.

> 지연 시간과 응답시간
> 지연 시간과 응답시간은 같은 뜻으로 쓰이지만 동일하지 않다.
> 응답 시간은 클라이언트 관점에서 본 시각으로 요청을 처리하는 실제 시간 외에도 네트워크 지연과 큐 지연도 포함한다.
> 지연 시간은 요청이 처리되길 기다리는 시간으로 서비스를 기다리며 휴지(latent) 상태인 시간을 말한다.

그래서 클라이언트는 매번 요청할 때 응답 시간이 다를 수 있는데, 꽤 오래 걸리는 트이 값이 있다.
아마도 느린 요청은 더 많은 데이터를 처리하기 때문에 본질적으로 더 비쌀지도 모른다.

백그라운드 프로세스의 컨텍스트 스위치, 네트워크 패킷 손실과, TCP 재전송, 가비지 컬렉션 휴지, 디스크에서 읽기를 강제하는 페이지 폴트, 서버 랙의 기계적인 진동이나 다른 원인으로 추가 지연이 생길 수 있다.

그래서 일반적으로 서비스 "평균" 응답 속도를 살피는 것이 일반적이다.

대게 산술 평균을 쓰지만 평균은 좋은 지표가 아니다.

그래서 백분위를 사용하는게 더 좋다.

가장 빠른 시간부터 제일 느린 시간까지 정렬하면 중간 지점이 중앙값이 된다.

그래서 중앙값이 더 좋은 지표이다.

중앙값은 p50이라고 볼 수 있다.

특이값이 얼마나 좋지 않은지 알아보려면 상위 백분위를 살펴보는 것도 좋다.
p95, p99, p999 등이 있다.

**꼬리 지연 시간(tal latency)**으로 알려진 상위 백분위 응답 시간은 서비스의 사용자 경험에 직접 영향을 주기 때문에 중요하다.

예를 들어 아마존은 내부 서비스의 응답 시간 요구사항을 999p로 기술한다.

보통 느린 응답인 경우 물건을 많이 담아서 그런 경우이다.

한편 p9999는 최적화하는 작업에 비용이 너무 들어서 아마존이 추구하는 목표에 충분히 이익을 가져다 주지 못한다고 여겨진다.

**큐 대기 지연(큐잉 레이턴시)**은 높은 백분위에서 응답 시간의 상당 부분을 차지한다.

서버는 병렬로 소수의 작업만 처리할 수 있기 때문에 소수의 느린 요청은 후속 요청 처리가 지체된다.

이 현상을(HOL) 문제라고 한다. 그래서 클라이언트 쪽 응답 시간 측정이 중요하다.

## 3.3. 부하 대응 접근 방식

## 4. 유지 보수성

### 4.1 운용성: 운영의 편리함 만들기

시스템이 지속해서 원활하게 작동하려면 운영팀이 필수다.

좋은 운영팀은 일반적으로 다음과 같은 작업 등을 책임진다.

- 시스템 상태를 모니터링하고 상태가 좋지 않다면 빠르게 서비스를 복원
- 시스템 장애, 성능 저하 등의 문제의 원인을 추적
- 보안 패치를 포함해 소프트웨어와 플랫폼을 최신 상태로 유지
- 다른 시스템이 서로 어떻게 영향을 주는지 확인해 문제가 생길 수 있는 변경 사항을 손상을 입히기 전에 차단
- 미래에 발생 가능한 문제를 예측해 문제가 발생하기 전에 해결
- 배포, 설정 관리 등을 위한 모범 사례와 도구를 마련
- 애플리케이션을 특정 플랫폼에서 다른 플랫폼으로 이동하는 등 복잡한 유지보수 태스크를 수행
- 설정 변경으로 생기는 시스템 보안 유지보수
- 예측 가능한 운영과 안정적인 서비스 환경을 유지하기 위핸 절차 정의
- 개인 인사 이동에도 시스템에 대한 조직의 지식을 보존함

좋은 운영성이란 동일하게 반복되는 태스크를 쉽게 수행하게끔 만들어 운영팀이 고부가가치 활동에 노력을 집중한다는 의미다.

### 4.2 단순성: 복잡도 관리

코드 레벨

### 4.3 발전성: 변화를 쉽게 만들기

조직 측면에서 애자일 작업 패턴은 변화에 적응하기 좋은 프레임워크를 제공한다.
