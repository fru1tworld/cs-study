# 15 쿠버네티스 프로덕션 운영

## 클러스터 아키텍처

### 고가용성 Control Plane

```
┌────────────────────────────────────┐
│       Load Balancer (API)          │
└────────┬──────────┬─────────┬──────┘
         │          │         │
    ┌────▼───┐ ┌────▼───┐ ┌───▼────┐
    │Master 1│ │Master 2│ │Master 3│
    ├────────┤ ├────────┤ ├────────┤
    │  etcd  │ │  etcd  │ │  etcd  │
    └────────┘ └────────┘ └────────┘
```

### etcd 백업

```bash
# 스냅샷 생성
ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# 스냅샷 복원
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir=/var/lib/etcd-restore
```

## 클러스터 업그레이드

### kubeadm 업그레이드

```bash
# Control Plane 업그레이드
# 1. kubeadm 업그레이드
apt-mark unhold kubeadm && \
apt-get update && apt-get install -y kubeadm=1.28.0-00 && \
apt-mark hold kubeadm

# 2. 업그레이드 계획 확인
kubeadm upgrade plan

# 3. 업그레이드 적용
kubeadm upgrade apply v1.28.0

# 4. kubelet과 kubectl 업그레이드
apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet=1.28.0-00 kubectl=1.28.0-00 && \
apt-mark hold kubelet kubectl

# 5. kubelet 재시작
systemctl daemon-reload
systemctl restart kubelet
```

### Worker Node 업그레이드

```bash
# 1. 노드 드레인
kubectl drain <node-name> --ignore-daemonsets

# 2. 노드에서 kubeadm, kubelet 업그레이드
kubeadm upgrade node

# 3. 노드 다시 스케줄 가능하게
kubectl uncordon <node-name>
```

## 리소스 관리

### Cluster Autoscaler

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.28.0
          name: cluster-autoscaler
          command:
            - ./cluster-autoscaler
            - --v=4
            - --cloud-provider=aws
            - --nodes=1:10:my-asg
```

### Horizontal Pod Autoscaler (HPA)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
```

### Vertical Pod Autoscaler (VPA)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: app
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: 2
          memory: 2Gi
```

## 모니터링과 로깅

### Prometheus 구성

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s

    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

### Grafana 대시보드

- **클러스터 오버뷰**: 노드, 파드, CPU, 메모리
- **Node Exporter**: 노드 레벨 메트릭
- **kube-state-metrics**: 쿠버네티스 리소스 상태
- **애플리케이션 메트릭**: 커스텀 메트릭

### ELK Stack (Elasticsearch, Logstash, Kibana)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: elasticsearch
spec:
  containers:
    - name: elasticsearch
      image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
      env:
        - name: discovery.type
          value: single-node
      resources:
        limits:
          memory: 2Gi
```

### Fluentd DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
          env:
            - name: FLUENT_ELASTICSEARCH_HOST
              value: "elasticsearch"
            - name: FLUENT_ELASTICSEARCH_PORT
              value: "9200"
          volumeMounts:
            - name: varlog
              mountPath: /var/log
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
```

## 재해 복구

### Velero

```bash
# Velero 설치
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket my-backup-bucket \
  --backup-location-config region=us-west-2 \
  --snapshot-location-config region=us-west-2 \
  --secret-file ./credentials-velero

# 전체 백업
velero backup create full-backup

# 네임스페이스 백업
velero backup create ns-backup --include-namespaces production

# 특정 리소스 백업
velero backup create app-backup --selector app=myapp

# 백업 복원
velero restore create --from-backup full-backup

# 백업 스케줄
velero schedule create daily-backup --schedule="@daily"
```

## 비용 최적화

### Spot/Preemptible Instances

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-job
spec:
  replicas: 10
  template:
    spec:
      nodeSelector:
        node-type: spot
      tolerations:
        - key: "spot"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      containers:
        - name: worker
          image: batch-worker:1.0
```

### Resource Quotas와 Limit Ranges

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: dev
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
```

### Kubecost

- 비용 가시성
- 리소스 할당 추적
- 최적화 권장사항

## 보안 강화

### Pod Security Admission

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### Falco (런타임 보안)

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: falco
  namespace: falco
spec:
  selector:
    matchLabels:
      app: falco
  template:
    metadata:
      labels:
        app: falco
    spec:
      containers:
        - name: falco
          image: falcosecurity/falco:0.36.0
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /host/var/run/docker.sock
              name: docker-socket
      volumes:
        - name: docker-socket
          hostPath:
            path: /var/run/docker.sock
```

## 멀티 클러스터 관리

### Cluster API

```yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: production
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.244.0.0/16
    services:
      cidrBlocks:
        - 10.96.0.0/12
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    name: production
```

### Federation (KubeFed)

```yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: myapp
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: myapp
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: myapp
      template:
        metadata:
          labels:
            app: myapp
        spec:
          containers:
            - name: app
              image: myapp:1.0
  placement:
    clusters:
      - name: cluster1
      - name: cluster2
  overrides:
    - clusterName: cluster1
      clusterOverrides:
        - path: "/spec/replicas"
          value: 5
```

## 운영 체크리스트

### 배포 전

- [ ] 리소스 requests/limits 설정
- [ ] Liveness/Readiness Probes 설정
- [ ] SecurityContext 설정
- [ ] RBAC 권한 최소화
- [ ] 이미지 태그 명시 (latest 사용 금지)
- [ ] 백업 전략 수립

### 배포 후

- [ ] 모니터링 대시보드 확인
- [ ] 로그 수집 확인
- [ ] 알림 설정
- [ ] HPA 동작 확인
- [ ] 네트워크 정책 검증

### 정기 점검

- [ ] etcd 백업 검증
- [ ] 인증서 만료 확인
- [ ] 리소스 사용량 모니터링
- [ ] 보안 패치 적용
- [ ] 권한 검토

## 교훈

1. **고가용성 Control Plane 구성**
2. **정기적인 백업과 복구 테스트**
3. **Autoscaling으로 리소스 효율화**
4. **종합적인 모니터링과 로깅**
5. **재해 복구 계획 수립**
6. **비용 최적화 지속 수행**
7. **보안을 최우선으로**
8. **운영 프로세스 문서화**
