# 사례

서비스 신뢰성 계층 구조

1. 제품
2. 개발
3. 수용 계획
4. 테스트 및 릴리즈 절차
5. 포스트모텀/주요 원인 분석
6. 장애 대응
7. 모니터링

## 모니터링

모니티링이 없다면 서비스가 동작하는지 알 수 있는 방법이 없다.
견고하게 디자인된 모니터링 인프라가 없다면 그저 눈 먼 장님일 뿐이다.

## 장애 대응

SRE가 순전히 긴급 대응으만으로 목적으로 대기하는 경우는 드물다.

긴급 대응 지원은 좀 더 큰 미션을 달성하고 분산 시스템이 실제로 어떻게 동작하는지를 살펴보기 위한 도구다.

서류 뭉치를 들고 다닐 필요가 없는 방법이 있다면 당연히 그래야 하지 않겠는가 ?

## 포스트모텀과 주요 원인 분석

우리는 서비스에서 발생하는 문제들중 새롭고 관심이 있는 것들에 대해서만 알림을 받고 이를 직접 해결하는 것을 목적으로 한다.

계속해서 반복되는 똑같은 이슈를 해결하는 것은 미칠 듯이 지겨운 일이다.

포스트모텀(회고) 문화는 어떤 부분에 문제가 있었는지를 파악하는 첫 걸음이다.

## 테스트

어느 부분에서 오동작이 발생했는지를 이해했다면 그 다음 단계는 같은 현상이 재현되는 것을 방지하는 것이다.

그 이유는 잘못된 동작을 고치는 것보다는 그 동작을 미연에 방지하는 것이 훨씬 가치 있는 일이기 때문이다.

테트트 도구들을 이용하면 제품을 프로덕션 환경에 릴리즈하기 전에, 소프트웨어에서 어떤 종류의 오류들이 발생할 수 있는지를 먼저 확인할 수 있다.

## 수용 계획

SRE 조직의 소프트웨어 엔지니어링에는 SRE의 소프트웨어 엔지니링링어리에 대한 사례로서 자동화된 수용 계획 도구인 Auxon을 개발했던 이야기를 한다

## 개발

구글의 사이트 신뢰성 엔지니어링의 핵심 가치는 조직 내에서 타의 추종을 불허하는 대용량 시스템 디자인과 소프트웨어 엔지니어링을 수행하고 있다는 점이다.
제 23장 신뢰성을 위한 분산에 대한 합의에서는 구글이 다양한 분산 시스템의 핵심을 이루는 조화로운 분산에 대해 설명한다.
전 세계에 걸쳐 분산된 크론 시스템에 대해서도 함께 소개한다.
제 24장 Cron을 활용한 분산된 주기적 스케줄링

## 제품

마지막으로 신뢰성 계층 구조의 피라미드의 정점에는 실제로 동작하는 제품이 동작한다.
27장 스케일링을 지원하는 신뢰성 있는 제품 출시에서는 사용자에게 최상의 경험을 제공하기 위해 스케일링을 지원하는 신뢰성 있는 제품을 출시하기 위해 구글이 어떤 노력을 기울이는지 처음부터 끝까지 소개한다.

# 10 시계열 데이터에 대한 실용적인 알림

구글의 모니터링 시스템은 부하가 크지 않은 유럽 웹 서버의 평균 응답 시간 따위의 지표를 측정하는 시스템이 아니다.
한 지역의 전체 웹 서버에 대한 응답 시간이 어떻게 분산되는지를 이해해야 하기 때문에 지연응답의 증가를 유발하는 요인들을 정확히 파악할 수 있다.

보그몬의 탄생
애플리케이션의 조작
내보낸 데이터의 수집
시계열 데이터를 위한 저장소
규칙의 평가
알림
모니터링 토폴리지의 샤딩
블랙박스 모니터링
설정의 유지보수

# 11 비상 대기

온콜은 많은 운영팀 및 엔지니어링팀이 서비스의 신뢰성과 가용성을 위해 반드시 수행해야 할 중요한 임무다.

소개
구글은 성능과 신뢰성을 책임지는 전담 SRE가 있다.

그래서 SRE들은 서비스를 위한 비상 대기 업무를 수행한다.

SRE 팀은 순수한 운영팀과는 사뭇 달라서 문제를 해결하기 위한 엔지니어링 접근법을 강조한다.

이 문제들은 대부분 운영과 관련있지만 스케일링과 관련해서 소프트웨어 엔지니어링 솔루션 없니는 다루기 어려운 것들이다.

## 비상 대기 엔지니어의 삶

이 절에서는 통상 비상 대기 엔지니어가 어떤 활동을 하는지에 대한 설명과 더불어 이 장의 나머지 부분에 대한 약간의 배경지식을 제공하고자 한다.

비상 대기 엔지니어는 프로덕션 시스템의 보호자로서 팀에 영향을 미치는 장애를 관리하고 프로뎍선 환경의 변경을 추진 및 진단하는 등 할애된 운영 업무를 수행한다.

장애에 대한 대응 시간은 서비스의 가용성에 따라 다르지만 규칙은 간단하다 사용자에게 노출되는 서비스의 경우에는 분기별로 99.99%의 가용성을 반드시 확보해야 한다.

SLO가 비교적 낮은 시스템의 경우에는 대응 시간을 10분 단위로 정의할 수 있다.

## 비상 대기 업무의 균형 맞추기

SRE팀에는 비상 대기 업무에 편성될 경우 업무의 양과 품질에 대한 상세한 제약이 있다.

비상 대기 업무의 양은 엔지니어가 비상 대기 업무에 할애한 시간의 백분율로 계산한다.

그리고 비상 대기 품질은 비상 대기 업무 기간 동안 발생한 장애의 수로 계산될 수 있다.

### 업무 양의 균형

우리는 SRE의 E가 조직의 특성을 정의한다고 믿고 있으므로 최소 50%를 투자해야한다.
그래서 25% 정도를 비상 대기에 사용하고 나머지는 프로젝트가 아닌 다른 운영 업무에 할애한다.
25%의 비상 대기 규칙 덕분에 최소한 SRE의 인력으로 24/7 대기할 수 있다.

### 품질의 균형

포스트 모텀 작성, 버그 수정과 같은 후속

## 안전에 대해 고려하기

최근 연구결과에 따르면 사람은 자신이 직면한 도전에 대해 크게 두 가지 방향으로 생각하는 것으로 밝혀졌다.

- 직관적 자동화적 그리고 신속한 대응
- 합리적, 집중적, 그리고 계획적이며 경험에 기반한 행위

복잡한 시스템의 장애는 두 번째 방법이 더 나은 결과를 도출해내며 계획에 따른 장애 조치가 가능하다.

그래서 후자의 마음으로 적절하게 제어할 수 있도록 하기 위해서는 비상 대기와 관련된 스트레스를 줄여주는 것이 중요하다.

직감에 의한 대응은 미덕이지만 단점도 존재한다. 감이 틀렸을 수 있을 뿐 아니라 명확한 데이터에 근거한 지원이 제대로 이루어지지 않을 수 있기 때문이다.

장애를 처리 한후에는 반드시 포스트모텀 문서에 사건에 집중해서 작성된 포스트모텀 문서는 큰 가치를 제공한다.

이 문서는 누군가를 비난하는 것이 아니라, 프로덕션 환경의 장애를 시스템적으로 분석함으로써 그 가치를 이끌어낸다. 실수는 일어나게 마련이다.

## 부적절한 운영 부하에서 벗어나기

### 운영부하

큰 팀에 경력 있는 SRE를 임시로 보내면 팀의 숨통이 트이고 이슈를 해결하는 데 속도를 낼 수 있게 된다.

이상적으로는 운영 업무 부담의 증가에 대한 증상을 측정할 수 있어서 그 목표치가 정량화될 수 있어야 한다.

구글은 연간 장애 복구 대회를 열어 이론과 실전 지식을 모두 투입하여 며칠에 걸쳐 인프라스트럭처 시스템과 개별 시스템에 대한 테스트를 수행하곤 한다.

# 12 효과적인 장애 조치

장애 조치에 대한 연습은 크게 두 가지 요소와 관련 있는데 하나는 범용적으로 수행하는 방법에 대한 이해고 두 번째는 시스템에 대한 탄탄한 이해다. 시스템에 대한 지식은 해당 시스템에 익숙하지 않은 SRE의 효율성에 제한을 가하는 경향이 있다.

시스템이 어떻게 디자인되고 만들어졌는지에 대해 그다지 많이 알지 못하는 상태이기 때문이다.

## 이론

통상 애플리케이션의 장애 조치 방법은 가설 연역 방법이라고 생각하면 된다.
즉, 시스템을 관찰한 결과와 시스템의 행동에 대한 이해를 바탕으로 한 이론적 기반을 토대로 장애의 잠재적 원인에 대해 계속해서 가설을 세워나가고 이 가설을 시험하는 방법이다.

장애 조치 과정
문제 보고 -> 장애 등급 선정 -> 장애 분석 -> 장애 진단 -> 테스트 및 조치 -> 장애 처리

그런 다음 두 가지 방법으로 테스트를 해볼 수 있다.

관찰한 시스템의 상태를 이론과 비교하여 정황들이 들어맞는지 그렇지 않은지 판단하거나, 아니면 적극적으로 시스템을 고쳐보고 그 결과를 다시 관찰할 수 있다.
아니면 시스템의 상태에 대한 이해도와 보고된 문제에 대한 가능한 원인들을 다시 한번 되돌아볼 수 있는 계기가 되기도 한다.

어떤 방법을 사용하든 근본 원인이 발견될 때까지는 계속해서 테스트를 수행하고 원인이 발견되면 장애 재발을 방지하기 위해 그에 대한 조치를 수행한 후 포스트모텀 문서를 작성한다.

통상적인 문제
비효율적인 장애 조치 세션은 장애 등급 선정, 분석 및 진단 단계에 영향을 미친다.
이들 중 대부분은 시스템에 대한 깊은 이해가 부족한 데서 발생한다.

- 관련이 없는 증상을 들여보거나 시스템의 지표의 의미를 잘못 이해하는 경우 멍청하게 결과만 쫓는 행동일 뿐이다.
- 시스템의 변경이나 입력 값 혹은 환경에 대한 잘못된 이해는 안전하고 효과적인 가설의 검증에 방해가 된다.
- 장애 원인에 대한 가능성이 희박한 가설을 세우거나 과거에 발생한 문제의 원인과 결부시켜 한 번 발생한 문제는 다시 발행할 것이라고 결부해버리는 행위

연관 현상은 원인이 아니다.

## 실전에 들어가보자

모든 문제 해결은 문제에 대한 보고에서부터 시작한다.

효과적인 문제 보고는 실제로 기대한 동작은 무엇인지, 그리고 현재 어떻게 동작하고 있는지, 더불어 가능하다면 문제가 되는 동작을 어떻게 재현할 수 있는지를 설명하고 있어야 한다.

### 문제의 우선순위 판단

서비스의 전체적인 장애를 유발하면 모두가 참여해야겠지만 한 사람의 사용자에게만 발생한 문제인 경우 그럴 필요가 없을 수 있다.

숙련된 파일럿은 긴급 상황에서 자신의 최우선 과제는 비행기를 계속 비행하게 하는 것이고, 그 후 비행기와 그 외 모든 것들을 안전하게 지상에 착륙시키는 것이다.

### 단순화하기와 범위를 좁히기

시스템을 각 계층별로 분할정복 기법을 활용하여 문제를 해결한다.

반대로 너무 거대한 경우 시스템을 반으로 나누어 각 컴포넌트 사이의 통신 경로를 활용한다

### '무엇이', '어디서', '왜'를 고민하기

오동작 중인 시스템은 원하는 동작 이외의 어떤 다른 동작을 계속해서 실행하고 있을 가능성이 크다.

### 가장 마지막으로 수정된 부분에 주목하기

시스템은 관성이 있다.
그래서 무엇이 잘못되고 있는지 파악하기 위한 가장 좋은 시작 지점은 가장 최근에 변경된 부분이다.

## 테스트와 조치

가능한 원인들을 파악했다면 이들 중 어떤게 근본 원인인지 파악한다

- 이상적인 테스트는 상호 배타적이어서 가설의 어느 한 집합을 검증함으로써 다른 가설의 가능성이 없음을 밝혀낼 수 있어야 한다. 실제로는 이런 테스트를 구성하기란 굉장히 어렵다.
- 가장 명확한 것은 최우선으로 고려해야 한다. 가능성이 큰 테스트부터 순차적으로 진행하면서 테스트로 인해 시스템이 발생할 수 있는 위험에 대해서도 고려해야 한다.
- 혼란 요소로 인해 특정 실험이 잘못된 결과를 도출하게 될 수 있다. 예를 들어 방화벽 정책에 의해 특정 IP 주소로부터의 요청에만 응답하도록 적용되어 있어 애플리케이션 로직 서버 머신에서는 데이터베이스 서버에 성공적으로 핑을 할 수 있는 반면, 독자의 머신에서는 실패하는 상황이 발생할 수도 있다.
- 적극적인 테스트가 나중에 실행할 테스트의 결과에 부작용을 초래할 수도 있다. 예를 들어 프로세스가 더 많은 CPU를 사용할 수 있게 설정하면 작업은 더 빨리 끝낼 수 있지만 데이터가 경쟁 상태에 놓이게 될 가능성 역시 커진다.
- 일부 테스트는 설득력이 떨어질 수 있다. 정기적이면서 반복 가능한 형태로 경쟁 상태나 데드락이 발생한 상황을 인위적으로 만들어내기란 매우 어려우므로 이들이 문제의 원인임을 증명하기에는 비교적 불확실한 증거임에도 그것에 만족해야 할 수도 있다.

## 부정적인 결과의 마법

부정적인 결과는 기대한 효과가 나타나지 않은 경우를 의미하는 경험의 산물이다.
즉 계획한 대로 되지 않은 모든 실험을 의미한다. 새로운 디자인, 경험에 의한 학습 등을 통해 시스템을 개선하고자 했으나, 의도대로 되지 않은 경우가 이에 해당한다.

부정적인 결과로 끝난 실험 역시 결론이다.
이런 결과들을 통해 프로덕션 환경이나 디자인 공간 혹은 기존 시스템의 성능 한계 등을 확실히 알 수 있다.

또한 다른 사람들이 자신의 실험이나 디자인이 가치가 있는 것인지를 확인할 수 있는 계기가 되기도 한다.

예를 들어 어떤 개발팀이 특정 웹 서버가 8,000개의 연결을 처리해야 하는데, 잠금 현상 때문에 800개 연결밖에 못 한다면 800개 이하도 충분한지, 아니면 잠금 문제가 해결되었는지를 빠르게 살펴보고 결정을 내릴 수 있다.

도구와 방법은 실험의 결과와는 무관하며 향후의 작업에 대한 단서가 된다.

부정적인 결과를 공표하는 것은 업계의 데이터 주도 성향을 증진시킨다.

## 시스템에 문제가 생기면 어떻게 해야 할까?

가장 먼저 당황하지 말아야 한다.
당신은 혼자가 아니고 하늘이 무너지고 있는 것도 아니다.

## 테스트로 인한 장애

구글은 재난과 긴급 상황에 대한 사전적 테스트 접근법을 취하고 있다.
SRE들은 시스템에 장애를 일으킨 후 이로 인해 시스템이 어떻게 실패하는지 지켜본 다음, 그 현상이 반복해서 발생하지 않도록 신뢰성을 향상시킬 수 있는 변경 사항을 적용한다.

대부분의 경우 장애는 계획에 따라 제어가 가능하며 대상 시스템과 그에 의존하는 시스템들은 대체로 예상했던 대로 동작한다.
이 과정에서 어떤 취약점이나 숨겨진 의존성을 발견하면 해당 결함을 수정할 수 있도록 후속 조치 문서에 기록해둔다.

**카오스 엔지니어링(Chaos Engineering)**: 이러한 접근법을 체계화한 방법론으로, 프로덕션 환경에서 통제된 실험을 통해 시스템의 약점을 찾아낸다.

## 변경으로 인한 장애

구글은 어마어마한 수의 설정을 관리하고 있으며 지속적으로 변경이 발생하고 있다.

어느 날 악의적 사용을 방지하기 위해 인프라의 설정을 변경/적용했다. 이 인프라는 기본적으로 외부로 노출되는 모든 서비스들과 함께 동작하는 것이었고, 변경된 설정은 크래시 루프를 유발했다.

**변경 관리의 중요성**: 모든 변경 사항은 점진적 롤아웃(gradual rollout), 카나리 배포(canary deployment), 롤백 계획 등을 통해 관리되어야 한다.

## 절차에 의한 장애

절차나 프로세스 자체의 결함으로 인한 장애도 발생할 수 있다. 예를 들어, 잘못 문서화된 복구 절차나 오래된 런북(runbook)을 따르다가 오히려 상황을 악화시키는 경우가 있다.

## 모든 문제가 해결되었다

장애가 해결된 후에도 작업은 끝나지 않는다. 근본 원인 분석과 재발 방지 대책 수립이 필요하다.

## 지난 일로부터 배우기 그리고 반복하지 않기

- **장애에 대한 기록을 남기자**: 상세한 타임라인, 영향 범위, 조치 내용을 문서화한다.
- **커다란, 어쩌면 불가능할지도 모를 것에 대한 질문을 던지자**: "만일 ...라면?"이라는 질문으로 worst-case 시나리오를 검토한다.
- **사전 테스트 장려하기**: 정기적인 재해 복구 훈련(disaster recovery drill)과 게임데이(game day) 실시
- **실제로 장애가 발생하면 이론과 실제가 양립하게 된다**: 실전 경험을 통해 이론적 지식을 검증한다.

**결론**: 장애를 대응하는 사람은 우선 침착해야 한다.
그리고 필요하다면 다른 사람들에게 도움을 요청할 수 있어야 한다.
또한 이전에 발생한 장애를 연구하고 이로부터 새로운 것을 학습해야 한다.
그런 다음에는 시스템이 이런 종류의 장애에 더욱 잘 대처할 수 있도록 개선해야 한다.

# 14. 장애 관리하기

## 미흡한 장애 관리

장애 관리가 미흡할 경우 발생하는 문제점:

- 책임 소재가 불분명하여 초기 대응 지연
- 의사소통 부재로 인한 중복 작업 또는 상충되는 조치
- 우선순위 설정 실패로 2차 피해 발생
- 문서화 부족으로 동일한 장애 반복

## 미흡한 장애 처리에 대한 자세한 분석

**일반적인 실패 패턴**:

- 명확한 리더십과 역할 분담 부재
- 커뮤니케이션 채널 혼란
- 근거 없는 가정에 기반한 조치
- 장애의 영향 범위에 대한 과소평가
- 시간 압박으로 인한 성급한 판단

## 장애 관리 절차의 기본 요소들

1. **장애 사령관(Incident Commander)**: 전체 대응을 조율하고 의사결정을 내림
2. **커뮤니케이션 담당자**: 이해관계자들에게 상황을 전달
3. **운영 담당자**: 실제 복구 작업 수행
4. **명확한 에스컬레이션 절차**: 언제, 누구에게 도움을 요청할지 정의
5. **전용 커뮤니케이션 채널**: 장애 대응 전용 채팅방/회의실
6. **상태 페이지**: 사용자와 이해관계자를 위한 실시간 정보 제공

## 적절하게 관리한 장애 조치

**성공적인 장애 대응 사례**:

- 즉각적인 장애 선언과 역할 할당
- 체계적인 문제 진단과 가설 검증
- 단계적 복구와 검증
- 실시간 문서화 및 타임라인 기록
- 명확한 내부/외부 커뮤니케이션
- 사후 철저한 포스트모텀 진행

## 언제 장애를 선언할 것인가?

**장애 선언 기준**:

- SLA/SLO 위반이 발생하거나 임박한 경우
- 사용자에게 명확한 영향이 있는 경우
- 데이터 손실 위험이 있는 경우
- 보안 침해 의심 상황
- 불확실하지만 잠재적 영향이 큰 경우

**조기 선언의 이점**: 필요 없는 장애였다면 취소하면 되지만, 늦은 선언은 피해를 키운다.

# 15. 포스트모텀 문화: 실패로부터 배우기

## 장애 조치에 대한 모범 사례

### 우선순위 우선

출혈을 막고 서비스를 되살린 후에 근본 원인에 대한 증거를 찾자.
**Mitigation first, root cause later**: 먼저 완화하고, 나중에 근본 원인을 파악한다.

### 사전 준비

장애 조치에 참여한 사람들의 자문을 받아 장애 관리 절차를 미리 개발하고 문서화해두자.

- 런북(Runbook) 작성 및 정기 업데이트
- 장애 대응 체크리스트 준비
- 비상 연락망 최신화

### 신뢰

장애 조치에 참여 중인 모든 사람들에게 충분한 자율권을 보장하자.

- Blameless culture(비난 없는 문화) 구축
- 신속한 의사결정을 위한 권한 위임

### 감정 조절

장애를 조치하는 동안 스스로의 감정적 상태에 주의를 기울이자. 만일 너무 부담되면 다른 이에게 도움을 청하자.

- 번아웃 방지를 위한 교대 근무
- 스트레스 관리 기법 활용

### 대체 방안에 대한 모색

주기적으로 현재 선택할 수 있는 방법에 대해 다시 생각하고 이 방법이 여전히 유효한지, 아니면 다른 방법을 찾아야 하는지를 판단하자.

- 고정관념에서 벗어나기
- 다양한 관점 수용

### 실습

이 과정을 정기적으로 수행해서 자연스럽게 활용할 수 있는 수준으로 만들자.

- 분기별 재해 복구 훈련(Disaster Recovery Drill)
- 월간 게임데이(Game Day) 실시
- 장애 시뮬레이션 연습

### 개선

그리고 계속해서 개선하자. 모든 팀 구성원들이 모든 역할에 익숙해질 수 있도록 독려하자.

- 포스트모텀에서 도출된 액션 아이템 추적
- 반복되는 문제에 대한 자동화
- 지식 공유 세션 정례화

## 포스트모텀의 핵심 요소

1. **비난 없는(Blameless) 문화**: 개인이 아닌 시스템과 프로세스에 초점
2. **상세한 타임라인**: 모든 이벤트와 조치의 시간 기록
3. **근본 원인 분석**: 5 Whys, Fishbone Diagram 등 활용
4. **액션 아이템**: 구체적이고 책임자가 명확한 개선 과제
5. **공유와 학습**: 조직 전체가 배울 수 있도록 공개

# 16 시스템 중단 추적하기

## 에스컬레이터

구글에서는 SRE를 위한 모든 알림은 사람이 알람의 수신을 확인했는지 여부를 추적하는 중앙 응답 시스템을 공유하고 있다.
설정된 시간이 지나도 확인을 안하면 알림을 격상한다 이를 에스컬레이터라고 한다. 이 기능 덕분에 사용자의 반응이 없어도 기존의 업무 흐름에 통합될 수 있다.

## 아우터레이터

개별적 알림의 격상뿐만 아니라 시스템 중단 장애까지도 처리할 수 있는 시스템이다.
아우터레이터의 사용자는 여러 큐에 보관된 알림을 시간 별로 한 번에 확인할 수 있다.

# 17. 신뢰성을 위한 테스트

소프트웨어의 신뢰성을 보장하기 위해서는 체계적인 테스트 전략이 필수적이다. 구글 SRE는 테스트를 통해 시스템의 안정성을 검증하고, 장애를 사전에 예방하며, 변경 사항이 안전하게 배포될 수 있도록 보장한다.

## 소프트웨어 테스트의 종류

### 단위 테스트 (Unit Test)

- **목적**: 개별 함수나 메서드의 정확성 검증
- **특징**:
  - 실행 속도가 매우 빠름 (밀리초 단위)
  - 격리된 환경에서 실행
  - 의존성은 모킹(mocking) 처리
  - 테스트 피라미드의 기반을 구성 (가장 많은 수)
- **예시**: 데이터 파싱 함수, 계산 로직, 유효성 검증 로직
- **모범 사례**: 코드 커버리지 80% 이상 목표

### 통합 테스트 (Integration Test)

- **목적**: 여러 컴포넌트 간의 상호작용 검증
- **특징**:
  - 실제 의존성을 사용 (데이터베이스, 외부 API 등)
  - 단위 테스트보다 느리지만 더 현실적
  - 인터페이스와 계약(contract) 검증
- **예시**:
  - 데이터베이스 CRUD 작업
  - 마이크로서비스 간 통신
  - 외부 API 연동
- **주의사항**: 테스트 데이터 관리와 환경 격리 중요

### 시스템 테스트 (System Test) / E2E 테스트

- **목적**: 전체 시스템의 엔드투엔드 동작 검증
- **특징**:
  - 실제 사용자 시나리오 재현
  - 모든 컴포넌트가 통합된 상태에서 테스트
  - 실행 시간이 가장 오래 걸림 (분~시간 단위)
- **예시**:
  - 사용자 회원가입부터 구매까지의 전체 플로우
  - 주문 처리 전체 프로세스
- **도구**: Selenium, Cypress, Playwright

### 성능 테스트 (Performance Test)

- **부하 테스트 (Load Test)**:
  - 예상되는 정상 부하에서의 시스템 동작 검증
  - 응답 시간, 처리량(throughput) 측정
- **스트레스 테스트 (Stress Test)**:
  - 시스템의 한계점 파악
  - 어느 지점에서 시스템이 실패하는지 확인
- **스파이크 테스트 (Spike Test)**:
  - 급격한 부하 증가 시 시스템 반응 확인
  - 블랙 프라이데이, 티켓팅 등 이벤트 대비
- **내구성 테스트 (Endurance Test / Soak Test)**:
  - 장시간 부하 상태에서 메모리 누수, 성능 저하 확인
  - 24시간 이상 실행하여 안정성 검증

### 카오스 엔지니어링 / 장애 주입 테스트

- **목적**: 시스템의 복원력(resilience) 검증
- **방법**:
  - 무작위로 서버 종료
  - 네트워크 지연/단절 시뮬레이션
  - 디스크 full 상황 재현
  - CPU/메모리 고갈 상황 생성
- **도구**: Chaos Monkey, Gremlin, Litmus
- **원칙**: 프로덕션에서 실행하되, 영향 범위를 제한

### 회귀 테스트 (Regression Test)

- **목적**: 새로운 변경이 기존 기능을 손상시키지 않는지 확인
- **특징**:
  - 기존 테스트 스위트 전체를 재실행
  - CI/CD 파이프라인에 자동화
  - 코드 변경 시마다 실행

### 스모크 테스트 (Smoke Test)

- **목적**: 핵심 기능의 기본 동작 검증
- **특징**:
  - 빠르게 실행 (수분 이내)
  - 배포 후 즉시 실행하여 치명적 오류 조기 발견
  - "불이 붙었는지" 확인하는 수준의 간단한 테스트
- **예시**:
  - 애플리케이션 시작 가능 여부
  - 핵심 API 엔드포인트 응답 확인
  - 데이터베이스 연결 확인

### 카나리 테스트 (Canary Test)

- **목적**: 프로덕션 환경에서 제한된 사용자에게 먼저 배포
- **방법**:
  - 전체 트래픽의 1~5%만 새 버전으로 라우팅
  - 모니터링하며 점진적으로 확대
  - 문제 발견 시 즉시 롤백

### A/B 테스트

- **목적**: 서로 다른 버전의 성능/사용성 비교
- **사용 사례**:
  - UI/UX 변경 효과 측정
  - 알고리즘 개선 효과 검증
  - 비즈니스 메트릭 영향 분석

## 테스트 및 빌드 환경 구성하기

### 테스트 환경의 종류

#### 개발 환경 (Development)

- 개발자 로컬 머신
- 빠른 피드백 루프
- 격리된 환경에서 자유로운 실험

#### 테스트 환경 (Testing/QA)

- 프로덕션과 유사한 구성
- 통합 테스트 및 E2E 테스트 실행
- 테스트 데이터 관리

#### 스테이징 환경 (Staging)

- 프로덕션의 완전한 복제본
- 최종 검증 단계
- 프로덕션 데이터의 익명화된 복사본 사용

#### 프로덕션 환경 (Production)

- 실제 사용자 서비스
- 카나리 배포를 통한 점진적 롤아웃
- 실시간 모니터링 필수

### 빌드 파이프라인 구성

#### 지속적 통합 (CI - Continuous Integration)

**주요 구성 요소**:

1. **버전 관리**: Git, trunk-based development
2. **빌드 자동화**:
   - 컴파일
   - 패키징
   - 아티팩트 생성
3. **자동화된 테스트 실행**:
   - 단위 테스트 (매 커밋)
   - 통합 테스트 (매 커밋 또는 정기)
4. **코드 품질 검증**:
   - 정적 분석 (linting)
   - 코드 커버리지 측정
   - 보안 취약점 스캔

#### 지속적 배포 (CD - Continuous Deployment)

**배포 전략**:

- **블루-그린 배포**: 두 개의 동일한 환경을 유지하며 전환
- **카나리 배포**: 점진적 트래픽 증가
- **롤링 업데이트**: 한 번에 일부 인스턴스만 업데이트

### 테스트 데이터 관리

#### 테스트 데이터 전략

- **합성 데이터 생성**: 테스트용 가짜 데이터 생성
- **프로덕션 데이터 익명화**: 개인정보 마스킹/암호화
- **픽스처(Fixture)**: 재사용 가능한 테스트 데이터 세트
- **팩토리 패턴**: 동적 테스트 데이터 생성

#### 데이터베이스 테스트

- **테스트 격리**: 각 테스트는 독립적으로 실행
- **트랜잭션 롤백**: 테스트 후 데이터베이스 상태 복구
- **인메모리 DB**: H2, SQLite 등으로 빠른 테스트

### 환경 구성 관리

#### Infrastructure as Code (IaC)

- **Terraform**: 클라우드 인프라 프로비저닝
- **Ansible/Chef**: 서버 구성 관리
- **Docker**: 컨테이너화로 환경 일관성 확보
- **Kubernetes**: 컨테이너 오케스트레이션

#### 시크릿 관리

- 환경별 시크릿 분리
- **도구**: HashiCorp Vault, AWS Secrets Manager, Google Secret Manager
- 코드 저장소에 시크릿 절대 커밋 금지

## 대규모 환경에서의 테스트

### 확장성 문제

#### 테스트 실행 시간 증가

**문제**: 테스트 스위트가 커지면 실행 시간이 급증
**해결책**:

- **병렬화**: 테스트를 여러 머신에서 동시 실행
- **샤딩**: 테스트를 그룹으로 분할하여 분산 실행
- **우선순위화**: 중요하고 자주 실패하는 테스트 먼저 실행
- **스마트 테스트 선택**: 변경된 코드와 관련된 테스트만 실행

#### 테스트 불안정성 (Flakiness)

**원인**:

- 비동기 처리의 타이밍 이슈
- 외부 의존성 (네트워크, 외부 API)
- 테스트 간 상태 공유
- 무작위성 또는 시간 의존성

**해결책**:

- **격리 강화**: 각 테스트는 완전히 독립적
- **재시도 로직**: 일시적 실패에 대한 자동 재시도
- **타임아웃 적절히 설정**: 너무 짧거나 길지 않게
- **Flaky 테스트 추적**: 자주 실패하는 테스트 식별 및 수정
- **결정론적 테스트**: 무작위성 제거 (고정된 시드 사용)

### 분산 테스트 실행

#### 테스트 인프라

- **클라우드 기반 테스트**: AWS, GCP, Azure의 온디맨드 리소스 활용
- **컨테이너 기반 테스트**: Docker, Kubernetes로 일관된 환경 제공
- **테스트 오케스트레이션**:
  - Jenkins, GitLab CI, CircleCI
  - Google Cloud Build, AWS CodeBuild

#### 효율적인 리소스 활용

- **동적 스케일링**: 테스트 부하에 따라 리소스 자동 조정
- **캐싱**: 빌드 아티팩트, 의존성 캐싱으로 시간 단축
- **증분 테스트**: 변경된 부분만 재테스트

### 프로덕션 환경 테스트

#### 프로덕션 모니터링을 테스트로 활용

- **합성 모니터링 (Synthetic Monitoring)**:

  - 실제 사용자 시나리오를 지속적으로 시뮬레이션
  - 전 세계 여러 위치에서 실행
  - 문제를 사용자보다 먼저 발견

- **실제 사용자 모니터링 (RUM - Real User Monitoring)**:
  - 실제 사용자의 경험 측정
  - 성능 메트릭 수집
  - 오류율, 로딩 시간 추적

#### 트래픽 섀도잉 (Traffic Shadowing)

- 프로덕션 트래픽을 복제하여 새 버전으로 전송
- 실제 영향 없이 새 버전의 동작 관찰
- 성능 및 정확성 비교

#### 프로덕션 검증

- **Feature Flags**: 기능을 단계적으로 활성화
- **점진적 롤아웃**: 1% → 5% → 25% → 50% → 100%
- **자동 롤백**: 에러율 급증 시 자동으로 이전 버전으로 복구
- **다크 런칭**: 기능은 배포하되 사용자에게는 노출하지 않음

### 대규모 조직에서의 테스트 문화

#### 테스트 소유권

- **개발자가 테스트 작성**: "You build it, you test it"
- **테스트 커버리지 목표**: 프로젝트별 최소 기준 설정
- **테스트 리뷰**: 코드 리뷰 시 테스트도 함께 검토

#### 테스트 자동화 투자

- **초기 비용 vs 장기 이익**: 자동화는 시간이 지날수록 ROI 증가
- **테스트 유지보수**: 깨진 테스트는 즉시 수정
- **테스트 리팩토링**: 코드와 마찬가지로 테스트도 개선

#### 메트릭 추적

- **배포 빈도**: 얼마나 자주 배포하는가
- **변경 실패율**: 배포 중 몇 %가 실패하는가
- **복구 시간 (MTTR)**: 장애 발생 시 얼마나 빨리 복구하는가
- **변경 리드 타임**: 코드 커밋부터 프로덕션까지 걸리는 시간

# 18. SRE 조직의 소프트웨어 엔지니어링

## SRE 조직의 소프트웨어 엔지니어링 역량이 중요한 이유

SRE는 단순히 운영만 하는 조직이 아니다. 구글의 SRE는 소프트웨어 엔지니어링을 통해 운영 문제를 해결한다.

### 수작업의 한계

- **확장성 부족**: 수작업은 시스템 규모에 비례해서 증가
- **오류 발생**: 반복 작업에서 인간은 실수하기 마련
- **비효율성**: 귀중한 엔지니어 시간을 단순 반복 작업에 소비
- **번아웃**: 지루하고 반복적인 작업은 팀 사기 저하

### 소프트웨어 엔지니어링을 통한 해결

- **자동화**: 반복적인 운영 작업을 코드로 해결
- **확장성**: 한 번 작성한 코드는 무한히 재사용 가능
- **신뢰성**: 검증된 자동화는 수작업보다 일관되고 정확
- **시간 확보**: 토일(toil) 감소로 더 가치 있는 작업에 집중

### SRE의 50% 규칙

구글 SRE는 운영 업무(토일)에 최대 50%의 시간만 할애한다. 나머지 50%는 다음 활동에 사용:

- 소프트웨어 개발 (자동화, 도구 개발)
- 시스템 설계 및 아키텍처 개선
- 프로젝트 작업
- 엔지니어링 기술 향상

이 규칙을 지키지 못하면 개발팀으로 운영 부담을 다시 돌려보낸다.

## Auxon 사례 연구: 프로젝트 배경 및 문제가 발생한 부분

### 프로젝트 배경

Auxon은 구글의 내부 수용량 계획 도구다. 이 시스템은 다음을 수행한다:

- 각 서비스가 필요로 하는 리소스(CPU, 메모리, 디스크, 네트워크) 추적
- 데이터센터별 가용 수용량 모니터링
- 미래의 수요 예측
- 수용량 부족 사전 경고

### 초기 시스템의 문제점

#### 1. 명령형 수용량 관리

**문제**: 엔지니어들이 필요한 리소스를 수동으로 계산하고 요청해야 했다.

**결과**:

- 복잡한 계산 작업 (서버 수, CPU 코어, 메모리 등)
- 오류 발생 빈번 (과소 또는 과다 요청)
- 시간 소모적 (각 엔지니어가 동일한 계산 반복)
- 일관성 부족 (팀마다 다른 방식으로 계산)

#### 2. 수작업 프로세스

**문제**: 수용량 요청이 이메일이나 티켓 기반으로 처리되었다.

**한계**:

- 확장 불가능 (구글의 규모에서 감당 불가)
- 느린 처리 속도
- 추적과 감사(audit) 어려움
- 데이터 분석 불가능

#### 3. 반응적 접근

**문제**: 문제가 발생한 후에야 수용량 추가

**위험**:

- 서비스 중단 또는 성능 저하
- 긴급 구매로 인한 비용 증가
- 엔지니어링 시간 낭비 (긴급 대응)

### 근본 원인

SRE 조직이 소프트웨어 엔지니어링 솔루션 대신 수작업 프로세스에 의존했기 때문이다.

## 의도 기반 수용량 계획

### 개념

**의도 기반(Intent-Based) 접근법**: 엔지니어는 "무엇을" 원하는지만 명시하고, 시스템이 "어떻게" 제공할지 자동으로 결정한다.

### Auxon의 개선

#### 선언적 수용량 명세

**Before (명령형)**:
"서울 데이터센터에 16코어 CPU, 64GB RAM 서버 50대 필요합니다."
시스템이 이 요구사항을 충족하는 데 필요한 실제 리소스를 자동 계산한다.

#### 자동화된 수용량 계산

**기능**:

1. **성능 프로파일링**: 각 서비스의 리소스 사용 패턴 자동 학습
2. **수학적 모델링**: 통계 모델을 사용한 필요 리소스 계산
3. **안전 마진**: 트래픽 스파이크와 장애를 위한 여유 자동 포함
4. **지역별 최적화**: 데이터센터 위치별 최적 배치 계산

#### 지속적 모니터링과 조정

**Before**: 정적인 수용량 할당
**After**: 동적 조정

- 실제 트래픽 패턴 모니터링
- 예측 모델 지속적 개선
- 트렌드 기반 사전 경고
- 자동 스케일링 권장사항 제공

### 구현 요소

#### API 기반 인터페이스

시스템이 이 요구사항을 충족하는 데 필요한 실제 리소스를 자동 계산한다.

#### 자동화된 수용량 계산

**기능**:

1. **성능 프로파일링**: 각 서비스의 리소스 사용 패턴 자동 학습
2. **수학적 모델링**: 통계 모델을 사용한 필요 리소스 계산
3. **안전 마진**: 트래픽 스파이크와 장애를 위한 여유 자동 포함
4. **지역별 최적화**: 데이터센터 위치별 최적 배치 계산

#### 지속적 모니터링과 조정

**Before**: 정적인 수용량 할당
**After**: 동적 조정

- 실제 트래픽 패턴 모니터링
- 예측 모델 지속적 개선
- 트렌드 기반 사전 경고
- 자동 스케일링 권장사항 제공

여기서 `id`는 패킷을 입력으로 받아 연결 ID를 생성하는 함수이고, `N`은 구성된 백엔드 수다.

**문제점**:

- 백엔드 하나가 실패하면 `N`이 `N-1`이 되고, 거의 모든 패킷이 갑자기 다른 백엔드로 매핑된다.
- 백엔드가 상태를 공유하지 않으면 거의 모든 기존 연결이 재설정된다.

#### 3. 일관성 해싱 (Consistent Hashing)

1997년에 제안된 일관성 해싱은 백엔드 목록에서 추가되거나 제거될 때도 비교적 안정적으로 유지되는 매핑 알고리즘을 제공한다.

**장점**:

- 백엔드 풀이 변경될 때 기존 연결에 대한 중단을 최소화한다.
- 간단한 연결 추적을 사용하되, 시스템이 압박받을 때(예: 지속적인 DoS 공격 중) 일관성 해싱으로 전환할 수 있다.

### 패킷 전달 방법

#### 1. 네트워크 주소 변환 (NAT)

모든 연결의 항목을 추적 테이블에 유지해야 하므로, 완전히 무상태인 대체 메커니즘을 배제한다.

#### 2. 직접 서버 응답 (DSR - Direct Server Response)

데이터 링크 계층(OSI 네트워킹 모델의 레이어 2)의 정보를 수정한다. 전달된 패킷의 목적지 MAC 주소를 변경하여 상위 계층의 모든 정보를 그대로 유지하면, 백엔드는 원래 소스 및 목적지 IP 주소를 받는다.

**장점**:

- 사용자 요청이 작고 응답이 큰 경우(예: 대부분의 HTTP 요청) 엄청난 절약 효과를 제공한다.
- 로드밸런서 장치에 상태를 유지할 필요가 없다.

**단점**:

- 모든 머신(모든 로드밸런서와 모든 백엔드)이 데이터 링크 계층에서 서로 연결될 수 있어야 한다.
- 모든 머신이 단일 브로드캐스트 도메인에 있어야 한다.
- 구글은 이 솔루션을 오래전에 넘어섰다.

#### 3. 패킷 캡슐화 (구글의 현재 솔루션)

네트워크 로드밸런서는 전달된 패킷을 GRE(Generic Routing Encapsulation)를 사용하여 다른 IP 패킷에 넣고, 백엔드의 주소를 목적지로 사용한다.

**작동 방식**:

- 백엔드는 패킷을 수신하면 외부 IP+GRE 계층을 벗겨내고, 내부 IP 패킷을 네트워크 인터페이스로 직접 전달된 것처럼 처리한다.
- 네트워크 로드밸런서와 백엔드는 더 이상 동일한 브로드캐스트 도메인에 있을 필요가 없다.
- 둘 사이에 경로가 존재하기만 하면 서로 다른 대륙에 있을 수도 있다.

**장점**:

- 네트워크 설계 및 진화 방식에 큰 유연성을 제공한다.

**단점**:

- 패킷 크기 증가: 캡슐화는 오버헤드를 도입한다(IPv4+GRE의 경우 정확히 24바이트).
- 사용 가능한 최대 전송 단위(MTU) 크기를 초과하여 단편화가 필요할 수 있다.

## 결론

로드밸런싱은 표면적으로는 간단해 보이지만("일찍, 자주 로드밸런싱하라"), 프런트엔드 로드밸런싱과 데이터센터에 도달한 후 패킷 처리 모두에서 세부 사항이 어렵다.

# 20. 데이터센터의 로드밸런싱

이 장에서는 데이터센터 내부의 로드밸런싱에 초점을 맞춘다. 구체적으로는 쿼리 스트림에 대해 주어진 데이터센터 내에서 작업을 분산하는 알고리즘을 다룬다.

## 전제 조건

**가정**:

- 데이터센터에 도착하는 쿼리 스트림이 있다(데이터센터 자체, 원격 데이터센터 또는 혼합에서 올 수 있다).
- 쿼리 비율이 데이터센터가 처리할 수 있는 리소스를 초과하지 않는다(또는 매우 짧은 시간 동안만 초과).
- 데이터센터 내에 이러한 쿼리에 대해 작동하는 서비스가 있다.
- 서비스는 대부분 서로 다른 머신에서 실행되는 많은 동질적이고 교환 가능한 서버 프로세스로 구현된다.

**용어**:

- **백엔드 태스크(Backend Tasks)**: 서비스를 구현하는 프로세스. 가장 작은 서비스는 최소 3개의 프로세스를 가지며, 가장 큰 서비스는 10,000개 이상을 가질 수 있다. 일반적으로 100~1,000개의 프로세스가 있다.
- **클라이언트 태스크(Client Tasks)**: 백엔드 태스크에 연결을 유지하는 다른 태스크. 각 들어오는 쿼리에 대해 클라이언트 태스크는 어떤 백엔드 태스크가 쿼리를 처리해야 하는지 결정해야 한다.

## 이상적인 경우

이상적인 경우, 주어진 서비스의 부하는 모든 백엔드 태스크에 완벽하게 분산되며, 특정 시점에 가장 적게 로드된 백엔드 태스크와 가장 많이 로드된 백엔드 태스크가 정확히 동일한 양의 CPU를 소비한다.

### 용량 낭비 문제

가장 로드된 태스크가 용량 한계에 도달하는 지점까지만 데이터센터로 트래픽을 보낼 수 있다. 부하 분산이 고르지 않으면 가장 로드된 태스크를 제외한 모든 태스크의 유휴 용량이 낭비된다.

**용량 낭비 계산**:

- `CPU[i]`를 특정 시점에 태스크 i가 소비하는 CPU 비율이라고 하자.
- 태스크 0이 가장 로드된 태스크라고 가정하면,
- 낭비되는 용량 = 모든 태스크 i에 대해 `(CPU[0] - CPU[i])`의 합

**예시**: 데이터센터에 서비스를 위해 1,000개의 CPU를 예약했지만, 부하 분산이 좋지 않으면 실제로는 700개의 CPU만 사용할 수 있을 수 있다.

## 불량 태스크 식별: 흐름 제어와 Lame Duck

백엔드 태스크를 선택하기 전에 백엔드 풀에서 불량한 태스크를 식별하고 피해야 한다.

### 간단한 접근법: 흐름 제어

클라이언트 태스크는 각 백엔드 태스크로의 연결에서 보낸 활성 요청 수를 추적한다. 이 활성 요청 수가 설정된 한계에 도달하면 클라이언트는 백엔드를 비정상으로 간주하고 더 이상 요청을 보내지 않는다.

**문제점**:

- 매우 극단적인 과부하 형태만 방지한다.
- 백엔드가 이 한계에 도달하기 훨씬 전에 과부하가 걸릴 수 있다.
- 일부 경우에는 백엔드에 여전히 충분한 여유 리소스가 있을 때 클라이언트가 이 한계에 도달할 수 있다.

### 강력한 접근법: Lame Duck 상태

클라이언트 관점에서 주어진 백엔드 태스크는 다음 상태 중 하나에 있을 수 있다:

**1. 정상 (Healthy)**

- 백엔드 태스크가 올바르게 초기화되고 요청을 처리 중이다.

**2. 연결 거부 (Refusing Connections)**

- 백엔드 태스크가 응답하지 않는다.
- 태스크가 시작 중이거나 종료 중이거나, 비정상 상태에 있기 때문일 수 있다.

**3. Lame Duck**

- 백엔드 태스크가 포트에서 수신 중이고 서비스를 제공할 수 있지만, 명시적으로 클라이언트에게 요청 전송을 중지하도록 요청한다.

### Lame Duck의 작동 방식

**활성 클라이언트**: 태스크가 lame duck 상태에 들어가면 모든 활성 클라이언트에게 브로드캐스트한다.

**비활성 클라이언트**: 구글의 RPC 구현에서는 비활성 클라이언트(활성 TCP 연결이 없는 클라이언트)도 주기적인 UDP 상태 확인을 보낸다.

**결과**: Lame duck 정보는 현재 상태와 관계없이 모든 클라이언트에 빠르게 전파된다(일반적으로 1~2 RTT).

### 깨끗한 종료 (Clean Shutdown)

Lame duck 상태의 주요 장점은 깨끗한 종료를 단순화한다는 것이다.

**종료 절차**:

1. 작업 스케줄러가 백엔드 태스크에 SIGTERM 신호를 보낸다.
2. 백엔드 태스크가 lame duck 상태에 들어가고 클라이언트에게 새 요청을 다른 백엔드 태스크로 보내도록 요청한다.
3. 백엔드 태스크가 lame duck 상태에 들어가기 전(또는 들어간 후 클라이언트가 감지하기 전)에 시작된 모든 진행 중인 요청은 정상적으로 실행된다.
4. 응답이 클라이언트로 다시 흐르면서 백엔드에 대한 활성 요청 수가 점차 0으로 감소한다.
5. 설정된 간격 후 백엔드 태스크가 깨끗하게 종료되거나 작업 스케줄러가 강제 종료한다.

**간격 설정**: 모든 일반적인 요청이 완료할 충분한 시간을 가질 수 있을 만큼 커야 한다. 서비스에 따라 다르지만, 경험상 클라이언트 복잡성에 따라 10~150초 사이가 적당하다.

## 서브세팅을 이용한 연결 풀 제한

상태 관리 외에도 로드밸런싱의 또 다른 고려사항은 서브세팅(subsetting)이다: 클라이언트 태스크가 상호작용하는 잠재적 백엔드 태스크 풀을 제한하는 것이다.

### 연결 풀 유지

구글의 RPC 시스템에서 각 클라이언트는 백엔드에 대한 장기 연결 풀을 유지한다. 이러한 연결은 일반적으로 클라이언트가 시작할 때 초기에 설정되며 클라이언트가 종료될 때까지 열린 상태로 유지된다.

**대안 모델의 문제점**: 각 요청에 대해 연결을 설정하고 해체하는 것은 상당한 리소스와 지연 비용이 든다.

### 연결 오버헤드

모든 연결은 양쪽 끝에서 약간의 메모리와 CPU(주기적인 상태 확인으로 인해)를 필요로 한다. 이론적으로는 작지만, 많은 머신에 걸쳐 발생하면 빠르게 상당해질 수 있다.

**서브세팅이 피하는 상황**:

- 단일 클라이언트가 매우 많은 수의 백엔드 태스크에 연결
- 단일 백엔드 태스크가 매우 많은 수의 클라이언트 태스크로부터 연결을 받음

### 올바른 서브셋 선택

**서브셋 크기**: 일반적으로 20~100개의 백엔드 태스크를 사용하지만, 시스템의 "올바른" 서브셋 크기는 서비스의 일반적인 동작에 크게 의존한다.

**더 큰 서브셋 크기가 필요한 경우**:

- **클라이언트 수가 백엔드 수보다 훨씬 적은 경우**: 트래픽을 절대 받지 않을 백엔드 태스크가 없도록 하려면 클라이언트당 백엔드 수가 충분히 커야 한다.
- **클라이언트 작업 내 부하 불균형이 빈번한 경우**: 요청 버스트가 클라이언트의 할당된 서브셋에 집중되므로, 사용 가능한 백엔드 태스크의 더 큰 집합에 부하를 고르게 분산하려면 더 큰 서브셋 크기가 필요하다.

### 서브셋 선택 알고리즘

서브셋 크기가 결정되면 각 클라이언트 태스크가 사용할 백엔드 태스크의 서브셋을 정의하는 알고리즘이 필요하다.

**선택 알고리즘의 요구사항**:

- **균일한 백엔드 할당**: 효율적인 프로비저닝을 위해 백엔드를 균일하게 할당해야 한다. 예를 들어, 서브세팅이 한 백엔드를 10% 과부하시키면 전체 백엔드 세트를 10% 과다 프로비저닝해야 한다.
- **재시작과 장애의 우아한 처리**: 가능한 한 균일하게 백엔드를 계속 로드하면서 처닝(churn)을 최소화해야 한다.
- **클라이언트/백엔드 수의 크기 조정 처리**: 이러한 수를 미리 알지 못한 채로 최소한의 연결 처닝으로 처리해야 한다.

**처닝(Churn)**: 백엔드 교체 선택과 관련이 있다. 백엔드 태스크를 사용할 수 없게 되면 클라이언트는 일시적으로 교체 백엔드를 선택해야 할 수 있다. 교체 백엔드가 선택되면 클라이언트는 새로운 TCP 연결을 생성해야 하며(애플리케이션 레벨 협상을 수행해야 할 가능성이 높음), 이는 추가 오버헤드를 생성한다.

### 무작위 서브세팅 (Random Subsetting)

**순진한 구현**: 각 클라이언트가 백엔드 목록을 한 번 무작위로 섞고, 목록에서 해결 가능한/정상적인 백엔드를 선택하여 서브셋을 채운다.

**문제점**: 이 전략은 대부분의 실제 시나리오에서 부하를 매우 고르지 않게 분산한다.

**예시 (300 클라이언트, 300 백엔드, 30% 서브셋 크기)**:

- 가장 적게 로드된 백엔드: 평균 부하의 63% (57개 연결, 평균은 90개)
- 가장 많이 로드된 백엔드: 평균 부하의 121% (109개 연결)

**더 작은 서브셋 크기의 경우 불균형 악화**:

- 10% 서브셋 크기 (클라이언트당 30개 백엔드):
  - 가장 적게 로드된: 평균의 50% (15개 연결)
  - 가장 많이 로드된: 평균의 150% (45개 연결)

**결론**: 무작위 서브세팅이 모든 사용 가능한 태스크에 부하를 상대적으로 균등하게 분산하려면 75%만큼 큰 서브셋 크기가 필요하다. 이는 대규모에서 실용적이지 않다.

### 결정론적 서브세팅 (Deterministic Subsetting)

구글의 해결책은 결정론적 서브세팅이다.

**알고리즘 설명**:

클라이언트 태스크를 "라운드(rounds)"로 나눈다. 여기서 라운드 i는 태스크 `subset_count × i`에서 시작하는 `subset_count`개의 연속적인 클라이언트 태스크로 구성되며, `subset_count`는 서브셋 수(백엔드 태스크 수를 원하는 서브셋 크기로 나눈 값)다.

각 라운드 내에서 각 백엔드는 정확히 하나의 클라이언트에 할당된다(마지막 라운드는 충분한 클라이언트가 없을 수 있으므로 일부 백엔드가 할당되지 않을 수 있음).

**예시 (12개 백엔드 [0, 11], 서브셋 크기 3, 10개 클라이언트)**:

- `subset_count = 12/3 = 4` (라운드당 4개의 클라이언트)

**라운드별 섞인 백엔드**:

- 라운드 0: [0, 6, 3, 5, 1, 7, 11, 9, 2, 4, 8, 10]
- 라운드 1: [8, 11, 4, 0, 5, 6, 10, 3, 2, 7, 9, 1]
- 라운드 2: [8, 3, 7, 2, 1, 4, 9, 10, 6, 5, 0, 11]

**핵심**: 각 라운드는 전체 목록의 각 백엔드를 하나의 클라이언트에만 할당한다. 이 예시에서 모든 백엔드는 정확히 2~3개의 클라이언트에 할당된다.

**서브셋 정의**:

- Subset[0] = shuffled_backends[0~2]
- Subset[1] = shuffled_backends[3~5]
- Subset[2] = shuffled_backends[6~8]
- Subset[3] = shuffled_backends[9~11]

**클라이언트 할당**:

- client[0], client[4], client[8] → subset[0]
- client[1], client[5], client[9] → subset[1]
- client[2], client[6] → subset[2]
- client[3], client[7] → subset[3]

**라운드별로 다른 섞기를 사용하는 이유**:

- 백엔드가 실패하면 동일한 섞기를 사용하는 경우 해당 백엔드가 받던 부하가 서브셋의 나머지 백엔드에만 분산된다.
- 다른 섞기를 사용하면 이 부하가 모든 나머지 백엔드에 분산된다.

**결과**: 300개의 클라이언트가 각각 300개 백엔드 중 10개에 연결하는 경우, 각 백엔드는 정확히 동일한 수의 연결을 받는다.

## 로드밸런싱 정책

이제 클라이언트 태스크가 정상으로 알려진 연결 세트를 유지하는 기초를 확립했으므로, 로드밸런싱 정책을 살펴보자. 이는 클라이언트 태스크가 서브셋의 어떤 백엔드 태스크가 클라이언트 요청을 받아야 하는지 선택하는 데 사용하는 메커니즘이다.

### 단순 라운드 로빈 (Simple Round Robin)

각 클라이언트가 성공적으로 연결할 수 있고 lame duck 상태가 아닌 서브셋의 각 백엔드 태스크에 라운드 로빈 방식으로 요청을 보낸다.

**장점**: 매우 간단하며 백엔드 태스크를 무작위로 선택하는 것보다 훨씬 우수한 성능을 제공한다.

**문제점**: 이 정책의 결과는 매우 나쁠 수 있다. 라운드 로빈은 가장 적게 로드된 태스크와 가장 많이 로드된 태스크 사이에 최대 2배의 CPU 소비 차이를 초래할 수 있다.

**라운드 로빈이 부하를 제대로 분산하지 못하는 이유**:

**1. 작은 서브세팅**:
모든 클라이언트가 동일한 속도로 요청을 발행하지 않을 수 있다. 특히 매우 다른 프로세스가 동일한 백엔드를 공유하는 경우 더욱 그렇다. 상대적으로 작은 서브셋 크기를 사용하는 경우, 가장 많은 트래픽을 생성하는 클라이언트의 서브셋에 있는 백엔드는 자연스럽게 더 많이 로드되는 경향이 있다.

**2. 다양한 쿼리 비용**:
많은 서비스는 처리에 매우 다른 양의 리소스를 필요로 하는 요청을 처리한다. 실제로 구글에서는 가장 비싼 요청이 가장 저렴한 요청보다 1000배(또는 그 이상) 많은 CPU를 소비하는 경우가 많다.

**예시**: "지난 하루 동안 사용자 XYZ가 받은 모든 이메일 반환" 쿼리는 매우 저렴할 수도 있고(사용자가 하루 동안 이메일을 거의 받지 않은 경우) 매우 비쌀 수도 있다.

**해결책**: 서비스 인터페이스를 기능적으로 조정하여 요청당 수행되는 작업량을 제한한다. 예: "지난 하루 동안 사용자 XYZ가 받은 가장 최근 100개(또는 그 이하)의 이메일 반환"으로 페이지네이션 인터페이스 도입.

그러나 이러한 의미론적 변경을 도입하는 것은 종종 어렵다. 모든 클라이언트 코드에 변경이 필요할 뿐만 아니라 추가적인 일관성 고려사항도 수반한다.

**3. 머신 다양성**:
동일한 데이터센터의 모든 머신이 반드시 동일한 것은 아니다. 주어진 데이터센터는 성능이 다른 CPU를 가진 머신을 가질 수 있으며, 따라서 동일한 요청이 서로 다른 머신에 대해 상당히 다른 양의 작업을 나타낼 수 있다.

**구글의 해결책**: GCU(Google Compute Units)라는 CPU 비율에 대한 가상 단위를 만들었다. GCU는 CPU 비율 모델링의 표준이 되었으며, 성능을 기반으로 데이터센터의 각 CPU 아키텍처에서 해당 GCU로의 매핑을 유지하는 데 사용되었다.

**4. 예측 불가능한 성능 요인**:
단순 라운드 로빈의 가장 큰 복잡성 요인은 머신, 더 정확하게는 백엔드 태스크의 성능이 정적으로 설명할 수 없는 여러 예측 불가능한 측면으로 인해 크게 다를 수 있다는 것이다.

**예측 불가능한 성능에 기여하는 요인**:

**적대적 이웃 (Antagonistic Neighbors)**:

- 다른 프로세스(종종 완전히 무관하고 다른 팀에서 실행)가 프로세스 성능에 상당한 영향을 미칠 수 있다.
- 최대 20%의 성능 차이를 확인했다.
- 메모리 캐시 공간이나 대역폭과 같은 공유 리소스에 대한 경쟁에서 비롯된다.

**태스크 재시작**:

- 태스크가 재시작되면 몇 분 동안 훨씬 더 많은 리소스가 필요한 경우가 많다.
- Java와 같이 코드를 동적으로 최적화하는 플랫폼이 다른 플랫폼보다 더 많은 영향을 받는다.
- 일부 서버 코드에 로직을 추가하여 서버를 lame duck 상태로 유지하고 성능이 명목상이 될 때까지 일정 기간 동안 예열(prewarm)한다.

**결론**: 로드밸런싱 정책이 예측하지 못한 성능 제한에 적응할 수 없다면 대규모에서 작업할 때 본질적으로 최적이 아닌 부하 분산을 갖게 된다.

### 최소 부하 라운드 로빈 (Least-Loaded Round Robin)

각 클라이언트 태스크가 서브셋의 각 백엔드 태스크에 대한 활성 요청 수를 추적하고, 최소 활성 요청 수를 가진 태스크 세트 중에서 라운드 로빈을 사용한다.

**작동 방식 예시**:

클라이언트가 백엔드 태스크 t0~t9의 서브셋을 사용하고 현재 각 백엔드에 대해 다음과 같은 활성 요청 수를 가지고 있다고 가정:

| t0  | t1  | t2  | t3  | t4  | t5  | t6  | t7  | t8  | t9  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 3   | 4   | 1   | 1   | 2   | 1   | 5   | 1   | 1   | 2   |

새 요청의 경우 클라이언트는 잠재적 백엔드 태스크 목록을 최소 연결 수를 가진 태스크(t2, t3, t5, t7, t8)로만 필터링하고 해당 목록에서 백엔드를 선택한다.

**아이디어**: 로드된 태스크는 여유 용량이 있는 태스크보다 지연 시간이 더 높은 경향이 있으며, 이 전략은 자연스럽게 이러한 로드된 태스크에서 부하를 제거한다.

**위험한 함정: 싱크홀링 (Sinkholing)**:
태스크가 심각하게 비정상적이면 100% 오류를 제공하기 시작할 수 있다. 오류의 특성에 따라 지연 시간이 매우 낮을 수 있다. 실제로 요청을 처리하는 것보다 "나는 비정상입니다!" 오류를 반환하는 것이 훨씬 빠른 경우가 많다.

결과적으로 클라이언트는 태스크가 사용 가능하다고 잘못 생각하여 비정상 태스크로 매우 많은 양의 트래픽을 보내기 시작할 수 있다. 비정상 태스크가 이제 트래픽을 싱크홀링하고 있다고 말한다.

**해결책**: 최근 오류를 활성 요청인 것처럼 계산하도록 정책을 수정한다. 이렇게 하면 백엔드 태스크가 비정상이 되면 로드밸런싱 정책이 과부하된 태스크에서 부하를 전환하는 것과 동일한 방식으로 부하를 전환하기 시작한다.

**최소 부하 라운드 로빈의 두 가지 중요한 제한사항**:

1. **활성 요청 수는 주어진 백엔드의 능력에 대한 매우 좋은 프록시가 아닐 수 있다**:

   - 많은 요청은 수명의 상당 부분을 네트워크의 응답을 기다리는 데(다른 백엔드에 시작한 요청에 대한 응답을 기다리는 등) 보내고 실제 처리에는 거의 시간을 소비하지 않는다.
   - I/O 대기는 종종 CPU를 전혀 소비하지 않고, RAM도 거의 소비하지 않으며, 대역폭도 소비하지 않는다.

2. **각 클라이언트의 활성 요청 수에는 다른 클라이언트에서 동일한 백엔드로의 요청이 포함되지 않는다**:
   - 각 클라이언트 태스크는 백엔드 태스크의 상태에 대해 매우 제한된 보기만 가지고 있다: 자체 요청의 보기.

**결과**: 실제로 최소 부하 라운드 로빈을 사용하는 대규모 서비스는 가장 많이 로드된 백엔드 태스크가 가장 적게 로드된 것의 2배만큼 CPU를 사용하는 것을 볼 수 있으며, 라운드 로빈만큼 나쁜 성능을 보인다.

### 가중 라운드 로빈 (Weighted Round Robin)

가중 라운드 로빈은 백엔드 제공 정보를 의사결정 프로세스에 통합하여 단순 및 최소 부하 라운드 로빈을 개선하는 중요한 로드밸런싱 정책이다.

**원리**: 각 클라이언트 태스크는 서브셋의 각 백엔드에 대한 "능력(capability)" 점수를 유지한다. 요청은 라운드 로빈 방식으로 분산되지만, 클라이언트는 백엔드에 대한 요청 분산을 비례적으로 가중치를 둔다.

**작동 방식**:

- 각 응답(상태 확인에 대한 응답 포함)에서 백엔드는 초당 쿼리 및 오류의 현재 관찰된 비율과 활용률(일반적으로 CPU 사용량)을 포함한다.
- 클라이언트는 현재 성공적으로 처리된 요청 수와 어떤 활용 비용으로 처리되었는지에 따라 백엔드 태스크를 선택하기 위해 주기적으로 능력 점수를 조정한다.
- 실패한 요청은 향후 결정에 영향을 미치는 페널티를 초래한다.

**효과**: 가중 라운드 로빈은 실제로 매우 잘 작동했으며 가장 많이 활용되는 태스크와 가장 적게 활용되는 태스크 간의 차이를 크게 줄였다.

클라이언트가 최소 부하에서 가중 라운드 로빈으로 전환한 시점 전후의 무작위 백엔드 태스크 서브셋의 CPU 비율을 보면, 가장 적게 로드된 태스크에서 가장 많이 로드된 태스크까지의 분산이 급격히 감소했다.

## 결론

데이터센터 내부의 로드밸런싱은 다음과 같은 여러 요소를 고려해야 한다:

**핵심 요소**:

1. **불량 태스크 식별**: 흐름 제어와 lame duck 상태를 통해 비정상 백엔드 회피
2. **서브세팅**: 연결 풀을 효율적으로 관리하기 위한 결정론적 서브세팅
3. **로드밸런싱 정책**: 단순 라운드 로빈에서 가중 라운드 로빈으로 진화

**교훈**:

- 단순한 알고리즘(단순 라운드 로빈)은 대규모에서 최대 2배의 부하 불균형을 초래할 수 있다.
- 백엔드 상태 정보를 활용하는 정책(가중 라운드 로빈)이 가장 효과적이다.
- 분산 시스템에서는 각 클라이언트가 제한된 정보만 가지고 실시간으로 결정을 내려야 한다.
- 예측 불가능한 성능 요인(적대적 이웃, 태스크 재시작 등)에 적응할 수 있는 로드밸런싱 정책이 필요하다.

# 21. 과부하 처리하기

과부하를 피하는 것은 로드밸런싱 정책의 목표다. 하지만 로드밸런싱 정책이 아무리 효율적이어도 결국 시스템의 일부는 과부하 상태가 될 것이다. 과부하 상황을 우아하게 처리하는 것은 신뢰할 수 있는 서빙 시스템을 운영하는 데 있어 근본적이다.

## 과부하 처리 옵션

### 저하된 응답 제공

정상 응답만큼 정확하지 않거나 적은 데이터를 포함하지만 계산하기 더 쉬운 응답을 제공한다.

**예시**:

- 최상의 결과를 제공하기 위해 전체 말뭉치를 검색하는 대신, 후보 세트의 작은 비율만 검색한다.
- 정식 저장소에 접근하는 대신 완전히 최신이 아닐 수 있지만 사용 비용이 더 저렴한 로컬 복사본에 의존한다.

### 극심한 과부하 시

서비스가 저하된 응답조차 계산하고 제공할 수 없을 수 있다. 이 시점에서는 오류를 제공하는 것 외에 즉각적인 선택지가 없을 수 있다.

**완화 방법**: 데이터센터 간 트래픽을 균형 있게 분배하여 어떤 데이터센터도 처리 용량보다 많은 트래픽을 받지 않도록 한다.

**예시**: 데이터센터가 100개의 백엔드 태스크를 실행하고 각 태스크가 초당 최대 500개의 요청을 처리할 수 있다면, 로드밸런싱 알고리즘은 해당 데이터센터로 초당 50,000개 이상의 쿼리를 보내지 않는다.

**결론**: 클라이언트와 백엔드가 리소스 제한을 우아하게 처리하도록 구축하는 것이 가장 좋다. 가능하면 리디렉션하고, 필요하면 저하된 결과를 제공하며, 다른 모든 방법이 실패하면 리소스 오류를 투명하게 처리한다.

## "초당 쿼리 수"의 함정

### 문제점

서로 다른 쿼리는 리소스 요구사항이 크게 다를 수 있다. 쿼리의 비용은 다음과 같은 임의적 요인에 따라 달라질 수 있다:

- 쿼리를 발행하는 클라이언트의 코드
- 하루 중 시간대 (가정 사용자 대 업무 사용자, 대화형 최종 사용자 트래픽 대 배치 트래픽)

**구글의 교훈**: "초당 쿼리 수"로 용량을 모델링하거나 요청이 소비하는 리소스의 프록시라고 믿어지는 요청의 정적 기능을 사용하는 것(예: "요청이 읽는 키의 수")은 종종 좋지 않은 지표다.

### 더 나은 해결책: 가용 리소스로 직접 측정

주어진 데이터센터의 주어진 서비스에 예약된 500개의 CPU 코어와 1TB의 메모리가 있다면, 이러한 숫자를 직접 사용하여 데이터센터의 용량을 모델링하는 것이 훨씬 더 잘 작동한다.

**요청 비용**: 정규화된 측정값으로, 다양한 CPU 아키텍처에서 성능 차이를 고려하여 얼마나 많은 CPU 시간을 소비했는지를 나타낸다.

**CPU 소비를 프로비저닝 신호로 사용하는 이유**:

- 가비지 컬렉션이 있는 플랫폼에서는 메모리 압력이 자연스럽게 증가된 CPU 소비로 전환된다.
- 다른 플랫폼에서는 CPU가 소진되기 전에 나머지 리소스가 소진될 가능성이 매우 낮은 방식으로 프로비저닝할 수 있다.

CPU가 아닌 리소스를 과다 프로비저닝하는 것이 지나치게 비싼 경우, 리소스 소비를 고려할 때 각 시스템 리소스를 별도로 고려한다.

## 고객별 제한

### 글로벌 과부하 대응

완벽한 세계에서는 팀들이 백엔드 종속성의 소유자와 신중하게 출시를 조정하여 글로벌 과부하가 절대 발생하지 않고 백엔드 서비스는 항상 고객을 서비스할 충분한 용량을 가진다. 하지만 현실에서는 글로벌 과부하가 꽤 자주 발생한다(특히 많은 팀에서 실행하는 많은 클라이언트를 가진 내부 서비스의 경우).

### 고객별 할당량

글로벌 과부하가 발생할 때, 서비스는 잘못 동작하는 고객에게만 오류 응답을 전달하고 다른 고객은 영향을 받지 않도록 하는 것이 중요하다.

**구현**: 서비스 소유자는 고객과 협상된 사용량을 기반으로 용량을 프로비저닝하고, 이러한 합의에 따라 고객별 할당량을 정의한다.

**예시 (전 세계적으로 10,000 CPU 할당된 백엔드 서비스)**:

- Gmail: 최대 초당 4,000 CPU 초 소비 허용
- Calendar: 최대 초당 4,000 CPU 초 소비 허용
- Android: 최대 초당 3,000 CPU 초 소비 허용
- Google+: 최대 초당 2,000 CPU 초 소비 허용
- 기타 모든 사용자: 최대 초당 500 CPU 초 소비 허용

**주의**: 이 숫자들은 백엔드 서비스에 할당된 10,000 CPU보다 많이 합산될 수 있다. 서비스 소유자는 모든 고객이 동시에 리소스 한계에 도달할 가능성이 낮다는 사실에 의존한다.

**구현 세부사항**: 모든 백엔드 태스크로부터 실시간으로 글로벌 사용 정보를 집계하고, 해당 데이터를 사용하여 개별 백엔드 태스크에 유효한 한계를 푸시한다.

## 클라이언트 측 조절 (Client-Side Throttling)

### 문제점

고객이 할당량을 초과하면 백엔드 태스크는 요청을 빠르게 거부해야 한다. "고객 할당량 초과" 오류를 반환하는 것이 실제로 요청을 처리하고 올바른 응답을 제공하는 것보다 훨씬 적은 리소스를 소비한다는 기대가 있다.

하지만 모든 서비스에 이 논리가 적용되는 것은 아니다. 예를 들어, 간단한 RAM 조회가 필요한 요청을 거부하는 것(요청/응답 프로토콜 처리 오버헤드가 응답 생성 오버헤드보다 훨씬 큰 경우)은 해당 요청을 수락하고 실행하는 것과 거의 동일하게 비싸다.

거부된 요청의 양이 상당하면 이 숫자들은 빠르게 합산된다. 백엔드가 대부분의 CPU를 단지 요청을 거부하는 데 소비하더라도 과부하 상태가 될 수 있다!

### 적응형 조절 (Adaptive Throttling)

클라이언트 측 조절이 이 문제를 해결한다. 클라이언트가 최근 요청의 상당 부분이 "할당량 초과" 오류로 인해 거부되었음을 감지하면, 자체 조절을 시작하고 생성하는 나가는 트래픽의 양을 제한한다. 제한을 초과하는 요청은 네트워크에 도달하지도 않고 로컬에서 실패한다.

**작동 방식**: 각 클라이언트 태스크는 최근 2분간의 다음 정보를 유지한다:

- `requests`: 애플리케이션 레이어에서 시도한 요청 수(클라이언트에서, 적응형 조절 시스템 위에서)
- `accepts`: 백엔드가 수락한 요청 수

**정상 조건**: 두 값이 동일하다.

**백엔드가 트래픽 거부 시작**: `accepts` 수가 `requests`보다 작아진다.

**조절 시작 조건**: 클라이언트는 `requests`가 `accepts`의 K배만큼 클 때까지 백엔드에 요청을 계속 발행할 수 있다. 그 임계값에 도달하면, 클라이언트는 자체 조절을 시작한다.

**클라이언트 요청 거부 확률**:

### 승수 K 조정

요청 처리 비용이 요청 거부 비용과 매우 가까운 서비스의 경우, 백엔드 리소스의 약 절반이 거부된 요청에 의해 소비되도록 허용하는 것은 받아들일 수 없을 수 있다.

**해결책**: 클라이언트 요청 거부 확률에서 `accepts` 승수 K(예: 2)를 수정한다.

- 승수를 줄이면 적응형 조절이 더 공격적으로 동작한다.
- 승수를 늘리면 적응형 조절이 덜 공격적으로 동작한다.

**예시**: `requests = 2 × accepts`일 때 클라이언트가 자체 조절하는 대신, `requests = 1.1 × accepts`일 때 자체 조절하도록 한다. 승수를 1.1로 줄이면 백엔드가 수락하는 10개 요청마다 하나의 요청만 거부된다.

**일반적 선호**: 2배 승수를 선호한다. 실제로 허용될 것으로 예상되는 것보다 더 많은 요청이 백엔드에 도달하도록 허용함으로써 백엔드에서 더 많은 리소스를 낭비하지만, 백엔드에서 클라이언트로의 상태 전파를 가속화한다.

**장점**:

- 대규모 과부하 상황에서도 백엔드는 실제로 처리하는 각 요청에 대해 하나의 요청을 거부하게 된다.
- 결정이 전적으로 로컬 정보를 기반으로 클라이언트 태스크에 의해 이루어진다.
- 추가 종속성이나 지연 페널티가 없다.
- 비교적 간단한 구현이다.

**제한사항**: 클라이언트 측 조절은 백엔드에 매우 드물게만 요청을 보내는 클라이언트와는 잘 작동하지 않을 수 있다. 이 경우 각 클라이언트가 가진 백엔드 상태에 대한 뷰가 급격히 축소된다.

## 중요도 (Criticality)

중요도는 글로벌 할당량 및 조절의 맥락에서 매우 유용하다고 판단한 또 다른 개념이다. 백엔드에 대한 요청은 해당 요청을 얼마나 중요하게 여기는지에 따라 네 가지 가능한 중요도 값 중 하나와 연관된다.

### 중요도 수준

**CRITICAL_PLUS**:

- 가장 중요한 요청을 위해 예약됨
- 실패하면 심각한 사용자 가시적 영향을 초래한다

**CRITICAL**:

- 프로덕션 작업에서 전송된 요청의 기본값
- 사용자 가시적 영향을 초래하지만 CRITICAL_PLUS보다 덜 심각할 수 있다
- 서비스는 예상되는 모든 CRITICAL 및 CRITICAL_PLUS 트래픽을 위한 충분한 용량을 프로비저닝할 것으로 예상된다

**SHEDDABLE_PLUS**:

- 부분적 사용 불가능이 예상되는 트래픽
- 배치 작업의 기본값으로, 몇 분 또는 몇 시간 후에 요청을 재시도할 수 있다

**SHEDDABLE**:

- 빈번한 부분적 사용 불가능과 때때로 완전한 사용 불가능이 예상되는 트래픽

**네 가지 값의 이유**: 거의 모든 서비스를 모델링하기에 충분히 강력하다는 것을 발견했다. 더 많은 값을 추가하자는 제안이 있었지만, 추가 값을 정의하면 다양한 중요도 인식 시스템을 운영하는 데 더 많은 리소스가 필요하다.

### 중요도의 통합

중요도를 RPC 시스템의 일급 개념으로 만들고 과부하 상황에 대응할 때 고려될 수 있도록 많은 제어 메커니즘에 통합하기 위해 열심히 작업했다.

**예시**:

- 고객이 글로벌 할당량을 초과하면, 백엔드 태스크는 모든 낮은 중요도의 모든 요청을 이미 거부하고 있는 경우에만 주어진 중요도의 요청을 거부한다.
- 태스크 자체가 과부하 상태일 때, 낮은 중요도의 요청을 더 빨리 거부한다.
- 적응형 조절 시스템도 각 중요도에 대해 별도의 통계를 유지한다.

### 중요도와 지연 시간

요청의 중요도는 지연 시간 요구사항과 직교하며 따라서 사용되는 기본 네트워크 QoS(Quality of Service)와도 직교한다.

**예시**: 사용자가 검색 쿼리를 입력하는 동안 시스템이 검색 결과나 제안을 표시할 때, 기본 요청은 매우 sheddable하지만(시스템이 과부하 상태이면 이러한 결과를 표시하지 않는 것이 허용됨) 엄격한 지연 시간 요구사항을 가지는 경향이 있다.

### 중요도의 자동 전파

RPC 시스템을 크게 확장하여 중요도를 자동으로 전파한다. 백엔드가 요청 A를 수신하고, 해당 요청을 실행하는 일부로 다른 백엔드에 나가는 요청 B와 요청 C를 발행하면, 요청 B와 요청 C는 기본적으로 요청 A와 동일한 중요도를 사용한다.

**이점**: RPC 시스템의 일부로 중요도를 표준화하고 전파함으로써, 특정 지점에서 중요도를 일관되게 설정할 수 있다. 이는 과부하된 종속성이 RPC 스택에서 얼마나 깊이 있든 상관없이 트래픽을 거부할 때 원하는 상위 수준 중요도를 준수할 것이라는 확신을 가질 수 있음을 의미한다.

**관행**: 중요도를 브라우저나 모바일 클라이언트에 가능한 한 가까이 설정한다—일반적으로 반환될 HTML을 생성하는 HTTP 프런트엔드에서—그리고 스택의 특정 지점에서 의미가 있는 특정 경우에만 중요도를 재정의한다.

## 활용도 신호 (Utilization Signals)

태스크 수준 과부하 보호 구현은 활용도 개념을 기반으로 한다.

### 활용도 측정

많은 경우 활용도는 단순히 CPU 비율(현재 CPU 비율을 태스크에 예약된 총 CPU로 나눈 값)의 측정이다. 일부 경우에는 예약된 메모리 중 현재 사용 중인 부분과 같은 측정도 고려한다.

활용도가 설정된 임계값에 접근하면, 중요도를 기반으로 요청 거부를 시작한다(더 높은 중요도에 대해 더 높은 임계값).

### Executor Load Average

가장 일반적으로 유용한 신호는 프로세스의 "부하"를 기반으로 하며, 이는 executor load average라는 시스템을 사용하여 결정된다.

**작동 방식**:

- 프로세스에서 활성 스레드 수를 계산한다.
- "활성"은 현재 실행 중이거나 실행 준비가 되어 있고 여유 프로세서를 기다리는 스레드를 의미한다.
- 이 값을 지수 감쇠로 평활화한다.
- 활성 스레드 수가 태스크에 사용 가능한 프로세서 수를 넘어서 증가하면 요청 거부를 시작한다.

**장점**: 매우 큰 팬아웃을 가진 들어오는 요청(즉, 매우 많은 수의 단기 작업 버스트를 스케줄링하는 요청)은 부하가 매우 짧게 급증하게 하지만, 평활화가 대부분 그 스파이크를 흡수한다. 그러나 작업이 단기적이지 않으면(즉, 부하가 증가하고 상당한 시간 동안 높게 유지되면) 태스크는 요청 거부를 시작한다.

### 다른 활용도 신호

시스템은 특정 백엔드가 필요로 할 수 있는 모든 활용도 신호를 플러그인할 수 있다.

**예시**: 메모리 압력—백엔드 태스크의 메모리 사용량이 정상 운영 매개변수를 넘어섰는지 여부를 나타낸다.

**조합**: 시스템은 여러 신호를 결합하고 결합된(또는 개별) 목표 활용도 임계값을 초과할 요청을 거부하도록 구성할 수도 있다.

## 과부하 오류 처리

부하를 우아하게 처리하는 것 외에도, 클라이언트가 부하 관련 오류 응답을 받을 때 어떻게 반응해야 하는지에 대해 상당한 생각을 했다.

### 두 가지 과부하 상황

**1. 데이터센터의 백엔드 태스크 중 큰 서브셋이 과부하 상태**:

- 데이터센터 간 로드밸런싱 시스템이 완벽하게 작동하는 경우(즉, 상태를 전파하고 트래픽 변화에 즉시 반응할 수 있는 경우) 이 조건은 발생하지 않는다.
- **대응**: 요청을 재시도하지 않아야 하며 오류가 호출자에게까지 버블업되어야 한다(예: 최종 사용자에게 오류 반환).

**2. 데이터센터의 백엔드 태스크 중 작은 서브셋이 과부하 상태**:

- 일반적으로 데이터센터 내부의 로드밸런싱의 불완전성으로 인해 발생한다.
- 예: 태스크가 최근에 매우 비싼 요청을 받았을 수 있다.
- 이 경우 데이터센터에 요청을 처리할 다른 태스크에 남은 용량이 있을 가능성이 매우 높다.
- **대응**: 선호되는 응답은 요청을 즉시 재시도하는 것이다.

### 재시도 전략

**로드밸런싱 정책의 관점에서**: 요청의 재시도는 새 요청과 구별할 수 없다. 재시도가 실제로 다른 백엔드 태스크로 가도록 보장하는 명시적 로직을 사용하지 않는다. 단순히 참여하는 백엔드 수의 덕분에 재시도가 다른 백엔드 태스크에 도달할 가능성에 의존한다.

**유기적 로드밸런싱**: 백엔드가 약간만 과부하 상태인 경우에도, 백엔드가 재시도와 새 요청을 동등하고 빠르게 거부하면 클라이언트 요청이 더 잘 제공되는 경우가 많다. 그러면 이러한 요청을 여유 리소스가 있을 수 있는 다른 백엔드 태스크에서 즉시 재시도할 수 있다. 백엔드에서 재시도와 새 요청을 동일하게 취급하는 결과, 서로 다른 태스크에서 요청을 재시도하는 것이 유기적 로드밸런싱의 한 형태가 된다: 해당 요청에 더 적합할 수 있는 태스크로 부하를 리디렉션한다.

## 재시도 결정

클라이언트가 "태스크 과부하" 오류 응답을 받으면, 요청을 재시도할지 여부를 결정해야 한다. 클러스터의 태스크 중 상당 부분이 과부하 상태일 때 재시도를 피하기 위해 몇 가지 메커니즘을 마련했다.

### 1. 요청당 재시도 예산

최대 3회 시도의 요청당 재시도 예산을 구현한다. 요청이 이미 3번 실패했으면 실패를 호출자에게 버블업한다.

**근거**: 요청이 이미 3번 과부하된 태스크에 도달했다면, 다시 시도하는 것이 도움이 될 가능성이 상대적으로 낮다. 전체 데이터센터가 과부하 상태일 가능성이 높기 때문이다.

### 2. 클라이언트당 재시도 예산

각 클라이언트는 재시도에 해당하는 요청의 비율을 추적한다. 이 비율이 10% 미만인 경우에만 요청이 재시도된다.

**근거**: 태스크의 작은 서브셋만 과부하 상태라면 재시도할 필요가 상대적으로 적을 것이다.

**최악의 시나리오 예시**: 데이터센터가 적은 양의 요청을 수락하고 많은 부분의 요청을 거부한다고 가정하자. X를 클라이언트 측 로직에 따라 데이터센터에 대해 시도된 요청의 총 비율이라고 하자. 발생할 재시도 수로 인해 요청 수가 3X 바로 아래로 크게 증가한다. 재시도로 인한 성장을 효과적으로 제한했지만, 요청의 3배 증가는 상당하다. 그러나 클라이언트당 재시도 예산(10% 재시도 비율)을 계층화하면 일반적인 경우 성장이 1.1배로만 줄어든다—상당한 개선이다.

### 3. 재시도 카운터 전파

클라이언트는 요청 메타데이터에 요청이 이미 시도된 횟수의 카운터를 포함한다. 예를 들어, 카운터는 첫 시도에서 0으로 시작하고 매 재시도마다 증가하여 2에 도달하며, 그 시점에서 요청당 예산으로 인해 재시도가 중지된다.

백엔드는 최근 이력에서 이러한 값의 히스토그램을 유지한다. 백엔드가 요청을 거부해야 할 때, 이러한 히스토그램을 참조하여 다른 백엔드 태스크도 과부하 상태일 가능성을 결정한다. 이러한 히스토그램이 상당한 양의 재시도를 나타내면(다른 백엔드 태스크도 과부하 상태일 가능성이 있음을 나타냄), 재시도를 트리거하는 표준 "태스크 과부하" 오류 대신 "과부하; 재시도하지 마세요" 오류 응답을 반환한다.

**시나리오별 예시**: 슬라이딩 윈도우(재시도를 세지 않고 1,000개의 초기 요청에 해당)에 걸쳐 주어진 백엔드 태스크가 받은 각 요청의 시도 횟수를 보면, 단순성을 위해 클라이언트당 재시도 예산은 무시되고(즉, 이 숫자들은 재시도에 대한 유일한 제한이 요청당 3회 시도의 재시도 예산이라고 가정), 서브세팅이 이러한 숫자들을 다소 변경할 수 있다.

### 재시도의 계층적 제한

대규모 서비스는 심층 시스템 스택인 경향이 있으며, 이들은 서로 종속성을 가질 수 있다. 이 아키텍처에서 요청은 거부하는 레이어 바로 위의 레이어에서만 재시도되어야 한다. 주어진 요청을 제공할 수 없고 재시도하지 않아야 한다고 결정하면, "과부하; 재시도하지 마세요" 오류를 사용하여 조합적 재시도 폭발을 피한다.

**예시 (실제로는 스택이 훨씬 더 복잡함)**:

종속성 스택 구조:

Frontend
↓
Backend A
↓
Backend B
↓
DB Frontend

DB Frontend가 현재 과부하 상태이고 요청을 거부한다고 상상해보자:

1. **Backend B의 대응**:

   - Backend B는 앞서 설명한 가이드라인에 따라 요청을 재시도한다.
   - Backend B가 DB Frontend로의 요청을 제공할 수 없다고 판단하면(예: 요청이 이미 3번 시도되고 거부되었기 때문에), Backend B는 Backend A에 "과부하; 재시도하지 마세요" 오류 또는 저하된 응답을 반환해야 한다(DB Frontend로의 요청이 실패했을 때도 어느 정도 유용한 응답을 생성할 수 있다고 가정).

2. **Backend A의 대응**:
   - Backend A는 Frontend로부터 받은 요청에 대해 정확히 동일한 옵션을 가지며, 그에 따라 진행한다.

**핵심 포인트**: DB Frontend로부터의 실패한 요청은 Backend B, 즉 그 바로 위의 레이어에 의해서만 재시도되어야 한다. 여러 레이어가 재시도하면 조합적 폭발이 발생한다.

## 연결로부터의 부하

연결과 관련된 부하는 언급할 가치가 있는 마지막 요인이다.

### 연결 유지 비용

때때로 백엔드가 받는 요청에 의해 직접 발생하는 부하만 고려한다(이는 초당 쿼리 수를 기반으로 부하를 모델링하는 접근법의 문제 중 하나다). 그러나 이렇게 하면 큰 연결 풀을 유지하는 CPU 및 메모리 비용이나 연결의 빠른 처닝 비율의 비용을 간과한다. 이러한 문제는 작은 시스템에서는 무시할 수 있지만, 매우 대규모 RPC 시스템을 실행할 때 빠르게 문제가 된다.

### 비활성 연결 문제

앞서 언급했듯이, RPC 프로토콜은 비활성 클라이언트가 주기적인 상태 확인을 수행하도록 요구한다. 연결이 설정 가능한 시간 동안 유휴 상태였으면, 클라이언트는 TCP 연결을 끊고 상태 확인을 위해 UDP로 전환한다.

**문제점**: 매우 낮은 비율의 요청을 발행하는 매우 많은 수의 클라이언트 태스크가 있을 때 이 동작은 문제가 된다. 연결에 대한 상태 확인이 실제로 요청을 제공하는 것보다 더 많은 리소스를 필요로 할 수 있다.

**해결책**: 연결 매개변수를 신중하게 조정하거나(예: 상태 확인 빈도를 크게 줄임) 동적으로 연결을 생성 및 파괴하는 것과 같은 접근법이 이 상황을 크게 개선할 수 있다.

### 새 연결 요청 버스트 처리

새 연결 요청의 버스트를 처리하는 것은 두 번째(그러나 관련된) 문제다. 매우 많은 수의 작업자 클라이언트 태스크를 한꺼번에 생성하는 매우 큰 배치 작업의 경우 이러한 유형의 버스트가 발생하는 것을 확인했다. 과도한 수의 새 연결을 동시에 협상하고 유지해야 하는 필요성은 백엔드 그룹을 쉽게 과부하시킬 수 있다.

**완화 전략**:

1. **데이터센터 간 로드밸런싱 알고리즘에 부하 노출**:

   - 예를 들어, 단순히 요청 수가 아니라 클러스터의 활용도를 기반으로 로드밸런싱한다.
   - 이 경우 요청으로부터의 부하가 여유 용량이 있는 다른 데이터센터로 효과적으로 재균형된다.

2. **배치 프록시 사용**:
   - 배치 클라이언트 작업이 기본 백엔드로 요청을 전달하고 응답을 제어된 방식으로 클라이언트에 돌려주는 것 외에는 아무것도 하지 않는 별도의 배치 프록시 백엔드 태스크 세트를 사용하도록 의무화한다.
   - 따라서 "배치 클라이언트 → 백엔드" 대신 "배치 클라이언트 → 배치 프록시 → 백엔드"가 된다.
   - 이 경우 매우 큰 작업이 시작되면 배치 프록시 작업만 영향을 받아, 실제 백엔드(및 우선순위가 높은 클라이언트)를 보호한다.
   - 효과적으로 배치 프록시가 퓨즈처럼 작동한다.
   - 프록시를 사용하는 또 다른 장점은 일반적으로 백엔드에 대한 연결 수를 줄여 백엔드에 대한 로드밸런싱을 개선할 수 있다는 것이다(예: 프록시 태스크는 더 큰 서브셋을 사용할 수 있고 아마도 백엔드 태스크의 상태에 대한 더 나은 뷰를 가질 것이다).

## 결론

### 핵심 기술들

이 장과 "데이터센터의 로드밸런싱"에서는 다양한 기술(결정론적 서브세팅, 가중 라운드 로빈, 클라이언트 측 조절, 고객 할당량 등)이 데이터센터의 태스크에 걸쳐 부하를 상대적으로 균등하게 분산하는 데 어떻게 도움이 되는지 논의했다.

**중요한 한계**: 이러한 메커니즘은 분산 시스템에 걸친 상태의 전파에 의존한다. 일반적인 경우에는 합리적으로 잘 작동하지만, 실제 적용 결과 불완전하게 작동하는 소수의 상황이 발생했다.

### 개별 태스크 보호의 중요성

**결과적으로**: 개별 태스크가 과부하로부터 보호되도록 보장하는 것이 중요하다고 생각한다.

**간단히 말하면**: 특정 트래픽 비율을 제공하도록 프로비저닝된 백엔드 태스크는 태스크에 얼마나 많은 초과 트래픽이 던져지든 상관없이 지연 시간에 대한 어떠한 상당한 영향 없이 해당 비율로 트래픽을 계속 제공해야 한다.

**부수적 결과**: 백엔드 태스크는 부하 하에서 넘어지고 충돌하지 않아야 한다.

이러한 진술은 태스크가 처리하도록 프로비저닝된 것의 2배 또는 심지어 10배 이상의 특정 트래픽 비율까지 사실이어야 한다. 시스템이 고장나기 시작하는 특정 지점이 있을 수 있으며, 이 고장이 발생하는 임계값을 높이는 것은 달성하기 상대적으로 어려워질 수 있다는 것을 받아들인다.

### 성능 저하 조건을 진지하게 받아들이기

**핵심**: 이러한 성능 저하 조건을 진지하게 받아들이는 것이다.

**이러한 성능 저하 조건이 무시될 때**: 많은 시스템이 끔찍한 동작을 보인다. 작업이 쌓이고 태스크가 결국 메모리를 소진하고 충돌하면(또는 메모리 스래싱으로 거의 모든 CPU를 소모하게 되면), 트래픽이 드롭되고 태스크가 리소스를 놓고 경쟁하면서 지연 시간이 악화된다.

**방치될 경우**: 시스템의 서브셋(예: 개별 백엔드 태스크)의 실패가 다른 시스템 구성 요소의 실패를 트리거하여 잠재적으로 전체 시스템(또는 상당한 서브셋)이 실패하게 할 수 있다. 이러한 종류의 연쇄 실패로부터의 영향은 너무 심각하여 대규모로 운영되는 모든 시스템이 이에 대해 보호하는 것이 중요하다. "연쇄 실패 다루기"를 참조하라.

### 일반적인 실수: 모든 트래픽 거부

과부하된 백엔드가 모든 트래픽 수락을 중단하고 거부해야 한다고 가정하는 것은 일반적인 실수다. 그러나 이 가정은 실제로 강력한 로드밸런싱의 목표에 반한다.

**실제로 원하는 것**: 백엔드가 가능한 한 많은 트래픽을 계속 수락하되, 용량이 확보될 때만 해당 부하를 수락하는 것이다.

**잘 동작하는 백엔드**: 강력한 로드밸런싱 정책의 지원을 받는 잘 동작하는 백엔드는 처리할 수 있는 요청만 수락하고 나머지는 우아하게 거부해야 한다.

### 마법의 탄환은 없다

좋은 로드밸런싱과 과부하 보호를 구현하기 위한 방대한 도구 배열을 가지고 있지만, 마법의 탄환은 없다. 로드밸런싱은 종종 시스템과 요청의 의미론에 대한 깊은 이해가 필요하다.

**진화**: 이 장에서 설명한 기술들은 구글의 많은 시스템의 요구와 함께 진화했으며, 시스템의 특성이 계속 변화함에 따라 계속 진화할 것이다.

### 요약

**과부하 처리의 핵심 원칙**:

1. **저하된 응답 우선**: 가능한 경우 오류 대신 저하된 응답 제공
2. **리소스 기반 용량 측정**: "초당 쿼리 수" 대신 실제 리소스(CPU, 메모리)로 용량 측정
3. **고객별 제한**: 글로벌 과부하 시 잘못 동작하는 고객만 영향받도록 보장
4. **클라이언트 측 조절**: 적응형 조절로 백엔드 보호
5. **중요도 구분**: 요청의 중요도에 따라 차등적으로 처리
6. **활용도 신호**: 실시간 활용도를 기반으로 과부하 감지 및 대응
7. **지능적 재시도**: 재시도 예산과 전략을 통해 조합적 재시도 폭발 방지
8. **개별 태스크 보호**: 각 태스크가 과부하로부터 독립적으로 보호되도록 보장

**최종 목표**: 시스템이 과부하 상황에서도 계속 작동하며, 처리할 수 있는 만큼의 요청을 수락하고 나머지는 우아하게 거부하는 것이다.

# 22. 연쇄적 장애 다루기

"처음에 성공하지 못하면 지수적으로 백오프하라.
사람들은 왜 항상 약간의 지터를 추가해야 한다는 것을 잊는 걸까?"

## 연쇄적 장애란?

연쇄적 장애(Cascading Failure)는 긍정 피드백(positive feedback)의 결과로 시간이 지남에 따라 커지는 장애다. 전체 시스템의 일부가 실패하면 시스템의 다른 부분이 실패할 확률이 증가할 때 발생할 수 있다.

**예시**: 서비스의 단일 복제본이 과부하로 인해 실패하면 남은 복제본의 부하가 증가하고 실패 확률이 높아져, 서비스의 모든 복제본을 다운시키는 도미노 효과가 발생한다.

## 연쇄적 장애의 원인과 설계로 피하기

잘 설계된 시스템은 대부분의 연쇄적 장애를 설명하는 몇 가지 전형적인 시나리오를 고려해야 한다.

### 서버 과부하

연쇄적 장애의 가장 일반적인 원인은 과부하다. 여기서 설명하는 대부분의 연쇄적 장애는 서버 과부하로 인해 직접 발생하거나 이 시나리오의 확장 또는 변형으로 인한 것이다.

**시나리오**:

- 클러스터 A의 프런트엔드가 초당 1,000개의 요청(QPS)을 처리하고 있다.
- 클러스터 B가 실패하면 클러스터 A로의 요청이 1,200 QPS로 증가한다.
- A의 프런트엔드는 1,200 QPS에서 요청을 처리할 수 없으며, 리소스가 부족하여 충돌하거나 데드라인을 놓치거나 잘못 동작하기 시작한다.
- 결과적으로 A에서 성공적으로 처리된 요청의 비율이 1,000 QPS 훨씬 아래로 떨어진다.

**확산**: 이러한 유용한 작업 수행 비율의 감소는 다른 장애 도메인으로 확산되어 잠재적으로 전역적으로 확산될 수 있다. 예를 들어, 한 클러스터의 로컬 과부하는 서버가 충돌하도록 만들 수 있고, 이에 대응하여 로드밸런싱 컨트롤러가 다른 클러스터로 요청을 보내 해당 서버를 과부하시켜 서비스 전체의 과부하 장애로 이어진다.

**시간**: 이러한 이벤트가 발생하는 데 오래 걸리지 않을 수 있다(예: 몇 분 정도). 관련된 로드밸런서와 태스크 스케줄링 시스템이 매우 빠르게 작동할 수 있기 때문이다.

### 리소스 고갈

리소스가 부족하면 높은 지연 시간, 상승된 오류율 또는 낮은 품질의 결과 대체가 발생할 수 있다. 이것들은 실제로 리소스가 부족할 때 원하는 효과다. 부하가 서버가 처리할 수 있는 것을 넘어서 증가함에 따라 결국 무언가가 양보해야 한다.

서버에서 어떤 리소스가 고갈되고 서버가 어떻게 구축되었는지에 따라, 리소스 고갈은 서버를 덜 효율적으로 만들거나 서버를 충돌시켜 로드밸런서가 리소스 문제를 다른 서버로 분산하도록 유발할 수 있다.

#### CPU 고갈

CPU가 요청 부하를 처리하기에 불충분하면 일반적으로 모든 요청이 느려진다. 이 시나리오는 다양한 2차 효과를 초래할 수 있다:

- **진행 중인 요청 수 증가**: 요청을 처리하는 데 더 오래 걸리므로 더 많은 요청이 동시에 처리된다(큐잉이 발생할 수 있는 최대 용량까지). 이것은 메모리, 활성 스레드 수, 파일 디스크립터, 백엔드 리소스를 포함한 거의 모든 리소스에 영향을 미친다.

- **지나치게 긴 큐 길이**: 정상 상태에서 모든 요청을 처리할 충분한 용량이 없으면 서버가 큐를 포화시킨다. 이는 지연 시간이 증가하고(요청이 더 오랫동안 큐에 있음) 큐가 더 많은 메모리를 사용함을 의미한다.

- **스레드 기아**: 스레드가 잠금을 기다리고 있어 진행할 수 없을 때, 상태 확인 엔드포인트를 제시간에 제공할 수 없으면 상태 확인이 실패할 수 있다.

- **CPU 또는 요청 기아**: 서버의 내부 워치독이 서버가 진행하지 못하고 있음을 감지하여 CPU 기아로 인해 또는 워치독 이벤트가 원격으로 트리거되고 요청 큐의 일부로 처리되는 경우 요청 기아로 인해 서버가 충돌하게 한다.

- **RPC 데드라인 누락**: 서버가 과부하되면 클라이언트로의 RPC 응답이 늦게 도착하여 클라이언트가 설정한 데드라인을 초과할 수 있다. 그러면 서버가 응답하기 위해 수행한 작업이 낭비되고 클라이언트가 RPC를 재시도하여 더 많은 과부하로 이어질 수 있다.

- **CPU 캐싱 이점 감소**: 더 많은 CPU가 사용됨에 따라 더 많은 코어로 넘칠 가능성이 증가하여 로컬 캐시 사용이 감소하고 CPU 효율성이 저하된다.

#### 메모리 고갈

진행 중인 요청이 더 많으면 요청, 응답 및 RPC 객체를 할당하는 데서 더 많은 RAM을 소비한다. 메모리 고갈은 다음 효과를 초래할 수 있다:

- **태스크 죽음**: 예를 들어, 사용 가능한 리소스 한계를 초과하여 컨테이너 관리자(VM 또는 기타)에 의해 태스크가 제거되거나 애플리케이션 특정 충돌이 태스크를 죽게 할 수 있다.

- **Java에서 가비지 컬렉션(GC) 비율 증가로 인한 CPU 사용량 증가**: 이 시나리오에서 악순환이 발생할 수 있다. CPU가 덜 사용 가능하면 요청이 느려지고, RAM 사용이 증가하고, GC가 더 많이 발생하여 CPU 가용성이 더욱 낮아진다. 이것은 구어체로 "GC 데스 스파이럴(GC death spiral)"로 알려져 있다.

- **캐시 히트율 감소**: 사용 가능한 RAM의 감소는 애플리케이션 레벨 캐시 히트율을 감소시켜 백엔드로의 RPC를 더 많이 발생시킬 수 있으며, 이는 백엔드를 과부하시킬 수 있다.

#### 스레드 고갈

스레드 기아는 오류를 직접 초래하거나 상태 확인 실패로 이어질 수 있다. 서버가 필요에 따라 스레드를 추가하면 스레드 오버헤드가 너무 많은 RAM을 사용할 수 있다. 극단적인 경우 스레드 기아는 프로세스 ID가 부족하게 만들 수도 있다.

#### 파일 디스크립터 고갈

파일 디스크립터가 부족하면 네트워크 연결을 초기화할 수 없게 되어 상태 확인이 실패할 수 있다.

#### 리소스 간 종속성

이러한 리소스 고갈 시나리오 중 많은 것이 서로에게서 발생한다는 점에 유의하라. 과부하를 경험하는 서비스는 종종 근본 원인처럼 보일 수 있는 많은 2차 증상을 가지고 있어 디버깅이 어렵다.

**복잡한 시나리오 예시**:

1. Java 프런트엔드에 잘못 조정된 가비지 컬렉션(GC) 매개변수가 있다.
2. 높은(그러나 예상된) 부하 하에서 프런트엔드가 GC로 인해 CPU가 부족해진다.
3. CPU 고갈로 요청 완료가 느려진다.
4. 진행 중인 요청 수 증가로 요청을 처리하는 데 더 많은 RAM이 사용된다.
5. 요청으로 인한 메모리 압력과 전체 프런트엔드 프로세스에 대한 고정된 메모리 할당의 조합으로 캐싱에 사용 가능한 RAM이 줄어든다.
6. 캐시 크기 감소는 캐시의 항목이 적고 히트율이 낮아짐을 의미한다.
7. 캐시 미스 증가는 더 많은 요청이 서비스를 위해 백엔드로 넘어감을 의미한다.
8. 백엔드가 차례로 CPU 또는 스레드가 부족해진다.
9. 마지막으로 CPU 부족으로 기본 상태 확인이 실패하여 연쇄적 장애가 시작된다.

이처럼 복잡한 상황에서는 장애 중에 인과 관계 체인이 완전히 진단될 가능성이 낮다. 백엔드 충돌이 프런트엔드의 캐시 비율 감소로 인한 것임을 판단하기 매우 어려울 수 있으며, 특히 프런트엔드와 백엔드 구성 요소가 다른 소유자를 가진 경우 더욱 그렇다.

### 서비스 사용 불가

리소스 고갈은 서버 충돌로 이어질 수 있다. 예를 들어, 컨테이너에 너무 많은 RAM이 할당되면 서버가 충돌할 수 있다. 과부하로 인해 몇 개의 서버가 충돌하면 남은 서버의 부하가 증가하여 그들도 충돌하게 만들 수 있다. 문제는 눈덩이처럼 커지는 경향이 있으며 곧 모든 서버가 충돌 루프에 빠지기 시작한다.

이 시나리오에서 벗어나기 어려운 경우가 많다. 서버가 온라인으로 돌아오자마자 매우 높은 비율의 요청에 공격받아 거의 즉시 실패하기 때문이다.

**예시**: 서비스가 10,000 QPS에서 정상이었지만 11,000 QPS에서 충돌로 인한 연쇄적 장애를 시작했다면, 부하를 9,000 QPS로 줄이는 것이 충돌을 거의 확실히 멈추지 못할 것이다. 서비스가 감소된 용량으로 증가된 수요를 처리하기 때문이다. 일반적으로 서버의 작은 부분만 요청을 처리할 수 있을 만큼 정상일 것이다.

정상적인 서버의 비율은 몇 가지 요인에 따라 달라진다: 시스템이 태스크를 시작할 수 있는 속도, 바이너리가 전체 용량으로 서비스를 시작할 수 있는 속도, 새로 시작한 태스크가 부하를 견딜 수 있는 시간. 이 예에서 서버의 10%가 요청을 처리할 수 있을 만큼 정상이라면, 시스템이 안정화되고 복구되기 위해서는 요청 비율이 약 1,000 QPS로 떨어져야 할 것이다.

유사하게, 서버는 로드밸런싱 레이어에 비정상으로 보일 수 있어 로드밸런싱 용량이 감소할 수 있다. 서버가 "lame duck" 상태로 들어가거나 충돌 없이 상태 확인을 실패할 수 있다. 효과는 충돌과 매우 유사할 수 있다: 더 많은 서버가 비정상으로 보이고, 정상 서버는 비정상이 되기 전에 매우 짧은 시간 동안만 요청을 수락하는 경향이 있으며, 더 적은 서버가 요청 처리에 참여한다.

### 서버 과부하 방지

서버 과부하를 피하기 위한 전략을 대략적인 우선순위 순서로 제시한다:

**1. 서버의 용량 한계를 부하 테스트하고, 과부하에 대한 실패 모드 테스트**

- 이것은 서버 과부하를 방지하기 위해 수행해야 하는 가장 중요한 연습이다.
- 현실적인 환경에서 테스트하지 않으면 정확히 어떤 리소스가 고갈될지와 해당 리소스 고갈이 어떻게 나타날지 예측하기 매우 어렵다.

**2. 저하된 결과 제공**

- 사용자에게 더 낮은 품질의, 계산하기 더 저렴한 결과를 제공한다.
- 전략은 서비스별로 달라진다.

**3. 과부하될 때 요청을 거부하도록 서버 계측**

- 서버는 과부하되고 충돌하는 것으로부터 스스로를 보호해야 한다.
- 프런트엔드 또는 백엔드 레이어에서 과부하될 때 조기에 그리고 저렴하게 실패한다.

**4. 서버를 과부하시키는 것이 아니라 요청을 거부하도록 상위 레벨 시스템 계측**

- 속도 제한은 여러 곳에서 구현될 수 있다:
  - 역방향 프록시에서: DoS 공격과 악의적 클라이언트를 완화하기 위해 IP 주소와 같은 기준으로 요청 볼륨 제한
  - 로드밸런서에서: 서비스가 전역 과부하에 진입하면 요청 드롭
  - 개별 태스크에서: 로드밸런싱의 무작위 변동이 서버를 압도하는 것을 방지

**5. 용량 계획 수행**

- 좋은 용량 계획은 연쇄적 장애가 발생할 확률을 줄일 수 있다.
- 용량 계획은 서비스가 실패할 부하를 결정하기 위한 성능 테스트와 결합되어야 한다.

**주의**: 용량 계획은 연쇄적 장애를 트리거할 확률을 줄이지만 서비스를 연쇄적 장애로부터 보호하기에 충분하지 않다.

### 큐 관리

대부분의 스레드 per-request 서버는 요청을 처리하기 위해 스레드 풀 앞에 큐를 사용한다. 요청이 들어오고 큐에 앉아 있다가 스레드가 큐에서 요청을 꺼내 실제 작업을 수행한다. 일반적으로 큐가 가득 차면 서버는 새 요청을 거부한다.

주어진 태스크의 요청 비율과 지연 시간이 일정하면 요청을 큐에 넣을 이유가 없다. 일정한 수의 스레드가 점유되어야 한다. 이 이상화된 시나리오 하에서 요청은 들어오는 요청의 정상 상태 비율이 서버가 요청을 처리할 수 있는 비율을 초과하는 경우에만 큐에 들어가며, 이는 스레드 풀과 큐 모두의 포화를 초래한다.

큐에 있는 요청은 메모리를 소비하고 지연 시간을 증가시킨다. 예를 들어, 큐 크기가 스레드 수의 10배이고 스레드에서 요청을 처리하는 시간이 100밀리초라면, 큐가 가득 찬 경우 요청을 처리하는 데 1.1초가 걸리며 대부분의 시간이 큐에 소비된다.

시간이 지남에 따라 상당히 안정적인 트래픽을 가진 시스템의 경우 일반적으로 스레드 풀 크기에 비해 작은 큐 길이(예: 50% 이하)를 갖는 것이 더 낫다. 이는 서버가 들어오는 요청 비율을 유지할 수 없을 때 조기에 요청을 거부하게 한다.

"버스티(bursty)" 부하를 가진 시스템의 경우, 트래픽 패턴이 급격하게 변동하는 경우 현재 사용 중인 스레드 수, 각 요청에 대한 처리 시간, 버스트의 크기와 빈도를 기반으로 한 큐 크기로 더 나을 수 있다.

### 부하 차단과 우아한 성능 저하

부하 차단(Load Shedding)은 서버가 과부하 조건에 접근함에 따라 트래픽을 드롭하여 일부 비율의 부하를 드롭한다. 목표는 서버가 RAM 부족, 상태 확인 실패, 매우 높은 지연 시간으로의 서비스, 또는 과부하와 관련된 다른 증상을 겪지 않도록 유지하면서 가능한 한 많은 유용한 작업을 수행하는 것이다.

**부하 차단의 간단한 방법**: CPU, 메모리 또는 큐 길이를 기반으로 태스크당 조절을 수행한다.

**효과적인 접근법**: 진행 중인 클라이언트 요청이 주어진 수보다 많을 때 들어오는 모든 요청에 HTTP 503(서비스 사용 불가)을 반환한다.

**큐잉 방법 변경**: 표준 선입선출(FIFO)에서 후입선출(LIFO)로 변경하거나 제어된 지연(CoDel) 알고리즘을 사용하면 처리할 가치가 없는 요청을 제거하여 부하를 줄일 수 있다. 사용자의 웹 검색이 RPC가 10초 동안 큐에 있었기 때문에 느리다면, 사용자가 포기하고 브라우저를 새로 고쳐 또 다른 요청을 발행했을 가능성이 높다. 첫 번째 요청에 응답할 필요가 없다. 무시될 것이기 때문이다!

**우아한 성능 저하(Graceful Degradation)**: 부하 차단의 개념을 한 단계 더 나아가 수행해야 하는 작업의 양을 줄인다. 일부 애플리케이션에서는 응답의 품질을 낮춤으로써 필요한 작업의 양이나 시간을 크게 줄일 수 있다.

**예시**: 검색 애플리케이션은 과부하될 때 전체 온디스크 데이터베이스가 아닌 인메모리 캐시에 저장된 데이터의 서브셋만 검색하거나 덜 정확하지만 더 빠른 랭킹 알고리즘을 사용할 수 있다.

## 재시도

재시도는 시스템을 불안정하게 만들 수 있다.

**시나리오**:

1. 백엔드가 태스크당 알려진 10,000 QPS 한계를 가지고 있으며, 그 이후의 모든 추가 요청은 우아한 성능 저하 시도로 거부된다고 가정한다.
2. 프런트엔드가 일정한 10,100 QPS의 비율로 요청을 호출하고 100 QPS로 백엔드를 과부하시키며, 백엔드는 이를 거부한다.
3. 실패한 100 QPS는 1,000ms마다 재시도되고 아마도 성공한다. 그러나 재시도 자체가 백엔드로 전송되는 요청에 추가되어 이제 백엔드가 10,200 QPS를 받게 되며, 그 중 200 QPS가 과부하로 인해 실패한다.
4. 재시도 볼륨이 증가한다: 첫 번째 초의 100 QPS 재시도는 200 QPS로, 그 다음 300 QPS로 이어진다. 점점 더 적은 요청이 첫 시도에 성공할 수 있어 백엔드로의 요청 중 일부로서 수행되는 유용한 작업이 줄어든다.
5. 백엔드 태스크가 부하 증가를 처리할 수 없으면—백엔드에서 파일 디스크립터, 메모리, CPU 시간을 소비하는—요청과 재시도의 순전한 부하 하에서 녹아내리고 충돌할 수 있다.

**자동 재시도를 발행할 때 고려사항**:

1. **무작위화된 지수 백오프를 항상 사용**: 재시도를 스케줄링할 때 무작위화된 지수 백오프를 사용한다.

2. **요청당 재시도 제한**: 주어진 요청을 무한정 재시도하지 않는다.

3. **서버 전체 재시도 예산 고려**: 예를 들어, 프로세스에서 분당 60번의 재시도만 허용하고, 재시도 예산이 초과되면 재시도하지 않고 요청을 실패시킨다.

4. **서비스를 전체적으로 생각**: 주어진 레벨에서 재시도를 수행할 필요가 있는지 결정한다. 특히 여러 레벨에서 재시도를 발행하여 재시도를 증폭하는 것을 피한다.

5. **명확한 응답 코드 사용**: 다양한 실패 모드가 어떻게 처리되어야 하는지 고려한다. 예를 들어, 재시도 가능한 오류 조건과 재시도 불가능한 오류 조건을 분리한다.

## 지연 시간과 데드라인

### 데드라인 선택

데드라인을 설정하는 것이 일반적으로 현명하다. 데드라인을 설정하지 않거나 매우 높은 데드라인을 설정하면 오래 전에 지나간 단기 문제가 서버 재시작까지 계속해서 서버 리소스를 소비하게 만들 수 있다.

높은 데드라인은 스택의 하위 레벨에 문제가 있을 때 스택의 상위 레벨에서 리소스 소비를 초래할 수 있다. 짧은 데드라인은 일부 더 비싼 요청이 일관되게 실패하게 만들 수 있다.

### 데드라인 누락

많은 연쇄적 장애의 공통 주제는 서버가 클라이언트에서 데드라인을 초과할 요청을 처리하는 데 리소스를 소비한다는 것이다. 결과적으로 진행이 이루어지지 않는 동안 리소스가 소비된다. RPC에서는 늦은 과제에 대한 크레딧을 받지 못한다.

RPC가 클라이언트가 설정한 10초 데드라인을 가지고 있다고 가정하자. 서버가 매우 과부하되어 결과적으로 큐에서 스레드 풀로 이동하는 데 11초가 걸린다. 이 시점에서 클라이언트는 이미 요청을 포기했다. 대부분의 상황에서 서버가이 요청을 처리하려고 시도하는 것은 현명하지 않을 것이다. 크레딧을 받을 수 없는 작업을 하는 것이기 때문이다. 클라이언트는 데드라인이 지난 후에 서버가 수행하는 작업에 관심이 없다. 요청을 이미 포기했기 때문이다.

요청 처리가 여러 단계에 걸쳐 수행되는 경우(예: 몇 개의 콜백과 RPC 호출이 있는 경우), 서버는 요청에 대해 더 많은 작업을 수행하기 전에 각 단계에서 남은 데드라인을 확인해야 한다.

### 데드라인 전파 (Deadline Propagation)

백엔드로 RPC를 보낼 때 데드라인을 발명하는 대신, 서버는 데드라인 전파를 사용해야 한다.

**데드라인 전파 작동 방식**:

- 데드라인이 스택의 높은 곳(예: 프런트엔드)에서 설정된다.
- 초기 요청에서 나오는 RPC 트리는 모두 동일한 절대 데드라인을 갖게 된다.

**예시**:

- 서버 A가 30초 데드라인을 선택하고, 서버 B로 RPC를 보내기 전에 7초 동안 요청을 처리한다면, A에서 B로의 RPC는 23초 데드라인을 갖는다.
- 서버 B가 요청을 처리하는 데 4초가 걸리고 서버 C로 RPC를 보낸다면, B에서 C로의 RPC는 19초 데드라인을 갖는다.

**데드라인 전파 없이 발생할 수 있는 시나리오**:

1. 서버 A가 10초 데드라인으로 서버 B에 RPC를 보낸다.
2. 서버 B가 요청 처리를 시작하는 데 8초가 걸리고 서버 C로 RPC를 보낸다.
3. 서버 B가 데드라인 전파를 사용하면 2초 데드라인을 설정해야 하지만, 대신 하드코딩된 20초 데드라인을 사용한다고 가정한다.
4. 서버 C가 5초 후에 큐에서 요청을 꺼낸다.

서버 B가 데드라인 전파를 사용했다면, 서버 C는 2초 데드라인이 초과되었기 때문에 즉시 요청을 포기할 수 있었다. 그러나 이 시나리오에서 서버 C는 15초의 여유가 있다고 생각하고 요청을 처리하지만, 서버 A에서 서버 B로의 요청이 이미 데드라인을 초과했기 때문에 유용한 작업을 하지 않는다.

**추가 고려사항**:

- 네트워크 전송 시간과 클라이언트의 사후 처리를 고려하기 위해 나가는 데드라인을 약간(예: 몇 백 밀리초) 줄이고 싶을 수 있다.
- 나가는 데드라인에 대한 상한선 설정을 고려하라. 비중요 백엔드로의 RPC를 기다리는 시간 또는 일반적으로 짧은 기간에 완료되는 백엔드로의 RPC를 제한하고 싶을 수 있다.

### 취소 전파 (Cancellation Propagation)

취소를 전파하면 RPC 호출 스택의 서버들에게 그들의 노력이 더 이상 필요하지 않다고 알려줌으로써 불필요하거나 실패할 운명인 작업을 줄인다.

지연 시간을 줄이기 위해 일부 시스템은 "헤지된 요청(hedged requests)"을 사용하여 기본 서버로 RPC를 보낸 다음, 기본 서버가 응답하는 데 느린 경우를 대비하여 나중에 동일한 서비스의 다른 인스턴스로 동일한 요청을 보낸다. 클라이언트가 어느 서버로부터든 응답을 받으면 다른 서버들에게 이제 불필요한 요청을 취소하도록 메시지를 보낸다.

### 이중 모드 지연 시간 (Bimodal Latency)

**시나리오**:

- 프런트엔드가 10개의 서버로 구성되어 있으며, 각각 100개의 워커 스레드를 가지고 있다고 가정하자. 이는 프런트엔드가 총 1,000개 스레드의 용량을 가지고 있음을 의미한다.
- 일반적인 작동 중에 프런트엔드는 1,000 QPS를 수행하고 요청은 100ms에 완료된다. 이는 프런트엔드가 일반적으로 구성된 1,000개의 워커 스레드 중 100개의 워커 스레드를 점유하고 있음을 의미한다(1,000 QPS × 0.1초).
- 어떤 이벤트가 5%의 요청이 완료되지 않도록 만든다고 가정하자. 이것은 일부 Bigtable 행 범위의 사용 불가능으로 인한 결과일 수 있으며, 해당 Bigtable 키스페이스에 해당하는 요청을 서비스 불가능하게 만든다.
- 결과적으로 5%의 요청은 데드라인에 도달하고, 나머지 95%의 요청은 일반적인 100ms가 걸린다.

**100초 데드라인 사용 시**:

- 5%의 요청은 5,000개의 스레드를 소비할 것이다(50 QPS × 100초).
- 그러나 프런트엔드는 그렇게 많은 스레드를 사용할 수 없다.
- 다른 2차 효과가 없다고 가정하면, 프런트엔드는 요청의 19.6%만 처리할 수 있다(1,000개의 사용 가능한 스레드 / (5,000 + 95)개의 스레드 분량의 작업), 80.4%의 오류율을 초래한다.

따라서 5%의 요청만 오류를 받는 대신(키스페이스 사용 불가능으로 인해 완료되지 않은 것들), 대부분의 요청이 오류를 받는다.

**이 문제 클래스를 다루기 위한 가이드라인**:

1. **감지의 어려움**: 이 문제를 감지하는 것이 매우 어려울 수 있다. 특히 평균 지연 시간을 볼 때 이중 모드 지연 시간이 장애의 원인임이 명확하지 않을 수 있다. 지연 시간 증가를 볼 때 평균뿐만 아니라 지연 시간의 분포를 보려고 노력하라.

2. **조기 오류 반환**: 완료되지 않는 요청이 전체 데드라인을 기다리는 대신 조기에 오류를 반환하면 이 문제를 피할 수 있다.

3. **데드라인 크기**: 평균 요청 지연 시간보다 몇 배 이상 긴 데드라인을 갖는 것은 일반적으로 좋지 않다. 앞의 예에서 소수의 요청이 처음에 데드라인에 도달했지만, 데드라인이 정상 평균 지연 시간보다 3배 이상 컸기 때문에 스레드 고갈로 이어졌다.

4. **키스페이스별 제한**: 어떤 키스페이스에 의해 고갈될 수 있는 공유 리소스를 사용할 때, 해당 키스페이스에 의한 진행 중인 요청을 제한하거나 다른 종류의 남용 추적을 사용하는 것을 고려하라.

## 느린 시작과 콜드 캐싱

프로세스는 시작 직후에는 정상 상태보다 요청에 응답하는 것이 느린 경우가 많다. 이러한 느림은 다음 중 하나 또는 둘 다로 인해 발생할 수 있다:

### 필요한 초기화

- 주어진 백엔드가 필요한 첫 요청을 받을 때 연결 설정
- 일부 언어, 특히 Java에서의 런타임 성능 개선
- Just-In-Time 컴파일, 핫스팟 최적화, 지연된 클래스 로딩

### 캐시 효과

일부 바이너리는 캐시가 채워지지 않았을 때 덜 효율적이다. 예를 들어, 구글의 일부 서비스의 경우 대부분의 요청이 캐시에서 서비스되므로 캐시를 놓치는 요청은 훨씬 더 비싸다. 따뜻한 캐시로 정상 상태 작동에서는 몇 개의 캐시 미스만 발생하지만, 캐시가 완전히 비어 있으면 100%의 요청이 비싸다.

**콜드 캐시로 이어질 수 있는 시나리오**:

1. **새 클러스터 시작**: 새로 추가된 클러스터는 빈 캐시를 가질 것이다.

2. **유지보수 후 클러스터를 서비스로 복귀**: 캐시가 오래되었을 수 있다.

3. **재시작**: 캐시를 가진 태스크가 최근에 재시작되면 캐시를 채우는 데 시간이 걸릴 것이다.

### 캐싱 전략

캐싱이 서비스에 상당한 영향을 미치는 경우, 다음 전략 중 하나 또는 일부를 사용하고 싶을 수 있다:

1. **서비스 과다 프로비저닝**:

   - 지연 시간 캐시와 용량 캐시를 구분하는 것이 중요하다.
   - 지연 시간 캐시가 사용될 때 서비스는 빈 캐시로도 예상 부하를 유지할 수 있다.
   - 용량 캐시를 사용하는 서비스는 빈 캐시 하에서 예상 부하를 유지할 수 없다.

2. **일반적인 연쇄적 장애 방지 기술 사용**:

   - 특히 서버는 과부하될 때 요청을 거부하거나 성능 저하 모드에 들어가야 한다.
   - 대규모 재시작과 같은 이벤트 후에 서비스가 어떻게 동작하는지 테스트를 수행해야 한다.

3. **클러스터에 부하를 추가할 때 천천히 증가**:
   - 처음에 작은 요청 비율이 캐시를 예열한다.
   - 캐시가 따뜻해지면 더 많은 트래픽을 추가할 수 있다.
   - 모든 클러스터가 명목상 부하를 전달하고 캐시가 따뜻하게 유지되도록 하는 것이 좋은 생각이다.

## 스택에서 항상 아래로만 이동

예시 셰익스피어 서비스에서 프런트엔드는 백엔드와 통신하고, 백엔드는 차례로 저장소 레이어와 통신한다. 저장소 레이어에서 나타나는 문제는 이와 통신하는 서버에 문제를 일으킬 수 있지만, 저장소 레이어를 수정하면 일반적으로 백엔드와 프런트엔드 레이어 모두를 복구한다.

그러나 백엔드가 서로 간에 교차 통신한다고 가정하자. 예를 들어, 저장소 레이어가 요청을 서비스할 수 없을 때 사용자를 소유하는 사람을 변경하기 위해 백엔드가 서로에게 요청을 프록시할 수 있다.

**레이어 내 통신의 문제점**:

1. **분산 교착 상태에 취약**: 백엔드는 동일한 스레드 풀을 사용하여 원격 백엔드로 보낸 RPC를 기다리면서 원격 백엔드로부터 요청을 동시에 받을 수 있다.

2. **부하 증가 시 레이어 내 통신 증가**: 어떤 종류의 실패 또는 무거운 부하 조건에 대응하여 레이어 내 통신이 증가하면(예: 높은 부하 하에서 더 활성화되는 부하 재균형), 부하가 충분히 증가하면 레이어 내 통신이 낮은 레이어 내 요청 모드에서 높은 모드로 빠르게 전환할 수 있다.

3. **시스템 부트스트래핑 복잡성**: 교차 레이어 통신의 중요도에 따라 시스템 부트스트래핑이 더 복잡해질 수 있다.

**권장사항**: 사용자 요청 경로에서 레이어 내 통신, 즉 통신 경로의 가능한 순환을 피하는 것이 일반적으로 더 낫다. 대신 클라이언트가 통신을 하도록 한다.

**예시**: 프런트엔드가 백엔드와 통신하지만 잘못된 백엔드를 추측하는 경우, 백엔드는 올바른 백엔드로 프록시하지 말아야 한다. 대신 백엔드는 프런트엔드에게 올바른 백엔드에서 요청을 재시도하도록 알려야 한다.

## 연쇄적 장애의 트리거 조건

서비스가 연쇄적 장애에 취약한 경우, 도미노 효과를 시작할 수 있는 몇 가지 가능한 교란이 있다.

### 프로세스 죽음

일부 서버 태스크가 죽어 사용 가능한 용량이 감소할 수 있다. 태스크는 Query of Death(내용이 프로세스의 실패를 트리거하는 RPC), 클러스터 문제, 어설션 실패 또는 기타 여러 이유로 인해 죽을 수 있다.

### 프로세스 업데이트

바이너리의 새 버전을 푸시하거나 구성을 업데이트하면 많은 수의 태스크가 동시에 영향을 받는 경우 연쇄적 장애가 시작될 수 있다.

### 새로운 롤아웃

새 바이너리, 구성 변경 또는 기본 인프라 스택의 변경은 요청 프로필, 리소스 사용 및 한계, 백엔드 또는 연쇄적 장애를 트리거할 수 있는 여러 다른 시스템 구성 요소의 변경을 초래할 수 있다.

### 유기적 성장

많은 경우 연쇄적 장애는 특정 서비스 변경에 의해 트리거되지 않고, 사용량의 성장이 용량 조정을 동반하지 않았기 때문에 발생한다.

### 계획된 변경, 드레인 또는 종료

서비스가 멀티홈인 경우 클러스터의 유지보수 또는 장애로 인해 용량의 일부가 사용 불가능할 수 있다.

### 요청 프로필 변경

프런트엔드 서비스가 로드밸런싱 구성 변경, 트래픽 믹스의 변경 또는 클러스터 충만도로 인해 트래픽을 이동했기 때문에 백엔드 서비스가 다른 클러스터로부터 요청을 받을 수 있다.

### 리소스 한계

일부 클러스터 운영 체제는 리소스 과다 약정을 허용한다. CPU는 대체 가능한 리소스다. 종종 일부 머신은 약간의 여유 CPU를 사용할 수 있어 CPU 스파이크에 대한 약간의 안전망을 제공한다.

**주의**: 이 여유 CPU를 안전망으로 의존하는 것은 위험하다. 가용성은 클러스터의 다른 작업의 동작에 전적으로 의존하므로 언제든지 갑자기 사라질 수 있다.

## 연쇄적 장애 테스트

서비스가 실패할 특정 방법을 원칙에서 예측하는 것이 매우 어려울 수 있다. 이 섹션에서는 서비스가 연쇄적 장애에 취약한지 감지할 수 있는 테스트 전략을 논의한다.

### 실패할 때까지 그리고 그 이상으로 테스트

무거운 부하 하에서 서비스의 동작을 이해하는 것은 아마도 연쇄적 장애를 피하는 데 가장 중요한 첫 단계일 것이다.

**부하 테스트**:

- 구성 요소가 깨질 때까지 부하 테스트한다.
- 부하가 증가함에 따라 구성 요소는 일반적으로 더 많은 요청을 처리할 수 없는 지점에 도달할 때까지 요청을 성공적으로 처리한다.
- 이 시점에서 구성 요소는 이상적으로 추가 부하에 대응하여 오류 또는 저하된 결과를 제공하기 시작해야 하지만, 성공적으로 처리하는 요청 비율을 크게 줄이지 않아야 한다.

**테스트 시 고려사항**:

- 캐싱 효과로 인해 부하를 점차 증가시키는 것이 즉시 예상 부하 수준으로 증가시키는 것과 다른 결과를 낳을 수 있다.
- 점진적 및 충격 부하 패턴 모두를 테스트하는 것을 고려하라.

### 대형 클라이언트 테스트

대형 클라이언트가 서비스를 어떻게 사용하는지 이해하라. 예를 들어, 클라이언트가:

- 서비스가 다운되는 동안 작업을 큐에 넣을 수 있는지
- 오류에 대해 무작위화된 지수 백오프를 사용하는지
- 많은 양의 부하를 생성할 수 있는 외부 트리거에 취약한지

### 비중요 백엔드 테스트

비중요 백엔드를 테스트하고, 그들의 사용 불가능이 서비스의 중요 구성 요소를 방해하지 않는지 확인하라.

예를 들어, 프런트엔드에 중요 및 비중요 백엔드가 있다고 가정하자. 종종 주어진 요청은 중요 구성 요소(예: 쿼리 결과)와 비중요 구성 요소(예: 철자 제안) 모두를 포함한다. 요청이 비중요 백엔드가 완료되기를 기다리는 동안 상당히 느려지고 리소스를 소비할 수 있다.

## 연쇄적 장애를 다루기 위한 즉각적인 단계

서비스가 연쇄적 장애를 경험하고 있다고 식별하면 상황을 개선하기 위해 몇 가지 다른 전략을 사용할 수 있다.

### 1. 리소스 증가

시스템이 저하된 용량으로 실행 중이고 유휴 리소스가 있다면, 태스크를 추가하는 것이 장애에서 복구하는 가장 빠른 방법이 될 수 있다. 그러나 서비스가 어떤 종류의 데스 스파이럴에 진입한 경우 더 많은 리소스를 추가하는 것이 복구하기에 충분하지 않을 수 있다.

### 2. 상태 확인 실패/죽음 중지

일부 클러스터 스케줄링 시스템은 작업의 태스크 상태를 확인하고 비정상적인 태스크를 재시작한다. 이 관행은 상태 확인 자체가 서비스를 비정상으로 만드는 실패 모드를 생성할 수 있다.

### 3. 서버 재시작

서버가 어떤 식으로든 걸려 있고 진행하지 못하는 경우 재시작하는 것이 도움이 될 수 있다. 다음과 같은 경우 서버 재시작을 시도하라:

- Java 서버가 GC 데스 스파이럴에 있음
- 일부 진행 중인 요청이 데드라인이 없지만 리소스를 소비하고 있어 스레드를 차단함
- 서버가 교착 상태임

**주의**: 서버를 재시작하기 전에 연쇄적 장애의 원인을 식별했는지 확인하라. 이 조치를 취하는 것이 단순히 부하를 이동시키지 않을지 확인하라.

### 4. 트래픽 드롭

부하를 드롭하는 것은 큰 망치로, 일반적으로 진정한 연쇄적 장애를 경험하고 있고 다른 수단으로 수정할 수 없는 상황을 위해 예약된다.

**전략**:

1. 초기 트리거 조건 해결(예: 용량 추가)
2. 충돌이 멈출 만큼 부하를 줄임(공격적으로 하라 - 전체 서비스가 충돌 루핑 중이면 1%의 트래픽만 허용)
3. 대다수의 서버가 정상이 되도록 허용
4. 점진적으로 부하 증가

이 전략은 부하가 정상 수준으로 돌아가기 전에 캐시가 예열되고 연결이 설정되는 등을 허용한다.

### 5. 성능 저하 모드 진입

중요하지 않은 트래픽을 드롭하거나 적은 작업을 수행하여 저하된 결과를 제공한다.

### 6. 배치 부하 제거

일부 서비스는 중요하지만 필수적이지 않은 부하를 가지고 있다. 이러한 부하 소스를 끄는 것을 고려하라.

### 7. 나쁜 트래픽 제거

일부 쿼리가 무거운 부하나 충돌을 생성하는 경우(예: Query of Death), 이들을 차단하거나 다른 수단을 통해 제거하는 것을 고려하라.

## 셰익스피어와 연쇄적 장애

셰익스피어 작품에 대한 다큐멘터리가 일본에서 방송되고 우리의 셰익스피어 서비스를 추가 연구를 위한 훌륭한 장소로 명시적으로 가리킨다. 방송 후 아시아 데이터센터로의 트래픽이 서비스의 용량을 넘어서 급증한다. 이 용량 문제는 해당 데이터센터에서 동시에 발생하는 셰익스피어 서비스의 주요 업데이트로 더욱 악화된다.

다행히도 실패 가능성을 완화하는 데 도움이 되는 많은 안전장치가 마련되어 있다. Production Readiness Review 프로세스가 팀이 이미 해결한 몇 가지 문제를 식별했다. 예를 들어, 개발자들은 서비스에 우아한 성능 저하를 구축했다. 용량이 부족해지면 서비스는 더 이상 텍스트와 함께 사진이나 이야기가 발생하는 장소를 보여주는 작은 지도를 반환하지 않는다. 그리고 목적에 따라 타임아웃되는 RPC는 재시도되지 않거나(예: 앞서 언급한 사진의 경우) 무작위화된 지수 백오프로 재시도된다.

이러한 안전장치에도 불구하고 태스크는 하나씩 실패하고 Borg에 의해 재시작되며, 이는 작동하는 태스크의 수를 더욱 감소시킨다.

결과적으로 서비스 대시보드의 일부 그래프가 놀라운 빨간색으로 변하고 SRE에 페이지가 발생한다. 이에 대응하여 SRE는 셰익스피어 작업에 사용 가능한 태스크 수를 증가시켜 아시아 데이터센터에 임시로 용량을 추가한다. 그렇게 함으로써 아시아 클러스터의 셰익스피어 서비스를 복원할 수 있다.

그 후 SRE 팀은 일련의 이벤트, 잘된 점,잘못된 점, 그리고 향후 개선 사항을 문서화하는 사후 분석(postmortem)을 작성한다.

**잘된 점:**

- 모니터링 시스템이 문제를 신속하게 감지하고 적절한 팀에 알림을 전송했다.
- 우아한 성능 저하 메커니즘이 작동하여 완전한 서비스 중단을 방지했다.
- RPC 재시도 로직의 지수 백오프가 문제를 더 악화시키지 않았다.
- SRE 팀이 신속하게 대응하여 추가 용량을 할당함으로써 서비스를 복원했다.

**잘못된 점:**

- 주요 업데이트와 예상치 못한 트래픽 급증이 동시에 발생하여 문제가 복합적으로 악화되었다.
- 트래픽 급증에 대한 사전 경고나 예측 시스템이 없었다.
- 자동 스케일링이 충분히 빠르게 작동하지 않았다.
- 태스크 재시작 과정에서 일시적으로 가용 용량이 더욱 감소했다.

**학습된 교훈 및 개선 사항:**

1. **용량 계획**: 지역별 트래픽 패턴을 더 면밀히 모니터링하고, 미디어 노출이 예상되는 경우 사전에 용량을 증설한다.
2. **배포 조율**: 트래픽이 높은 시간대나 특별 이벤트 기간 동안 주요 업데이트 배포를 피한다.
3. **자동 스케일링 개선**: 트래픽 급증 시 더 빠르게 반응할 수 있도록 자동 스케일링 임계값과 속도를 조정한다.
4. **부하 차단**: 지나치게 무거운 쿼리나 비정상적인 트래픽 패턴을 자동으로 감지하고 제한하는 메커니즘을 구현한다.
5. **지역 간 부하 분산**: 한 데이터센터가 과부하될 때 다른 지역으로 트래픽을 자동으로 라우팅하는 기능을 강화한다.

이 사건은 아무리 잘 준비된 시스템이라도 예상치 못한 외부 요인과 내부 변경사항이 결합될 때 취약할 수 있음을 보여준다. 다층적인 방어 메커니즘과 신속한 사고 대응이 완전한 서비스 중단을 방지하는 데 핵심적이었다.

# 23 치명적인 상태 관리하기: 신뢰성을 위한 분산에 대한 합의

## 서론

프로세스는 충돌하거나 재시작이 필요할 수 있다. 하드 드라이브는 고장날 수 있다. 자연재해는 한 지역의 여러 데이터센터를 무용지물로 만들 수 있다. 사이트 신뢰성 엔지니어는 이러한 종류의 장애를 예상하고 이에도 불구하고 시스템을 계속 실행할 전략을 개발해야 한다. 이러한 전략은 일반적으로 여러 사이트에 걸쳐 시스템을 실행하는 것을 수반한다. 지리적으로 시스템을 분산하는 것은 비교적 간단하지만, 시스템 상태의 일관된 뷰를 유지해야 할 필요성도 도입되며, 이는 더 미묘하고 어려운 작업이다.

프로세스 그룹은 다음과 같은 질문에 대해 안정적으로 합의하기를 원할 수 있다:

- 프로세스 그룹의 리더는 어느 프로세스인가?
- 그룹에 속한 프로세스의 집합은 무엇인가?
- 메시지가 분산 큐에 성공적으로 커밋되었는가?
- 프로세스가 리스(lease)를 보유하고 있는가?
- 주어진 키에 대한 데이터스토어의 값은 무엇인가?

분산 합의는 시스템 상태의 일관된 뷰를 필요로 하는 신뢰할 수 있고 높은 가용성을 가진 시스템을 구축하는 데 효과적이라는 것을 발견했다. **분산 합의 문제는 신뢰할 수 없는 통신 네트워크로 연결된 프로세스 그룹 간의 합의에 도달하는 것을 다룬다.** 예를 들어, 분산 시스템의 여러 프로세스는 중요한 구성 정보, 분산 락이 보유되었는지 여부, 큐의 메시지가 처리되었는지 여부에 대해 일관된 뷰를 형성할 수 있어야 할 수 있다. 이는 분산 컴퓨팅에서 가장 기본적인 개념 중 하나이며, 우리가 제공하는 거의 모든 서비스에 의존하는 개념이다.

**리더 선출, 중요한 공유 상태 또는 분산 락킹이 보일 때마다 공식적으로 증명되고 철저히 테스트된 분산 합의 시스템을 사용할 것을 권장한다.** 이 문제를 해결하기 위한 비공식적 접근 방식은 중단으로 이어질 수 있으며, 더 교활하게는 시스템의 중단을 불필요하게 연장할 수 있는 미묘하고 수정하기 어려운 데이터 일관성 문제로 이어질 수 있다.

## CAP 정리

CAP 정리는 분산 시스템이 다음 세 가지 속성을 동시에 모두 가질 수 없다고 주장한다:

- **일관성(Consistency)**: 각 노드에서 데이터의 일관된 뷰
- **가용성(Availability)**: 각 노드에서 데이터의 가용성
- **분할 내성(Partition tolerance)**: 네트워크 분할에 대한 내성

논리는 직관적이다: 두 노드가 통신할 수 없는 경우(네트워크가 분할되었기 때문에), 시스템은 전체적으로 일부 또는 모든 노드에서 일부 또는 모든 요청 제공을 중단하거나(가용성 감소), 평소처럼 요청을 제공할 수 있으며, 이는 각 노드에서 데이터의 일관되지 않은 뷰를 초래한다.

네트워크 분할은 불가피하기 때문에(케이블이 절단되고, 패킷이 혼잡으로 인해 손실되거나 지연되고, 하드웨어가 고장나고, 네트워킹 구성 요소가 잘못 구성되는 등), 분산 합의를 이해하는 것은 실제로 특정 애플리케이션에 대해 일관성과 가용성이 어떻게 작동하는지 이해하는 것과 같다.

### ACID vs BASE

전통적인 ACID 데이터스토어 시맨틱(원자성, 일관성, 격리성, 내구성)에 익숙하지만, 점점 더 많은 분산 데이터스토어 기술이 BASE(기본적으로 사용 가능, 소프트 상태, 최종 일관성)로 알려진 다른 시맨틱 세트를 제공한다. BASE 시맨틱을 지원하는 데이터스토어는 특정 종류의 데이터에 유용한 애플리케이션을 가지고 있으며, ACID 시맨틱을 지원하는 데이터스토어로는 훨씬 더 비용이 많이 들고 아마도 완전히 실행 불가능할 대량의 데이터와 트랜잭션을 처리할 수 있다.

BASE 시맨틱을 지원하는 대부분의 시스템은 다중 마스터 복제에 의존하며, 여기서 쓰기는 서로 다른 프로세스에 동시에 커밋될 수 있고, 충돌을 해결하는 메커니즘이 있다(종종 "최신 타임스탬프 승리"만큼 간단함). 이 접근 방식은 일반적으로 **최종 일관성(eventual consistency)**으로 알려져 있다. 그러나 최종 일관성은 놀라운 결과를 초래할 수 있으며, 특히 클록 드리프트(분산 시스템에서 불가피함) 또는 네트워크 분할의 경우 그렇다.

Jeff Shute는 "개발자가 최종 일관성에 대처하고 오래된 데이터를 처리하기 위해 극도로 복잡하고 오류가 발생하기 쉬운 메커니즘을 구축하는 데 상당한 시간을 소비한다는 것을 발견했습니다. 우리는 이것이 개발자에게 부과할 수 없는 부담이며 일관성 문제는 데이터베이스 수준에서 해결되어야 한다고 생각합니다."라고 말했다.

**시스템 설계자는 특히 중요한 상태와 관련하여 신뢰성이나 성능을 달성하기 위해 정확성을 희생할 수 없다.** 예를 들어, 금융 거래를 처리하는 시스템을 고려해보라: 금융 데이터가 정확하지 않으면 신뢰성이나 성능 요구 사항은 큰 가치를 제공하지 않는다. 시스템은 여러 프로세스에 걸쳐 중요한 상태를 안정적으로 동기화할 수 있어야 한다. 분산 합의 알고리즘은 이 기능을 제공한다.

## 합의 사용 동기: 분산 시스템 조정 실패

분산 시스템은 이해하고, 모니터링하고, 문제를 해결하기 복잡하고 미묘하다. 이러한 시스템을 실행하는 엔지니어는 종종 장애가 있을 때의 동작에 놀란다. 장애는 비교적 드문 이벤트이며, 이러한 조건에서 시스템을 테스트하는 것은 일반적인 관행이 아니다. 장애 중 시스템 동작에 대해 추론하기가 매우 어렵다. 네트워크 분할은 특히 어렵다 - 완전한 분할로 인해 발생한 것처럼 보이는 문제는 실제로 다음의 결과일 수 있다:

- 매우 느린 네트워크
- 일부(전체는 아님) 메시지가 드롭됨
- 한 방향에서는 스로틀링이 발생하지만 다른 방향에서는 발생하지 않음

### 사례 연구 1: 스플릿 브레인 문제

서비스는 여러 사용자 간의 협업을 허용하는 콘텐츠 저장소다. 신뢰성을 위해 서로 다른 랙에 있는 두 개의 복제된 파일 서버 세트를 사용한다. 서비스는 두 파일 서버에 동시에 데이터를 쓰는 것을 피해야 하는데, 그렇게 하면 데이터 손상(그리고 아마도 복구 불가능한 데이터)이 발생할 수 있기 때문이다.

각 파일 서버 쌍에는 하나의 리더와 하나의 팔로워가 있다. 서버는 하트비트를 통해 서로를 모니터링한다. 파일 서버가 파트너에 연결할 수 없으면 파트너 노드에 STONITH(Shoot The Other Node in the Head) 명령을 발행하여 노드를 종료한 다음 파일의 마스터십을 가져온다. 이 관행은 스플릿 브레인 인스턴스를 줄이는 업계 표준 방법이지만, 우리가 보게 될 것처럼 개념적으로 건전하지 않다.

네트워크가 느려지거나 패킷을 드롭하기 시작하면 어떻게 될까? 이 시나리오에서 파일 서버는 하트비트 타임아웃을 초과하고, 설계된 대로 파트너 노드에 STONITH 명령을 보내고 마스터십을 가져온다. 그러나 일부 명령은 손상된 네트워크로 인해 전달되지 않을 수 있다. 이제 파일 서버 쌍은 두 노드 모두 동일한 리소스에 대해 활성 상태여야 하거나, 둘 다 STONITH 명령을 발행하고 수신했기 때문에 둘 다 다운된 상태에 있을 수 있다. 이는 데이터의 손상 또는 사용 불가능을 초래한다.

**여기서 문제는 시스템이 단순한 타임아웃을 사용하여 리더 선출 문제를 해결하려고 한다는 것이다. 리더 선출은 분산 비동기 합의 문제의 재구성이며, 하트비트를 사용하여 올바르게 해결할 수 없다.**

### 사례 연구 2: 페일오버에 인간 개입이 필요함

고도로 샤딩된 데이터베이스 시스템은 각 샤드에 대한 프라이머리를 가지고 있으며, 이는 다른 데이터센터의 세컨더리로 동기적으로 복제한다. 외부 시스템은 프라이머리의 상태를 확인하고, 더 이상 건강하지 않으면 세컨더리를 프라이머리로 승격시킨다. 프라이머리가 세컨더리의 상태를 확인할 수 없으면 자신을 사용 불가능하게 만들고 사례 연구 1에서 본 스플릿 브레인 시나리오를 피하기 위해 인간에게 에스컬레이션한다.

이 솔루션은 데이터 손실 위험을 감수하지 않지만, 데이터 가용성에 부정적인 영향을 미친다. 또한 시스템을 실행하는 엔지니어의 운영 부하를 불필요하게 증가시키며, 인간 개입은 확장성이 좋지 않다. 프라이머리와 세컨더리가 통신에 문제가 있는 이러한 종류의 이벤트는 더 큰 인프라 문제의 경우 발생할 가능성이 높으며, 이때 대응하는 엔지니어는 이미 다른 작업으로 과부하 상태일 수 있다.

### 사례 연구 3: 결함 있는 그룹 멤버십 알고리즘

시스템에는 인덱싱 및 검색 서비스를 수행하는 구성 요소가 있다. 시작할 때 노드는 가십 프로토콜을 사용하여 서로를 발견하고 클러스터에 참여한다. 클러스터는 조정을 수행하는 리더를 선출한다. 클러스터를 분할하는 네트워크 분할의 경우, 각 측은 (잘못) 마스터를 선출하고 쓰기 및 삭제를 수락하여 스플릿 브레인 시나리오와 데이터 손상으로 이어진다.

**프로세스 그룹에 걸쳐 그룹 멤버십의 일관된 뷰를 결정하는 문제는 분산 합의 문제의 또 다른 인스턴스다.**

사실, 많은 분산 시스템 문제는 마스터 선출, 그룹 멤버십, 모든 종류의 분산 락킹 및 리싱, 신뢰할 수 있는 분산 큐잉 및 메시징, 프로세스 그룹에 걸쳐 일관되게 보여야 하는 모든 종류의 중요한 공유 상태의 유지를 포함하여 분산 합의의 다른 버전으로 밝혀진다. **이러한 모든 문제는 공식적으로 올바른 것으로 증명되고 광범위하게 테스트된 구현을 가진 분산 합의 알고리즘만을 사용하여 해결되어야 한다.** 이러한 종류의 문제를 해결하는 임시 수단(예: 하트비트 및 가십 프로토콜)은 실제로 항상 신뢰성 문제를 가질 것이다.

## 분산 합의가 작동하는 방법

합의 문제에는 여러 변형이 있다. 분산 소프트웨어 시스템을 다룰 때 우리는 **비동기 분산 합의**에 관심이 있으며, 이는 메시지 전달에 잠재적으로 무제한 지연이 있는 환경에 적용된다. (동기 합의는 실시간 시스템에 적용되며, 여기서 전용 하드웨어는 메시지가 항상 특정 타이밍 보장과 함께 전달됨을 의미한다.)

분산 합의 알고리즘은 **크래시-페일(crash-fail)**일 수 있으며(충돌한 노드가 시스템으로 돌아오지 않는다고 가정) 또는 **크래시-복구(crash-recover)**일 수 있다. 크래시-복구 알고리즘은 실제 시스템의 대부분의 문제가 느린 네트워크, 재시작 등으로 인해 일시적이기 때문에 훨씬 더 유용하다.

알고리즘은 **비잔틴(Byzantine) 또는 비-비잔틴 장애**를 처리할 수 있다. 비잔틴 장애는 프로세스가 버그나 악의적인 활동으로 인해 잘못된 메시지를 전달할 때 발생하며, 처리하는 데 비교적 비용이 많이 들고 덜 자주 발생한다.

### FLP 불가능성 결과

기술적으로 제한된 시간 내에 비동기 분산 합의 문제를 해결하는 것은 불가능하다. Dijkstra Prize를 수상한 **FLP 불가능성 결과**에 의해 증명된 바와 같이, 신뢰할 수 없는 네트워크가 있는 상태에서 어떤 비동기 분산 합의 알고리즘도 진행을 보장할 수 없다.

실제로 우리는 시스템이 대부분의 시간 동안 안정적으로 진행할 수 있도록 충분한 건강한 복제본과 네트워크 연결을 갖도록 보장함으로써 제한된 시간 내에 분산 합의 문제에 접근한다. 또한 시스템은 무작위 지연이 있는 백오프를 가져야 한다. 이 설정은 재시도가 캐스케이드 효과를 일으키는 것을 방지하고 이 장의 뒷부분에서 설명하는 결투하는 제안자 문제를 피한다. 프로토콜은 안전성을 보장하고, 시스템의 적절한 중복성은 활성을 장려한다.

### Paxos 및 기타 프로토콜

분산 합의 문제에 대한 원래 솔루션은 Lamport의 **Paxos 프로토콜**이었지만, **Raft, Zab, Mencius**를 포함하여 문제를 해결하는 다른 프로토콜이 존재한다. Paxos 자체는 성능을 높이기 위한 많은 변형을 가지고 있다. 이것들은 일반적으로 프로토콜을 간소화하기 위해 하나의 프로세스에 특별한 리더 역할을 부여하는 것과 같은 단일 세부 사항에서만 다르다.

## Paxos 개요: 예제 프로토콜

Paxos는 시스템의 대다수의 프로세스에 의해 수락되거나 수락되지 않을 수 있는 제안의 시퀀스로 작동한다. 제안이 수락되지 않으면 실패한다. 각 제안에는 시스템의 모든 작업에 엄격한 순서를 부과하는 시퀀스 번호가 있다.

프로토콜의 첫 번째 단계에서 제안자는 수락자에게 시퀀스 번호를 보낸다. 각 수락자는 아직 더 높은 시퀀스 번호를 가진 제안을 보지 않은 경우에만 제안을 수락하는 데 동의한다. 필요한 경우 제안자는 더 높은 시퀀스 번호로 다시 시도할 수 있다. 제안자는 고유한 시퀀스 번호를 사용해야 한다(분리된 세트에서 가져오거나 호스트 이름을 시퀀스 번호에 통합하는 등).

제안자가 수락자의 대다수로부터 동의를 받으면 값이 있는 커밋 메시지를 보내 제안을 커밋할 수 있다.

제안의 엄격한 순서는 시스템의 메시지 순서와 관련된 모든 문제를 해결한다. 커밋을 위한 대다수의 요구 사항은 두 개의 다른 값이 동일한 제안에 대해 커밋될 수 없음을 의미한다. 왜냐하면 두 대다수는 적어도 하나의 노드에서 중복되기 때문이다. 수락자는 제안을 수락하는 데 동의할 때마다 영구 저장소에 저널을 써야 하는데, 수락자가 재시작한 후에도 이러한 보장을 존중해야 하기 때문이다.

**Paxos 자체는 그다지 유용하지 않다: 그것이 할 수 있는 전부는 값과 제안 번호에 한 번 동의하는 것이다.** 대다수의 노드만 값에 동의하면 되기 때문에 주어진 노드는 동의된 값 세트의 완전한 뷰를 가지지 못할 수 있다. 이 제한은 대부분의 분산 합의 알고리즘에 해당한다.

## 분산 합의를 위한 시스템 아키텍처 패턴

분산 합의 알고리즘은 낮은 수준이고 원시적이다: 단순히 노드 세트가 한 번 값에 동의할 수 있게 한다. 실제 설계 작업에 잘 매핑되지 않는다. 분산 합의를 유용하게 만드는 것은 데이터스토어, 구성 저장소, 큐, 락킹, 리더 선출 서비스와 같은 상위 수준 시스템 구성 요소의 추가로, 분산 합의 알고리즘이 다루지 않는 실용적인 시스템 기능을 제공한다. 상위 수준 구성 요소를 사용하면 시스템 설계자의 복잡성이 줄어든다. 또한 필요한 경우 시스템이 실행되는 환경의 변화나 비기능적 요구 사항의 변화에 대응하여 기본 분산 합의 알고리즘을 변경할 수 있다.

합의 알고리즘을 성공적으로 사용하는 많은 시스템은 실제로 **Zookeeper, Consul, etcd**와 같은 알고리즘을 구현하는 일부 서비스의 클라이언트로 그렇게 한다. Zookeeper는 분산 합의를 사용하도록 설계되지 않은 애플리케이션에서도 사용하기 쉬웠기 때문에 업계에서 주목을 받은 최초의 오픈 소스 합의 시스템이었다. Google에서는 **Chubby 서비스**가 유사한 틈새를 채운다. 저자들은 애플리케이션 유지 관리자가 고가용성 합의 서비스와 호환되는 방식으로 시스템을 배포하는(올바른 수의 복제본 실행, 그룹 멤버십 처리, 성능 처리 등) 부담에서 벗어나기 때문에 합의 프리미티브를 엔지니어가 애플리케이션에 구축하는 라이브러리가 아닌 서비스로 제공한다고 지적한다.

### 신뢰할 수 있는 복제된 상태 머신

**복제된 상태 머신(RSM)**은 동일한 작업 세트를 동일한 순서로 여러 프로세스에서 실행하는 시스템이다. RSM은 데이터나 구성 저장소, 락킹, 리더 선출과 같은 유용한 분산 시스템 구성 요소 및 서비스의 기본 빌딩 블록이다.

RSM의 작업은 합의 알고리즘을 통해 전역적으로 정렬된다. 이것은 강력한 개념이다: 여러 논문은 모든 결정론적 프로그램이 RSM으로 구현됨으로써 고가용성 복제 서비스로 구현될 수 있음을 보여준다.

복제된 상태 머신은 합의 알고리즘 위의 논리적 레이어에서 구현된 시스템이다. 합의 알고리즘은 작업의 시퀀스에 대한 합의를 처리하고, RSM은 해당 순서로 작업을 실행한다. 합의 그룹의 모든 구성원이 반드시 각 합의 쿼럼의 구성원은 아니기 때문에 RSM은 피어로부터 상태를 동기화해야 할 수 있다.

### 신뢰할 수 있는 복제된 데이터스토어 및 구성 저장소

신뢰할 수 있는 복제된 데이터스토어는 복제된 상태 머신의 애플리케이션이다. 복제된 데이터스토어는 작업의 중요한 경로에서 합의 알고리즘을 사용한다. 따라서 이 유형의 설계에서 성능, 처리량 및 확장 능력은 매우 중요하다. 다른 기본 기술로 구축된 데이터스토어와 마찬가지로 합의 기반 데이터스토어는 읽기 작업에 대해 다양한 일관성 시맨틱을 제공할 수 있으며, 이는 데이터스토어의 확장 방식에 큰 차이를 만든다.

다른(비분산 합의 기다른(비분산 합의 기반) 시스템은 종종 단순히 타임스탬프에 의존하여 반환되는 데이터의 연령에 대한 경계를 제공한다. 타임스탬프는 분산 시스템에서 매우 문제가 있는데, 여러 머신에 걸쳐 클록이 동기화되어 있다고 보장하는 것이 불가능하기 때문이다. **Spanner**는 관련된 최악의 불확실성을 모델링하고 해당 불확실성을 해결하기 위해 필요한 경우 처리를 늦춤으로써 이 문제를 해결한다.

### 리더 선출을 사용한 고가용성 처리

분산 시스템에서 리더 선출은 분산 합의와 동등한 문제다. 시스템에서 특정 유형의 작업을 수행하기 위해 단일 리더를 사용하는 복제된 서비스는 매우 일반적이다. 단일 리더 메커니즘은 거친 수준에서 상호 배제를 보장하는 방법이다.

이 유형의 설계는 서비스 리더의 작업이 하나의 프로세스에 의해 수행되거나 샤딩될 수 있는 경우에 적합하다. 시스템 설계자는 단순한 프로그램인 것처럼 작성하고, 해당 프로세스를 복제하고, 리더 선출을 사용하여 어느 시점에서나 하나의 리더만 작동하도록 보장함으로써 고가용성 서비스를 구성할 수 있다. 종종 리더의 작업은 시스템의 일부 작업자 풀을 조정하는 것이다. 이 패턴은 **GFS**(Colossus로 대체됨)와 **Bigtable** 키-값 저장소에서 사용되었다.

이 유형의 구성 요소에서는 복제된 데이터스토어와 달리 합의 알고리즘이 시스템이 수행하는 주요 작업의 중요한 경로에 있지 않으므로 처리량은 일반적으로 주요 관심사가 아니다.

### 분산 조정 및 락킹 서비스

분산 계산의 **배리어(barrier)**는 일부 조건이 충족될 때까지(예: 계산의 한 단계의 모든 부분이 완료될 때까지) 프로세스 그룹이 진행하지 못하도록 차단하는 프리미티브다. 배리어의 사용은 분산 계산을 논리적 단계로 효과적으로 분할한다. 예를 들어, 배리어는 **MapReduce** 모델을 구현하는 데 사용될 수 있으며, 계산의 Reduce 부분이 진행되기 전에 전체 Map 단계가 완료되도록 보장한다.

배리어는 단일 조정자 프로세스에 의해 구현될 수 있지만, 이 구현은 일반적으로 허용되지 않는 단일 장애 지점을 추가한다. 배리어는 RSM으로도 구현될 수 있다. Zookeeper 합의 서비스는 배리어 패턴을 구현할 수 있다.

**락**은 RSM으로 구현될 수 있는 또 다른 유용한 조정 프리미티브다. 작업자 프로세스가 일부 입력 파일을 원자적으로 소비하고 결과를 쓰는 분산 시스템을 고려해보라. 분산 락은 여러 작업자가 동일한 입력 파일을 처리하는 것을 방지하는 데 사용될 수 있다. 실제로 무기한 락 대신 타임아웃이 있는 **갱신 가능한 리스(renewable leases)**를 사용하는 것이 필수적인데, 그렇게 하면 충돌하는 프로세스에 의해 락이 무기한 보유되는 것을 방지하기 때문이다.

분산 락킹은 이 장의 범위를 넘어서지만, **분산 락은 주의해서 사용해야 하는 낮은 수준의 시스템 프리미티브**임을 명심하라. 대부분의 애플리케이션은 분산 트랜잭션을 제공하는 더 높은 수준의 시스템을 사용해야 한다.

### 신뢰할 수 있는 분산 큐잉 및 메시징

큐는 종종 여러 작업자 프로세스 간에 작업을 분배하는 방법으로 사용되는 일반적인 데이터 구조다.

큐 기반 시스템은 비교적 쉽게 장애와 작업자 노드의 손실을 견딜 수 있다. 그러나 시스템은 청구된 작업이 성공적으로 처리되도록 보장해야 한다. 그 목적을 위해 큐에서 완전히 제거하는 대신 리스 시스템(락과 관련하여 앞서 논의됨)이 권장된다. 큐 기반 시스템의 단점은 큐의 손실이 전체 시스템의 작동을 방해한다는 것이다. 큐를 RSM으로 구현하면 위험을 최소화하고 전체 시스템을 훨씬 더 견고하게 만들 수 있다.

**원자 브로드캐스트(Atomic broadcast)**는 메시지가 모든 참가자에 의해 안정적으로 동일한 순서로 수신되는 분산 시스템 프리미티브다. 이것은 믿을 수 없을 정도로 강력한 분산 시스템 개념이며 실용적인 시스템을 설계하는 데 매우 유용하다. 시스템 설계자가 사용할 수 있는 수많은 게시-구독 메시징 인프라가 존재하지만, 모두 원자 보장을 제공하는 것은 아니다. Chandra와 Toueg는 원자 브로드캐스트와 합의의 동등성을 입증한다.

큐를 로드 밸런싱 장치로 사용하는 작업 분배 패턴으로서의 큐잉은 점대점 메시징으로 간주될 수 있다. 메시징 시스템은 일반적으로 채널이나 토픽을 구독하는 많은 클라이언트가 메시지를 소비할 수 있는 게시-구독 큐도 구현한다. 이 일대다 경우에서 큐의 메시지는 영구적인 순서가 있는 목록으로 저장된다.

큐잉 및 메시징 시스템은 종종 탁월한 처리량이 필요하지만 (직접 사용자를 대면하는 경우가 거의 없기 때문에) 극도로 낮은 지연 시간은 필요하지 않다. 그러나 여러 작업자가 큐에서 작업을 청구하는 방금 설명한 것과 같은 시스템에서 매우 높은 지연 시간은 각 작업의 처리 시간 비율이 크게 증가하면 문제가 될 수 있다.

## 분산 합의 성능

기존의 통념은 일반적으로 합의 알고리즘이 높은 처리량과 낮은 지연 시간을 필요로 하는 많은 시스템에 사용하기에는 너무 느리고 비용이 많이 든다고 주장해왔다. **이 개념은 단순히 사실이 아니다** - 구현이 느릴 수 있지만 성능을 향상시킬 수 있는 여러 트릭이 있다. 분산 합의 알고리즘은 Google의 많은 중요한 시스템의 핵심에 있으며, 실제로 매우 효과적인 것으로 입증되었다.

Google의 규모는 여기서 이점이 아니다: 사실 우리의 규모는 두 가지 주요 과제를 도입하기 때문에 더 불리하다: 우리의 데이터 세트는 크고 우리의 시스템은 넓은 지리적 거리에 걸쳐 실행된다. 더 큰 데이터 세트에 여러 복제본을 곱하면 상당한 컴퓨팅 비용이 들고, 더 큰 지리적 거리는 복제본 간의 지연 시간을 증가시켜 성능을 감소시킨다.

### 워크로드 변동성

워크로드는 여러 방식으로 달라질 수 있으며 이것들이 어떻게 달라질 수 있는지 이해하는 것은 성능을 논의하는 데 중요하다. 합의 시스템의 경우 워크로드는 다음과 같은 측면에서 달라질 수 있다:

- **처리량**: 최고 부하 시 단위 시간당 이루어지는 제안의 수
- **요청 유형**: 상태를 변경하는 작업의 비율
- **읽기 작업에 필요한 일관성 시맨틱**
- 데이터 페이로드의 크기가 달라질 수 있는 경우 요청 크기

### 배포 전략

배포 전략도 달라진다. 예를 들어:

- 배포가 로컬 영역인가 광역인가?
- 어떤 종류의 쿼럼이 사용되며 대다수의 프로세스는 어디에 있는가?
- 시스템이 샤딩, 파이프라이닝, 배칭을 사용하는가?

많은 합의 시스템은 구별된 리더 프로세스를 사용하고 모든 요청이 이 특별한 노드로 가도록 요구한다. 결과적으로 클라이언트가 서로 다른 지리적 위치에 있는 시스템의 성능은 단순히 더 먼 노드가 리더 프로세스에 대한 왕복 시간이 더 길기 때문에 상당히 달라질 수 있다.

### Multi-Paxos: 상세한 메시지 흐름

Multi-Paxos 프로토콜은 강력한 리더 프로세스를 사용한다: 리더가 아직 선출되지 않았거나 일부 장애가 발생하지 않는 한, 합의에 도달하기 위해 제안자에서 수락자의 쿼럼까지 한 번의 왕복만 필요하다. 강력한 리더 프로세스를 사용하는 것은 전달될 메시지의 수 측면에서 최적이며, 많은 합의 프로토콜의 전형적인 특징이다.

새로운 제안자가 프로토콜의 첫 번째 Prepare/Promise 단계를 실행하는 초기 상태를 보여준다. 이 단계를 실행하면 새로운 번호가 매겨진 뷰 또는 리더 임기가 설정된다. 뷰가 동일하게 유지되는 동안 프로토콜의 후속 실행에서는 첫 번째 단계가 불필요한데, 뷰를 설정한 제안자가 단순히 Accept 메시지를 보낼 수 있고, 쿼럼의 응답이 수신되면(제안자 자체 포함) 합의에 도달하기 때문이다.

그룹의 다른 프로세스는 언제든지 제안자 역할을 맡아 메시지를 제안할 수 있지만, 제안자를 변경하는 데는 성능 비용이 든다. 이는 프로토콜의 단계 1을 실행하기 위해 추가 왕복이 필요하지만, 더 중요하게는 **결투하는 제안자(dueling proposers)** 상황을 일으킬 수 있으며, 이 상황에서는 제안이 반복적으로 서로를 방해하고 어떤 제안도 수락될 수 없다. 이 시나리오는 라이브락의 한 형태이기 때문에 무기한 계속될 수 있다.

모든 실용적인 합의 시스템은 이 충돌 문제를 해결하며, 일반적으로 시스템의 모든 제안을 하는 제안자 프로세스를 선출하거나, 각 프로세스에게 제안을 위한 특정 슬롯을 할당하는 회전하는 제안자를 사용한다.

리더 프로세스를 사용하는 시스템의 경우, **리더 선출 프로세스는 리더가 없을 때 발생하는 시스템 사용 불가능성과 결투하는 제안자의 위험 사이의 균형을 맞추기 위해 신중하게 조정되어야 한다.** 올바른 타임아웃과 백오프 전략을 구현하는 것이 중요하다. 여러 프로세스가 리더가 없다는 것을 감지하고 모두 동시에 리더가 되려고 시도하면, 어떤 프로세스도 성공할 가능성이 없다(다시, 결투하는 제안자). **무작위성을 도입하는 것이 최선의 접근 방식이다.** 예를 들어 Raft는 리더 선출 프로세스에 접근하는 잘 생각된 방법을 가지고 있다.

### 읽기 중심 워크로드 확장

많은 워크로드가 읽기 중심이기 때문에 읽기 워크로드를 확장하는 것은 종종 중요하다. 복제된 데이터스토어는 데이터가 여러 장소에서 사용 가능하다는 이점이 있으며, 이는 모든 읽기에 대해 강한 일관성이 필요하지 않은 경우 데이터가 모든 복제본에서 읽힐 수 있음을 의미한다. 복제본에서 읽는 이 기술은 Google의 **Photon 시스템**과 같은 특정 애플리케이션에 잘 작동하며, 이 시스템은 여러 파이프라인의 작업을 조정하기 위해 분산 합의를 사용한다. Photon은 상태 수정을 위해 원자 비교 및 설정 작업을 사용하며(원자 레지스터에서 영감을 받음), 이는 절대적으로 일관성이 있어야 한다. 그러나 읽기 작업은 모든 복제본에서 제공될 수 있는데, 오래된 데이터는 추가 작업이 수행되지만 잘못된 결과는 아니기 때문이다. 이 트레이드오프는 가치가 있다.

읽기 중인 데이터가 최신이고 읽기가 수행되기 전에 수행된 모든 변경 사항과 일관성이 있음을 보장하려면 다음 중 하나를 수행해야 한다:

- **읽기 전용 합의 작업 수행**
- 가장 최신임이 보장되는 복제본에서 데이터를 읽는다. 안정적인 리더 프로세스를 사용하는 시스템(많은 분산 합의 구현이 그러하듯이)에서 리더는 이 보장을 제공할 수 있다.
- **쿼럼 리스(quorum leases)** 사용: 일부 복제본이 시스템의 데이터 전체 또는 일부에 대한 리스를 부여받아 일부 쓰기 성능을 희생하고 강력하게 일관된 로컬 읽기를 허용한다.

### 쿼럼 리스

쿼럼 리스는 읽기 작업의 지연 시간을 줄이고 처리량을 높이는 것을 목표로 하는 최근 개발된 분산 합의 성능 최적화다. 앞서 언급했듯이, 클래식 Paxos 및 대부분의 다른 분산 합의 프로토콜의 경우, 강력하게 일관된 읽기(즉, 가장 최신 상태 뷰를 가지고 있음이 보장되는 읽기)를 수행하려면 복제본의 쿼럼에서 읽는 분산 합의 작업이나 최근의 모든 상태 변경 작업을 본 것이 보장되는 안정적인 리더 복제본이 필요하다. 많은 시스템에서 읽기 작업이 쓰기보다 훨씬 많기 때문에 분산 작업이나 단일 복제본에 대한 이러한 의존은 지연 시간과 시스템 처리량을 해친다.

쿼럼 리싱 기술은 단순히 복제된 데이터스토어의 상태의 일부 하위 집합에 대한 읽기 리스를 복제본의 쿼럼에 부여한다. 리스는 특정(일반적으로 짧은) 기간 동안 유효하다. 해당 데이터의 상태를 변경하는 모든 작업은 읽기 쿼럼의 모든 복제본에 의해 승인되어야 한다. 이러한 복제본 중 하나라도 사용할 수 없게 되면 리스가 만료될 때까지 데이터를 수정할 수 없다.

쿼럼 리스는 특정 데이터 하위 집합에 대한 읽기가 단일 지리적 지역에 집중되어 있는 읽기 중심 워크로드에 특히 유용하다.

### 분산 합의 성능 및 네트워크 지연 시간

합의 시스템은 상태 변경을 커밋할 때 성능에 대한 두 가지 주요 물리적 제약에 직면한다. 하나는 네트워크 왕복 시간이고 다른 하나는 영구 저장소에 데이터를 쓰는 데 걸리는 시간이며, 이는 나중에 검토될 것이다.

네트워크 왕복 시간은 소스와 목적지 위치에 따라 크게 달라지며, 이는 소스와 목적지 간의 물리적 거리와 네트워크의 혼잡 정도에 모두 영향을 받는다. 단일 데이터센터 내에서 머신 간의 왕복 시간은 밀리초 단위여야 한다. 미국 내의 일반적인 왕복 시간(RTT)은 45밀리초이고, 뉴욕에서 런던까지는 70밀리초다.

로컬 영역 네트워크를 통한 합의 시스템 성능은 많은 전통적인 데이터베이스가 복제에 사용하는 것과 같은 비동기 리더-팔로워 복제 시스템의 성능과 비교할 수 있다. 그러나 분산 합의 시스템의 가용성 이점의 대부분은 서로 다른 장애 도메인에 있기 위해 복제본이 서로 "멀리" 떨어져 있어야 한다.

많은 합의 시스템은 통신 프로토콜로 TCP/IP를 사용한다. TCP/IP는 연결 지향적이며 메시지의 FIFO 순서에 관한 일부 강력한 신뢰성 보장을 제공한다. 그러나 새로운 TCP/IP 연결을 설정하려면 데이터를 보내거나 받기 전에 연결을 설정하는 3방향 핸드셰이크를 수행하기 위한 네트워크 왕복이 필요하다. **TCP/IP 느린 시작(slow start)**은 제한이 설정될 때까지 초기에 연결의 대역폭을 제한한다.

### 디스크 액세스

영구 저장소에 로깅하는 것은 충돌하고 클러스터로 돌아온 노드가 진행 중인 합의 트랜잭션에 관해 이전에 한 약속을 존중할 수 있도록 필요하다. 예를 들어 Paxos 프로토콜에서 수락자는 이미 더 높은 시퀀스 번호를 가진 제안에 동의한 경우 제안에 동의할 수 없다. 합의되고 커밋된 제안의 세부 사항이 영구 저장소에 로깅되지 않으면, 수락자가 충돌하고 재시작되는 경우 프로토콜을 위반하여 일관되지 않은 상태로 이어질 수 있다.

디스크의 로그에 항목을 쓰는 데 필요한 시간은 사용되는 하드웨어나 가상화된 환경에 따라 크게 달라지지만, 1밀리초에서 몇 밀리초 사이일 것이다.

Multi-Paxos의 메시지 흐름에서 논의되었지만 이 섹션은 프로토콜이 상태 변경을 디스크에 로깅해야 하는 위치를 보여주지 않았다. **프로세스가 존중해야 하는 약속을 할 때마다 디스크 쓰기가 발생해야 한다.** Multi-Paxos의 성능이 중요한 두 번째 단계에서 이러한 시점은 수락자가 제안에 대한 응답으로 Accepted 메시지를 보내기 전과 제안자가 Accept 메시지를 보내기 전에 발생하는데, 이 Accept 메시지도 암시적 Accepted 메시지이기 때문이다.

이것은 단일 합의 작업의 지연 시간이 다음을 포함함을 의미한다:

- 제안자에서 하나의 디스크 쓰기
- 수락자에게 병렬 메시지
- 수락자에서 병렬 디스크 쓰기
- 반환 메시지

디스크에 작은 무작위 쓰기를 수행하는 지연 시간이 10밀리초 정도인 경우, 합의 작업의 비율은 초당 약 100개로 제한된다. 이 시간은 네트워크 왕복 시간이 무시할 수 있고 제안자가 수락자와 병렬로 로깅을 수행한다고 가정한다.

이미 살펴본 바와 같이 분산 합의 알고리즘은 종종 복제된 상태 머신을 구축하는 기초로 사용된다. RSM도 복구 목적으로 트랜잭션 로그를 유지해야 한다(모든 데이터스토어와 같은 이유로). **합의 알고리즘의 로그와 RSM의 트랜잭션 로그를 단일 로그로 결합할 수 있다.** 이러한 로그를 결합하면 디스크의 두 개의 다른 물리적 위치에 쓰는 것을 지속적으로 번갈아가며 할 필요가 없어 탐색 작업에 소요되는 시간이 줄어든다. 디스크는 초당 더 많은 작업을 유지할 수 있고, 따라서 시스템 전체가 더 많은 트랜잭션을 수행할 수 있다.

또 다른 가능한 최적화는 **배칭(batching)**으로, 제안자에서 여러 클라이언트 작업을 하나의 작업으로 배칭하는 것이다. 이것은 디스크 로깅 및 네트워크 지연 시간의 고정 비용을 더 많은 수의 작업에 분산시켜 처리량을 증가시킨다.

## 분산 합의 기반 시스템 배포

합의 기반 시스템을 배포할 때 시스템 설계자가 해야 하는 가장 중요한 결정은 배포할 복제본의 수와 해당 복제본의 위치에 관한 것이다.

### 복제본 수

일반적으로 합의 기반 시스템은 다수 쿼럼을 사용하여 작동한다. 즉, **2f + 1개의 복제본 그룹은 f개의 장애를 견딜 수 있다**(비잔틴 결함 허용이 필요한 경우, 시스템이 잘못된 결과를 반환하는 복제본에 저항하는 경우, 3f + 1개의 복제본이 f개의 장애를 견딜 수 있다). 비-비잔틴 장애의 경우 배포할 수 있는 최소 복제본 수는 3개다 - 2개가 배포되면 프로세스의 장애에 대한 허용이 없다. 3개의 복제본은 하나의 장애를 견딜 수 있다.

대부분의 시스템 다운타임은 계획된 유지보수의 결과다: **3개의 복제본은 하나의 복제본이 유지보수를 위해 다운된 경우 시스템이 정상적으로 작동할 수 있게 한다**(나머지 두 복제본이 허용 가능한 성능으로 시스템 부하를 처리할 수 있다고 가정).

유지보수 기간 동안 계유지보수 기간 동안 계획되지 않은 장애가 발생하면 합의 시스템을 사용할 수 없게 된다. 합의 시스템의 사용 불가능은 일반적으로 허용되지 않으므로, **5개의 복제본을 실행하여 최대 2개의 장애를 허용해야 한다.** 5개 중 4개의 복제본이 합의 시스템에 남아 있으면 반드시 개입이 필요한 것은 아니지만, 3개가 남으면 추가 복제본 하나 또는 두 개를 추가해야 한다.

합의 시스템이 쿼럼을 형성할 수 없을 정도로 많은 복제본을 잃으면, 이론적으로 해당 시스템은 복구 불가능한 상태다. 왜냐하면 누락된 복제본 중 적어도 하나의 내구성 있는 로그에 액세스할 수 없기 때문이다. 쿼럼이 남아 있지 않으면 누락된 복제본에서만 볼 수 있는 결정이 내려졌을 가능성이 있다. 관리자는 그룹 멤버십을 강제로 변경하고 기존 복제본에서 따라잡는 새로운 복제본을 추가하여 진행할 수 있지만, 데이터 손실 가능성은 항상 남아 있다 - 가능한 한 피해야 할 상황이다.

재해 시 관리자는 이러한 강제 재구성을 수행할지 또는 시스템 상태를 가진 머신이 사용 가능해질 때까지 일정 기간 동안 기다릴지 결정해야 한다. 이러한 결정이 내려질 때 시스템의 로그 처리(모니터링 외에)가 중요해진다. 이론적인 논문은 종종 합의가 복제된 로그를 구성하는 데 사용될 수 있다고 지적하지만, 장애가 발생하여 복구될 수 있는(따라서 일부 합의 결정 시퀀스를 놓칠 수 있는) 복제본이나 느림으로 인해 지연되는 복제본을 처리하는 방법에 대해서는 논의하지 않는다. **시스템의 견고성을 유지하기 위해 이러한 복제본이 따라잡는 것이 중요하다.**

복제된 로그는 분산 합의 이론에서 항상 일급 시민은 아니지만, 프로덕션 시스템의 매우 중요한 측면이다. **Raft**는 복제본의 로그에 있는 모든 간격이 채워지는 방법을 명시적으로 정의하여 복제된 로그의 일관성을 관리하는 방법을 설명한다. 5개 인스턴스의 Raft 시스템이 리더를 제외한 모든 멤버를 잃으면, 리더는 여전히 모든 커밋된 결정에 대한 완전한 지식을 가지고 있음이 보장된다. 반면에 누락된 다수의 멤버에 리더가 포함되어 있으면 나머지 복제본이 얼마나 최신 상태인지에 대해 강력한 보장을 할 수 없다.

시스템의 성능과 쿼럼의 일부를 형성할 필요가 없는 시스템의 복제본 수 사이에는 관계가 있다: **소수의 느린 복제본은 뒤처질 수 있어 더 나은 성능을 가진 복제본의 쿼럼이 더 빠르게 실행될 수 있다**(리더가 잘 수행하는 한). 복제본 성능이 크게 다르면 모든 장애가 시스템의 전체 성능을 감소시킬 수 있는데, 느린 이상값이 쿼럼을 형성하는 데 필요하기 때문이다. 시스템이 더 많은 장애나 지연되는 복제본을 견딜 수 있을수록 시스템의 전체 성능이 더 좋을 가능성이 높다.

**비용 문제도 복제본 관리에서 고려되어야 한다**: 각 복제본은 비용이 많이 드는 컴퓨팅 리소스를 사용한다. 문제의 시스템이 단일 프로세스 클러스터인 경우 복제본 실행 비용은 아마도 큰 고려 사항이 아니다. 그러나 복제본의 비용은 Photon과 같은 시스템의 경우 심각한 고려 사항이 될 수 있는데, 이 시스템은 각 샤드가 합의 알고리즘을 실행하는 전체 프로세스 그룹인 샤딩된 구성을 사용한다. 샤드 수가 증가함에 따라 각 추가 복제본의 비용도 증가하는데, 샤드 수와 동일한 수의 프로세스를 시스템에 추가해야 하기 때문이다.

따라서 시스템의 복제본 수에 대한 결정은 다음 요소 간의 트레이드오프다:

- 신뢰성의 필요
- 시스템에 영향을 미치는 계획된 유지보수의 빈도
- 위험
- 성능
- 비용

이 계산은 각 시스템마다 다를 것이다: 시스템마다 가용성에 대한 서비스 수준 목표가 다르고, 일부 조직은 다른 조직보다 더 정기적으로 유지보수를 수행하며, 조직은 다양한 비용, 품질 및 신뢰성의 하드웨어를 사용한다.

### 복제본 위치

합의 클러스터를 구성하는 프로세스를 배포할 위치에 대한 결정은 두 가지 요소를 기반으로 한다: 시스템이 처리해야 하는 장애 도메인과 시스템의 지연 시간 요구 사항 간의 트레이드오프다. 복제본을 배치할 위치를 결정하는 데는 여러 복잡한 문제가 작용한다.

**장애 도메인(failure domain)**은 단일 장애의 결과로 사용할 수 없게 될 수 있는 시스템 구성 요소의 집합이다. 장애 도메인의 예는 다음과 같다:

- 물리적 머신
- 단일 전원 공급 장치가 제공하는 데이터센터의 랙
- 하나의 네트워킹 장비가 제공하는 데이터센터의 여러 랙
- 광섬유 케이블 절단으로 인해 사용할 수 없게 될 수 있는 데이터센터
- 허리케인과 같은 단일 자연재해의 영향을 받을 수 있는 단일 지리적 지역의 데이터센터 세트

일반적으로 **복제본 간의 거리가 증가하면 복제본 간의 왕복 시간도 증가하고, 시스템이 견딜 수 있는 장애의 크기도 증가한다.** 대부분의 합의 시스템에서 복제본 간의 왕복 시간을 증가시키면 작업의 지연 시간도 증가한다.

지연 시간이 중요한 정도와 특정 도메인의 장애를 견딜 수 있는 능력은 매우 시스템에 따라 다르다. 일부 합의 시스템 아키텍처는 특별히 높은 처리량이나 낮은 지연 시간을 필요로 하지 않는다: 예를 들어, 고가용성 서비스의 그룹 멤버십 및 리더 선출 서비스를 제공하기 위해 존재하는 합의 시스템은 아마도 심하게 로드되지 않으며, 합의 트랜잭션 시간이 리더 리스 시간의 일부에 불과하면 성능은 중요하지 않다. 배치 지향 시스템도 지연 시간의 영향을 덜 받는다: 작업 배치 크기를 늘려 처리량을 높일 수 있다.

**시스템이 견딜 수 있는 손실의 장애 도메인 크기를 지속적으로 증가시키는 것이 항상 의미가 있는 것은 아니다.** 예를 들어, 합의 시스템을 사용하는 모든 클라이언트가 특정 장애 도메인(예: 뉴욕 지역) 내에서 실행되고 있고, 더 넓은 지리적 영역에 분산 합의 기반 시스템을 배포하면 해당 장애 도메인의 중단(예: 허리케인 샌디) 중에도 계속 서비스를 제공할 수 있다면, 그것이 가치가 있을까? 아마도 아닐 것이다. 왜냐하면 시스템의 클라이언트도 다운되어 시스템에 트래픽이 없을 것이기 때문이다. 지연 시간, 처리량 및 컴퓨팅 리소스 측면에서 추가 비용은 아무런 이점도 주지 않을 것이다.

**재해 복구는 복제본을 배치할 위치를 결정할 때 고려해야 한다**: 중요한 데이터를 저장하는 시스템에서 합의 복제본은 본질적으로 시스템 데이터의 온라인 복사본이기도 하다. 그러나 중요한 데이터가 위험에 처해 있을 때는 여러 다양한 장애 도메인에 배포된 견고한 합의 기반 시스템의 경우에도 정기적인 스냅샷을 다른 곳에 백업하는 것이 중요하다. 절대 벗어날 수 없는 두 가지 장애 도메인이 있다: **소프트웨어 자체와 시스템 관리자의 인적 오류**다. 소프트웨어의 버그는 비정상적인 상황에서 나타나 데이터 손실을 일으킬 수 있으며, 시스템 구성 오류도 유사한 효과를 가질 수 있다. 인간 운영자도 실수를 하거나 데이터 손실을 초래하는 사보타주를 수행할 수 있다.

복제본의 위치에 대한 결정을 내릴 때 **가장 중요한 성능 측정은 클라이언트 인식**임을 기억하라: 이상적으로 클라이언트에서 합의 시스템의 복제본까지의 네트워크 왕복 시간을 최소화해야 한다. 광역 네트워크를 통해 Mencius나 Egalitarian Paxos와 같은 리더 없는 프로토콜은 성능 우위를 가질 수 있으며, 특히 애플리케이션의 일관성 제약이 합의 작업을 수행하지 않고 모든 시스템 복제본에서 읽기 전용 작업을 실행할 수 있음을 의미하는 경우 그렇다.

### 용량 및 부하 분산

배포를 설계할 때 부하를 처리하기에 충분한 용량이 있는지 확인해야 한다. 샤딩된 배포의 경우 샤드 수를 조정하여 용량을 조정할 수 있다. 그러나 리더가 아닌 합의 그룹 멤버에서 읽을 수 있는 시스템의 경우 더 많은 복제본을 추가하여 읽기 용량을 늘릴 수 있다. 더 많은 복제본을 추가하는 데는 비용이 든다: 강력한 리더를 사용하는 알고리즘에서 복제본을 추가하면 리더 프로세스에 더 많은 부하가 가해지는 반면, 피어 투 피어 프로토콜에서는 복제본을 추가하면 모든 프로세스에 더 많은 부하가 가해진다. 그러나 쓰기 작업에 충분한 용량이 있지만 읽기 중심 워크로드가 시스템을 스트레스하는 경우 복제본을 추가하는 것이 최선의 접근 방식일 수 있다.

다수 쿼럼 시스템에서 복제본을 추가하면 잠재적으로 시스템 가용성이 다소 감소할 수 있다는 점에 유의해야 한다. Zookeeper 또는 Chubby의 일반적인 배포는 5개의 복제본을 사용하므로 다수 쿼럼에는 3개의 복제본이 필요하다. 시스템은 2개의 복제본 또는 40%가 사용 불가능해도 여전히 진행한다. 6개의 복제본으로 쿼럼에는 4개의 복제본이 필요하다: 시스템이 라이브 상태를 유지하려면 복제본의 33%만 사용할 수 없다.

따라서 장애 도메인에 대한 고려 사항은 6번째 복제본이 추가될 때 훨씬 더 강하게 적용된다: 조직에 5개의 데이터센터가 있고 일반적으로 5개의 프로세스로 합의 그룹을 실행하며 각 데이터센터에 하나씩 있는 경우, 하나의 데이터센터가 손실되어도 각 그룹에 하나의 여분의 복제본이 남는다. 6번째 복제본이 5개의 데이터센터 중 하나에 배포되면 해당 데이터센터의 중단으로 그룹의 여분의 복제본 두 개가 모두 제거되어 용량이 33% 감소한다.

**클라이언트가 특정 지리적 지역에 밀집되어 있는 경우 클라이언트 가까이에 복제본을 배치하는 것이 가장 좋다.** 그러나 복제본을 정확히 어디에 배치할지 결정하는 것은 부하 분산과 시스템이 과부하를 처리하는 방법에 대한 신중한 고려가 필요할 수 있다. 시스템이 단순히 클라이언트 읽기 요청을 가장 가까운 복제본으로 라우팅하는 경우, 한 지역에 집중된 부하의 큰 급증이 가장 가까운 복제본을 압도한 다음 그 다음으로 가까운 복제본을 압도할 수 있다 - 이것은 **연쇄적 장애**다(22장 참조). 이러한 유형의 과부하는 배치 작업이 시작될 때, 특히 여러 개가 동시에 시작되는 경우 종종 발생할 수 있다.

많은 분산 합의 시스템이 성능을 향상시키기 위해 리더 프로세스를 사용하는 이유를 이미 살펴보았다. 그러나 **리더 복제본이 더 많은 컴퓨팅 리소스, 특히 나가는 네트워크 용량을 사용한다는 것을 이해하는 것이 중요하다.** 리더는 제안된 데이터를 포함하는 제안 메시지를 보내지만 복제본은 일반적으로 특정 합의 트랜잭션 ID와의 합의만 포함하는 더 작은 메시지를 보내기 때문이다. 매우 많은 수의 프로세스를 가진 고도로 샤딩된 합의 시스템을 실행하는 조직은 서로 다른 샤드의 리더 프로세스가 서로 다른 데이터센터에 걸쳐 비교적 균등하게 분산되도록 보장해야 할 필요가 있을 수 있다. 그렇게 하면 시스템 전체가 하나의 데이터센터의 나가는 네트워크 용량에 병목 현상이 발생하는 것을 방지하고 훨씬 더 큰 전체 시스템 용량을 만든다.

여러 데이터센터에 합의 그룹을 배포하는 또 다른 단점은 리더를 호스팅하는 데이터센터가 광범위한 장애(전원, 네트워킹 장비 장애 또는 광섬유 절단 등)를 겪는 경우 시스템에서 발생할 수 있는 매우 극단적인 변화다. 이 장애 시나리오에서 모든 리더는 다른 데이터센터로 페일오버해야 하며, 균등하게 분할되거나 한 데이터센터로 일괄적으로 이동한다. 어느 경우든 다른 두 데이터센터 간의 링크는 갑자기 이 시스템으로부터 훨씬 더 많은 네트워크 트래픽을 받게 될 것이다. 이것은 해당 링크의 용량이 불충분하다는 것을 발견하기에 부적절한 순간일 것이다.

### 쿼럼 구성

합의 그룹에 복제본을 배치할 위치를 결정할 때 지리적 분포(또는 더 정확하게는 복제본 간의 네트워크 지연 시간)가 그룹의 성능에 미치는 영향을 고려하는 것이 중요하다.

한 가지 접근 방식은 복제본을 가능한 한 균등하게 분산시켜 모든 복제본 간에 유사한 RTT를 갖도록 하는 것이다. 다른 모든 요소가 동일하다면(워크로드, 하드웨어 및 네트워크 성능 등), 이 배치는 그룹 리더가 어디에 위치하든(또는 리더 없는 프로토콜이 사용되는 경우 합의 그룹의 각 멤버에 대해) 모든 지역에서 상당히 일관된 성능을 가져야 한다.

**지리는 이 접근 방식을 크게 복잡하게 만들 수 있다.** 이것은 특히 대륙 내 트래픽 대 태평양 횡단 및 대서양 횡단 트래픽의 경우 그렇다. 북미와 유럽에 걸쳐 있는 시스템을 고려해보라: 복제본을 서로 같은 거리에 배치하는 것은 불가능한데, 대서양 횡단 트래픽에 대한 지연이 항상 대륙 내 트래픽보다 더 길기 때문이다. 어떻게 해도 한 지역의 트랜잭션은 합의에 도달하기 위해 대서양 횡단 왕복을 해야 할 것이다.

그러나 가능한 한 균등하게 트래픽을 분산시키기 위해 시스템 설계자는 5개의 복제본을 배치하도록 선택할 수 있으며, 미국에 대략 중앙에 2개의 복제본, 동부 해안에 1개, 유럽에 2개를 배치한다. 이러한 분포는 평균적으로 북미에서 유럽의 응답을 기다리지 않고 합의에 도달할 수 있거나, 유럽에서 동부 해안 복제본과만 메시지를 교환하여 합의에 도달할 수 있음을 의미한다. 동부 해안 복제본은 일종의 중심축 역할을 하며, 두 개의 가능한 쿼럼이 중복된다.

이 복제본의 손실은 시스템 지연 시간이 크게 변경될 가능성이 있음을 의미한다: 주로 미국 중부에서 동부 해안 RTT 또는 EU에서 동부 해안 RTT의 영향을 받는 대신 지연 시간은 EU에서 중부 RTT를 기반으로 하며, 이는 EU에서 동부 해안 RTT보다 약 50% 더 높다. 가장 가까운 가능한 쿼럼 간의 지리적 거리와 네트워크 RTT가 엄청나게 증가한다.

이 시나리오는 구성원 간에 RTT가 매우 다른 복제본으로 구성된 그룹에 적용될 때 단순 다수 쿼럼의 주요 약점이다. 이러한 경우 **계층적 쿼럼(hierarchical quorum)** 접근 방식이 유용할 수 있다. 9개의 복제본이 3개의 그룹으로 3개씩 배포될 수 있다. 쿼럼은 그룹의 과반수로 형성될 수 있으며, 그룹의 멤버 과반수가 사용 가능한 경우 그룹이 쿼럼에 포함될 수 있다. 이것은 중앙 그룹에서 복제본이 손실되어도 중앙 그룹이 여전히 3개 중 2개의 복제본으로 트랜잭션에 투표할 수 있기 때문에 전체 시스템 성능에 큰 영향을 미치지 않을 수 있음을 의미한다.

그러나 더 많은 수의 복제본을 실행하는 것과 관련된 리소스 비용이 있다. 읽기 중심 워크로드가 주로 복제본에 의해 충족될 수 있는 고도로 샤딩된 시스템에서는 더 적은 수의 합의 그룹을 사용하여 이 비용을 완화할 수 있다. 이러한 전략은 시스템의 전체 프로세스 수가 변경되지 않을 수 있음을 의미한다.

## 분산 합의 시스템 모니터링

이미 살펴본 바와 같이 분산 합의 알고리즘은 Google의 많은 중요한 시스템의 핵심에 있다. 모든 중요한 프로덕션 시스템은 중단이나 문제를 감지하고 문제를 해결하기 위해 모니터링이 필요하다. 경험을 통해 특별한 주의가 필요한 분산 합의 시스템의 특정 측면이 있음을 알게 되었다. 이것들은:

- **각 합의 그룹에서 실행 중인 멤버 수와 각 프로세스의 상태**(건강 또는 불건강)

  - 프로세스가 실행 중일 수 있지만 일부(예: 하드웨어 관련) 이유로 진행할 수 없을 수 있다
  - 지속적으로 지연되는 복제본

- **합의 그룹의 건강한 멤버는 여전히 여러 다른 상태에 있을 수 있다.** 그룹 멤버는 시작 후 피어로부터 상태를 복구하고 있거나, 그룹의 쿼럼보다 뒤처지거나, 최신 상태이고 완전히 참여하고 있을 수 있으며, 리더일 수 있다.

- **리더가 존재하는지 여부**

  - Multi-Paxos와 같이 리더 역할을 사용하는 알고리즘을 기반으로 하는 시스템은 리더가 존재하는지 확인하기 위해 모니터링되어야 하는데, 시스템에 리더가 없으면 완전히 사용할 수 없기 때문이다.

- **리더 변경 횟수**

  - 빠른 리더십 변경은 안정적인 리더를 사용하는 합의 시스템의 성능을 손상시키므로 리더 변경 횟수를 모니터링해야 한다. 합의 알고리즘은 일반적으로 새로운 임기 또는 뷰 번호로 리더십 변경을 표시하므로 이 번호는 모니터링할 유용한 메트릭을 제공한다. 너무 빠른 리더 변경 증가는 리더가 플래핑하고 있음을 나타내며, 아마도 네트워크 연결 문제 때문일 것이다. 뷰 번호의 감소는 심각한 버그를 나타낼 수 있다.

- **합의 트랜잭션 번호**

  - 운영자는 합의 시스템이 진행하고 있는지 여부를 알아야 한다. 대부분의 합의 알고리즘은 진행을 나타내기 위해 증가하는 합의 트랜잭션 번호를 사용한다. 이 번호는 시스템이 건강하다면 시간이 지남에 따라 증가하는 것으로 보여야 한다.

- **확인된 제안 수; 합의된 제안 수**

  - 이 숫자는 시스템이 올바르게 작동하고 있는지 여부를 나타낸다.

- **처리량 및 지연 시간**
  - 분산 합의 시스템에 특정하지 않지만, 합의 시스템의 이러한 특성은 관리자가 모니터링하고 이해해야 한다.

시스템 성능을 이해하고 성능 문제 해결을 돕기 위해 다음도 모니터링할 수 있다:

- **제안 수락에 대한 지연 시간 분포**
- 서로 다른 위치에 있는 시스템의 부분 간에 관찰된 네트워크 지연 시간의 분포
- 수락자가 내구성 있는 로깅에 소비하는 시간
- 시스템에서 초당 수락된 전체 바이트 수

## 결론

우리는 분산 합의 문제의 정의를 탐구했고, 분산 합의 기반 시스템의 일부 시스템 아키텍처 패턴을 제시했으며, 성능 특성과 분산 합의 기반 시스템에 대한 일부 운영 관심사를 검토했다.

이 장에서는 특정 알고리즘, 프로토콜 또는 구현에 대한 심층적인 논의를 의도적으로 피했다. 분산 조정 시스템과 이를 뒷받침하는 기술은 빠르게 발전하고 있으며, 이러한 정보는 여기서 논의된 기본 원칙과 달리 빠르게 구식이 될 것이다. 그러나 이러한 기본 원칙과 이 장 전체에서 참조된 논문은 오늘날 사용 가능한 분산 조정 도구와 미래의 소프트웨어를 사용할 수 있게 해줄 것이다.

**이 장에서 다른 것을 기억하지 못하더라도, 분산 합의가 해결하는 데 사용될 수 있는 문제의 종류와 하트비트와 같은 임시 방법이 분산 합의 대신 사용될 때 발생할 수 있는 문제 유형을 명심하라. 리더 선출, 중요한 공유 상태 또는 분산 락킹이 보일 때마다 분산 합의를 생각하라: 더 작은 접근 방식은 시스템에서 폭발하기를 기다리는 째깍거리는 폭탄이다.**

- **처리량 및 지연 시간**
  - 분산 합의 시스템에 특정하지 않지만, 합의 시스템의 이러한 특성은 관리자가 모니터링하고 이해해야 한다.

시스템 성능을 이해하고 성능 문제 해결을 돕기 위해 다음도 모니터링할 수 있다:

- **제안 수락에 대한 지연 시간 분포**
- 서로 다른 위치에 있는 시스템의 부분 간에 관찰된 네트워크 지연 시간의 분포
- 수락자가 내구성 있는 로깅에 소비하는 시간
- 시스템에서 초당 수락된 전체 바이트 수

## 결론

우리는 분산 합의 문제의 정의를 탐구했고, 분산 합의 기반 시스템의 일부 시스템 아키텍처 패턴을 제시했으며, 성능 특성과 분산 합의 기반 시스템에 대한 일부 운영 관심사를 검토했다.

이 장에서는 특정 알고리즘, 프로토콜 또는 구현에 대한 심층적인 논의를 의도적으로 피했다. 분산 조정 시스템과 이를 뒷받침하는 기술은 빠르게 발전하고 있으며, 이러한 정보는 여기서 논의된 기본 원칙과 달리 빠르게 구식이 될 것이다. 그러나 이러한 기본 원칙과 이 장 전체에서 참조된 논문은 오늘날 사용 가능한 분산 조정 도구와 미래의 소프트웨어를 사용할 수 있게 해줄 것이다.

**이 장에서 다른 것을 기억하지 못하더라도, 분산 합의가 해결하는 데 사용될 수 있는 문제의 종류와 하트비트와 같은 임시 방법이 분산 합의 대신 사용될 때 발생할 수 있는 문제 유형을 명심하라. 리더 선출, 중요한 공유 상태 또는 분산 락킹이 보일 때마다 분산 합의를 생각하라: 더 작은 접근 방식은 시스템에서 폭발하기를 기다리는 째깍거리는 폭탄이다.**

---

# 24장. 크론을 이용한 분산된 주기적 스케줄링

**작성자**: Štěpán Davidovič  
**편집자**: Kavita Guliani

## 개요

이 장은 구글의 분산 크론(cron) 서비스 구현을 설명합니다. 이 서비스는 주기적인 컴퓨팅 작업 스케줄링이 필요한 대다수의 내부 팀을 지원합니다. 크론의 존재 기간 동안, 구글은 기본적으로 보이는 서비스를 설계하고 구현하는 방법에 대해 많은 교훈을 얻었습니다.

## 크론(Cron)

### 소개

크론은 시스템 관리자와 일반 사용자가 명령을 실행할 시간을 지정할 수 있도록 설계된 일반적인 Unix 유틸리티입니다. 크론은 다음과 같은 다양한 작업을 실행합니다:

- 가비지 컬렉션
- 주기적인 데이터 분석

가장 일반적인 시간 지정 형식은 "crontab"이라고 불립니다. 이 형식은 다음을 지원합니다:

- 간단한 간격 (예: "매일 정오에 한 번" 또는 "매 시간 정각")
- 복잡한 간격 (예: "매달 30일이면서 토요일인 날")

크론은 일반적으로 `crond`라고 불리는 단일 컴포넌트로 구현됩니다. `crond`는 예약된 크론 작업 목록을 로드하는 데몬입니다. 작업은 지정된 실행 시간에 따라 실행됩니다.

### 신뢰성 관점

크론 서비스의 신뢰성 관점에서 주목할 만한 점들:

1. **실패 도메인**

   - 크론의 실패 도메인은 본질적으로 단일 머신입니다
   - 머신이 실행되지 않으면 크론 스케줄러와 실행하는 작업 모두 실행될 수 없습니다
   - 두 머신을 사용하는 간단한 분산 케이스: 스케줄러 머신 또는 대상 머신 중 하나가 실패할 수 있는 두 개의 별개 실패 도메인이 존재

2. **상태 지속성**
   - `crond` 재시작(머신 재부팅 포함) 시 유지되어야 하는 유일한 상태는 crontab 구성 자체입니다
   - 크론 실행은 fire-and-forget 방식이며, `crond`는 이를 추적하지 않습니다
   - `anacron`은 주목할 만한 예외입니다:
     - 시스템이 다운되었을 때 실행되었어야 할 작업을 실행하려고 시도
     - 재실행 시도는 매일 또는 그보다 적은 빈도로 실행되는 작업으로 제한
     - 워크스테이션과 노트북에서 유지보수 작업을 실행하는 데 매우 유용
     - 모든 등록된 크론 작업의 마지막 실행 타임스탬프를 보존하는 파일로 지원

### 크론 작업과 멱등성(Idempotency)

크론 작업은 주기적인 작업을 수행하도록 설계되었지만, 그 이상의 기능을 미리 알기는 어렵습니다. 다양한 크론 작업이 요구하는 다양성은 명백히 신뢰성 요구사항에 영향을 미칩니다.

**멱등성 크론 작업**

- 가비지 컬렉션 프로세스와 같은 일부 크론 작업은 멱등적입니다
- 시스템 오작동 시 여러 번 실행해도 안전합니다

**비멱등성 크론 작업**

- 광범위한 배포를 위해 이메일 뉴스레터를 보내는 프로세스와 같은 작업
- 한 번 이상 실행되어서는 안 됩니다

**실행 실패 허용성**

- 일부 크론 작업은 실행 실패가 허용됩니다:
  - 5분마다 실행되도록 예약된 가비지 컬렉션 크론 작업은 한 번 건너뛸 수 있음
- 일부 크론 작업은 건너뛰어서는 안 됩니다:
  - 한 달에 한 번 실행되도록 예약된 급여 작업

**설계 원칙**

- 크론 작업의 다양성으로 인해 실패 모드에 대한 추론이 어렵습니다
- 일반적으로 이중 실행 위험보다는 실행 건너뛰기를 선호합니다
- 건너뛴 실행에서 복구하는 것이 이중 실행에서 복구하는 것보다 더 실행 가능합니다
- "fail closed" 방식을 선호하여 시스템적으로 잘못된 상태를 생성하는 것을 방지합니다

## 대규모 크론

### 확장된 인프라

**단일 머신의 한계**

- 일반적인 구현에서 크론은 단일 머신으로 제한됩니다
- 대규모 시스템 배포는 크론 솔루션을 여러 머신으로 확장합니다
- 예: 정확히 1,000대의 머신이 있는 데이터센터에서 단일 머신에 크론 서비스를 호스팅하면, 사용 가능한 머신의 1/1000만 실패해도 전체 크론 서비스가 중단될 수 있습니다

**프로세스와 머신의 분리**

- 크론의 신뢰성을 높이기 위해 프로세스를 머신에서 분리합니다
- 서비스를 실행하려면 서비스 요구사항과 실행할 데이터센터를 지정하기만 하면 됩니다
- 데이터센터 스케줄링 시스템(자체적으로 신뢰할 수 있어야 함)이 서비스를 배포할 머신을 결정하고 머신 장애를 처리합니다
- 데이터센터에서 작업을 실행하는 것은 효과적으로 데이터센터 스케줄러에 하나 이상의 RPC를 보내는 것으로 바뀝니다

**재스케줄링의 과제**

- 이 프로세스는 즉각적이지 않습니다:
  - 죽은 머신을 발견하는 데는 상태 확인 타임아웃이 필요
  - 서비스를 다른 머신에 재스케줄링하는 데는 소프트웨어 설치 및 새 프로세스 시작 시간이 필요

**상태 및 시간 요구사항 완화**

- 프로세스를 다른 머신으로 이동하면 이전 머신에 저장된 로컬 상태가 손실될 수 있습니다 (라이브 마이그레이션이 사용되지 않는 한)
- 재스케줄링 시간이 1분의 최소 스케줄링 간격을 초과할 수 있습니다
- 해결 방법:
  - **분산 파일시스템 사용**: GFS와 같은 분산 파일시스템에 상태를 유지하고, 시작 시 이를 사용하여 재스케줄링으로 인해 실행에 실패한 작업 식별
  - **핫 스페어(hot spares)**: 신속하게 개입하여 작업을 재개할 수 있는 핫 스페어는 이 시간 창을 크게 단축할 수 있습니다
  - 그러나 분산 파일시스템 솔루션은 적시성 기대에 미치지 못합니다: 5분마다 실행되는 크론 작업의 경우, 크론 시스템 재스케줄링의 총 오버헤드로 인한 1-2분 지연은 잠재적으로 상당히 큽니다

### 확장된 요구사항

**격리(Isolation)**

- 단일 머신 시스템은 일반적으로 모든 실행 중인 프로세스를 제한된 격리로 함께 배치합니다
- 컨테이너가 이제 일반적이지만, 단일 머신에 배포된 서비스의 모든 단일 컴포넌트를 격리하기 위해 컨테이너를 사용하는 것이 필요하거나 일반적이지는 않습니다
- 데이터센터 규모의 배포는 일반적으로 격리를 강제하는 컨테이너에 배포를 의미합니다
- 격리가 필요한 이유: 동일한 데이터센터에서 실행되는 독립 프로세스가 서로 부정적인 영향을 미치지 않아야 한다는 기본 기대

**리소스 요구사항**

- 실행하려는 프로세스에 대해 미리 필요한 리소스 양을 알아야 합니다 (크론 시스템과 실행하는 작업 모두)
- 크론 작업은 데이터센터에 크론 작업의 요구를 충족할 리소스가 없는 경우 지연될 수 있습니다
- 리소스 요구사항과 크론 작업 실행 모니터링에 대한 사용자 요구로 인해, 예약된 실행부터 종료까지 크론 작업 실행의 전체 상태를 추적해야 합니다

**부분 실행 실패**

- 특정 머신에서 프로세스 실행을 분리하면 크론 시스템이 부분 실행 실패에 노출됩니다
- 크론 작업 구성의 다양성은 데이터센터에서 새 크론 작업을 실행하는 데 여러 RPC가 필요할 수 있음을 의미합니다
- 때때로 일부 RPC는 성공했지만 다른 RPC는 성공하지 못한 시나리오 발생 (예: RPC를 보내는 프로세스가 이러한 작업을 실행하는 중간에 죽음)
- 크론 복구 절차도 이 시나리오를 고려해야 합니다

**복잡한 실패 모드**

- 데이터센터는 단일 머신보다 훨씬 더 복잡한 생태계입니다
- 단일 머신의 비교적 간단한 바이너리로 시작된 크론 서비스는 이제 대규모로 배포될 때 많은 명백하고 명백하지 않은 종속성을 가집니다
- 크론과 같은 기본 서비스의 경우, 데이터센터가 부분 장애를 겪더라도 (예: 부분 정전 또는 스토리지 서비스 문제) 서비스가 여전히 기능할 수 있도록 해야 합니다
- 데이터센터 스케줄러가 데이터센터 내 다양한 위치에 크론 복제본을 배치하도록 요구함으로써, 단일 전력 분배 장치의 장애가 크론 서비스의 모든 프로세스를 중단시키는 시나리오를 피합니다

**배포 전략**

- 전 세계에 걸쳐 단일 크론 서비스를 배포하는 것이 가능할 수 있지만, 단일 데이터센터 내에 크론을 배포하는 것이 이점이 있습니다:
  - 서비스가 낮은 지연 시간을 누림
  - 크론의 핵심 종속성인 데이터센터 스케줄러와 운명을 공유

## 구글에서 크론 구축하기

### 크론 작업 상태 추적

**상태 추적의 필요성**

- 크론 작업에 대한 일정량의 상태를 보유하고 장애 시 해당 정보를 신속하게 복원할 수 있어야 합니다
- 해당 상태의 일관성이 가장 중요합니다
- 많은 크론 작업 (급여 실행 또는 이메일 뉴스레터 전송과 같은)은 멱등적이지 않습니다

**상태 추적 옵션**

1. **일반적으로 사용 가능한 분산 스토리지에 데이터를 외부에 저장**
2. **크론 서비스 자체의 일부로 소량의 상태를 저장하는 시스템 사용**

**선택한 옵션: 두 번째 옵션**

선택 이유:

- **분산 파일시스템의 비효율성**: GFS 또는 HDFS와 같은 분산 파일시스템은 매우 큰 파일 (예: 웹 크롤링 프로그램의 출력) 사용 사례에 맞춰져 있지만, 크론 작업에 대해 저장해야 하는 정보는 매우 작습니다. 분산 파일시스템의 작은 쓰기는 파일시스템이 이러한 유형의 쓰기에 최적화되지 않았기 때문에 매우 비싸고 높은 지연 시간이 발생합니다.

- **종속성 최소화**: 크론과 같이 중단이 광범위한 영향을 미치는 기본 서비스는 매우 적은 종속성을 가져야 합니다. 데이터센터의 일부가 사라지더라도 크론 서비스는 적어도 일정 시간 동안 기능할 수 있어야 합니다. 그러나 이는 스토리지가 크론 프로세스 자체의 일부여야 한다는 의미는 아닙니다 (스토리지 처리 방법은 본질적으로 구현 세부사항). 그러나 크론은 많은 내부 사용자에게 서비스를 제공하는 다운스트림 시스템과 독립적으로 작동할 수 있어야 합니다.

### Paxos의 사용

**배포 및 일관성**

- 크론 서비스의 여러 복제본을 배포하고 Paxos 분산 합의 알고리즘을 사용하여 일관된 상태를 보장합니다
- 그룹 멤버의 대다수가 사용 가능한 한, 분산 시스템 전체는 인프라의 제한된 하위 집합의 장애에도 불구하고 새로운 상태 변경을 성공적으로 처리할 수 있습니다

**리더-팔로워 구조**

- 분산 크론은 단일 리더 작업을 사용합니다:
  - 공유 상태를 수정할 수 있는 유일한 복제본
  - 크론 작업을 실행할 수 있는 유일한 복제본
- Fast Paxos가 내부적으로 최적화로 리더 복제본을 사용한다는 사실을 활용합니다
- Fast Paxos 리더 복제본은 크론 서비스 리더 역할도 수행합니다

**리더 재선출**

- 리더 복제본이 죽으면 Paxos 그룹의 상태 확인 메커니즘이 이 이벤트를 신속하게 (몇 초 이내에) 발견합니다
- 다른 크론 프로세스가 이미 시작되어 사용 가능하므로 새 리더를 선출할 수 있습니다
- 새 리더가 선출되자마자 크론 서비스에 특정한 리더 선출 프로토콜을 따릅니다:
  - 이전 리더가 끝내지 못한 모든 작업을 인수할 책임이 있습니다
- 리더 재선출에 대한 빠른 반응 시간을 통해 일반적으로 허용 가능한 1분 장애 조치 시간 내에 잘 유지할 수 있습니다

**Paxos에 보관하는 중요한 상태**

- 가장 중요한 상태는 어떤 크론 작업이 실행되었는지에 대한 정보입니다
- 각 크론 작업에 대해 예약된 각 실행의 시작과 끝을 복제본의 정족수(quorum)에 동기적으로 알립니다

### 리더와 팔로워의 역할

#### 리더(Leader)

**주요 책임**

- 리더 복제본은 크론 작업을 적극적으로 실행하는 유일한 복제본입니다
- 리더는 내부 스케줄러를 가지고 있습니다:
  - 이 장의 시작 부분에서 설명한 간단한 `crond`와 마찬가지로 예약된 실행 시간별로 정렬된 크론 작업 목록을 유지합니다
  - 리더 복제본은 첫 번째 작업의 예약된 실행 시간까지 대기합니다

**작업 실행 프로세스**

1. 예약된 실행 시간에 도달하면 리더 복제본은 이 특정 크론 작업의 실행을 시작하려고 한다고 발표합니다
2. 일반적인 `crond` 구현과 마찬가지로 새로운 예약된 실행 시간을 계산합니다
3. 크론 작업 실행 사양이 마지막 실행 이후 변경되었을 수 있으므로 이 실행 사양도 팔로워와 동기화되어야 합니다
4. 크론 작업만 식별하는 것으로는 충분하지 않습니다:
   - 시작 시간을 사용하여 특정 실행을 고유하게 식별해야 합니다
   - 그렇지 않으면 크론 작업 실행 추적에서 모호성이 발생할 수 있습니다 (특히 매분마다 실행되는 것과 같은 고빈도 크론 작업의 경우)

**동기 통신의 중요성**

- Paxos 통신은 동기적으로 유지되어야 합니다
- 실제 크론 작업 실행은 Paxos 정족수가 실행 알림을 받았다는 확인을 받을 때까지 진행되지 않습니다
- 크론 서비스는 리더 장애 조치 시 다음 조치를 결정하기 위해 각 크론 작업이 실행되었는지 이해해야 합니다
- 이 작업을 동기적으로 수행하지 않으면:
  - 전체 크론 작업 실행이 팔로워 복제본에 알리지 않고 리더에서 발생할 수 있습니다
  - 장애 조치 시 팔로워 복제본은 실행이 이미 발생했음을 알지 못하기 때문에 동일한 실행을 다시 시도할 수 있습니다

**실행 완료 알림**

- 크론 작업 실행의 완료는 Paxos를 통해 다른 복제본에 동기적으로 발표됩니다
- 외부 이유로 실행이 성공했는지 실패했는지는 중요하지 않습니다 (예: 데이터센터 스케줄러를 사용할 수 없었던 경우)
- 여기서는 크론 서비스가 주어진 예약된 시간에 실행을 시도했다는 사실을 추적하고 있습니다

**리더십 상실 시 조치**

- 리더의 또 다른 매우 중요한 기능: 어떤 이유로든 리더십을 잃는 즉시 데이터센터 스케줄러와의 상호 작용을 즉시 중단해야 합니다
- 리더십을 보유하면 데이터센터 스케줄러에 대한 액세스의 상호 배제를 보장해야 합니다
- 이러한 상호 배제 조건이 없으면 이전 리더와 새 리더가 데이터센터 스케줄러에서 충돌하는 작업을 수행할 수 있습니다

#### 팔로워(Follower)

**주요 책임**

- 팔로워 복제본은 필요 시 즉시 인수하기 위해 리더가 제공하는 세계 상태를 추적합니다
- 팔로워 복제본이 추적하는 모든 상태 변경은 리더 복제본에서 Paxos를 통해 전달됩니다
- 리더와 마찬가지로 팔로워도 시스템의 모든 크론 작업 목록을 유지하며, 이 목록은 복제본 간에 일관성을 유지해야 합니다 (Paxos 사용을 통해)

**실행 알림 처리**

- 시작된 실행에 대한 알림을 받으면 팔로워 복제본은 주어진 크론 작업에 대한 로컬 다음 예약된 실행 시간을 업데이트합니다
- 이 매우 중요한 상태 변경 (동기적으로 수행됨)은 시스템 내의 모든 크론 작업 일정이 일관성을 유지하도록 보장합니다
- 모든 열린 실행 (시작되었지만 완료되지 않은 실행)을 추적합니다

**리더 선출**

- 리더 복제본이 죽거나 오작동하면 (예: 네트워크에서 다른 복제본과 분리됨) 팔로워가 새 리더로 선출되어야 합니다
- 선출은 크론 작업 실행을 놓치거나 과도하게 지연시키는 위험을 피하기 위해 1분보다 빠르게 수렴해야 합니다
- 리더가 선출되면 모든 열린 실행 (즉, 부분 실패)을 종료해야 합니다

### 부분 실패 해결

**문제점**

- 리더 복제본과 데이터센터 스케줄러 간의 상호 작용은 단일 논리적 크론 작업 실행을 설명하는 여러 RPC를 보내는 중간에 실패할 수 있습니다
- 시스템이 이 조건을 처리할 수 있어야 합니다

**동기화 포인트**
모든 크론 작업 실행에는 두 개의 동기화 포인트가 있습니다:

1. 실행을 수행하려고 할 때
2. 실행을 완료했을 때

이 두 포인트를 통해 실행을 구분할 수 있습니다.

**RPC 전송 확인 문제**

- 실행이 단일 RPC로 구성되어 있더라도 RPC가 실제로 전송되었는지 어떻게 알 수 있습니까?
- 예약된 실행이 시작되었음을 알고 있지만 리더 복제본이 죽기 전에 완료 알림을 받지 못한 경우를 고려하십시오

**해결 조건**

RPC가 실제로 전송되었는지 확인하려면 다음 조건 중 하나가 충족되어야 합니다:

1. **멱등성**: 재선출 시 계속해야 할 수 있는 외부 시스템의 모든 작업은 멱등적이어야 합니다 (즉, 작업을 안전하게 다시 수행할 수 있음)

2. **상태 조회**: 완료 여부를 명확하게 결정하기 위해 외부 시스템의 모든 작업 상태를 조회할 수 있어야 합니다

이러한 각 조건은 상당한 제약을 부과하며 구현하기 어려울 수 있지만, 이러한 조건 중 하나 이상을 충족할 수 있는 것은 단일 또는 여러 부분 실패를 겪을 수 있는 분산 환경에서 크론 서비스의 정확한 작동에 근본적입니다.
**구현 솔루션** (계속)

- 데이터센터에서 논리적 작업을 실행하는 대부분의 인프라 (예: Mesos)는 해당 데이터센터 작업에 대한 이름 지정을 제공하여 작업 상태를 조회하거나, 작업을 중지하거나, 기타 유지보수를 수행할 수 있습니다
- 멱등성 문제에 대한 합리적인 솔루션:
  1. 미리 작업 이름을 구성합니다 (따라서 데이터센터 스케줄러에서 변경 작업을 일으키지 않음)
  2. 그런 다음 이름을 크론 서비스의 모든 복제본에 배포합니다
  3. 크론 서비스 리더가 실행 중에 죽으면 새 리더는 단순히 모든 미리 계산된 이름의 상태를 조회하고 누락된 이름을 실행합니다

**작업 이름 지정의 중요성**

- 이름과 실행 시간으로 개별 크론 작업 실행을 식별하는 방법과 유사하게, 데이터센터 스케줄러에서 구성된 작업 이름에 특정 예약된 실행 시간을 포함하는 것이 중요합니다 (또는 이 정보를 다른 방식으로 검색할 수 있어야 함)
- 일반 작동에서 크론 서비스는 리더 장애 시 신속하게 장애 조치해야 하지만 빠른 장애 조치가 항상 발생하는 것은 아닙니다

**예시 시나리오**

- 짧은 수명이지만 자주 실행되는 크론 작업을 고려하십시오:
  1. 크론 작업이 실행되지만 실행이 모든 복제본에 전달되기 전에 리더가 충돌합니다
  2. 비정상적으로 긴 장애 조치가 발생합니다 - 크론 작업이 성공적으로 완료될 만큼 충분히 긴 시간
  3. 새 리더는 크론 작업의 상태를 조회하고 완료를 관찰한 다음 작업을 다시 실행하려고 시도합니다
  4. 실행 시간이 포함되었다면 새 리더는 데이터센터 스케줄러의 작업이 이 특정 크론 작업 실행의 결과임을 알 수 있었고, 이 이중 실행은 발생하지 않았을 것입니다

**실제 구현**

- 실제 구현에는 기본 인프라의 구현 세부사항에 따라 상태 조회를 위한 더 복잡한 시스템이 있습니다
- 그러나 앞의 설명은 이러한 시스템의 구현 독립적 요구사항을 다룹니다
- 사용 가능한 인프라에 따라 이중 실행 위험과 실행 건너뛰기 위험 간의 균형을 고려해야 할 수도 있습니다

### 상태 저장

**Paxos와 상태 관리**

- Paxos를 사용하여 합의를 달성하는 것은 상태 처리 방법 문제의 한 부분일 뿐입니다
- Paxos는 본질적으로 상태 변경이 발생할 때 동기적으로 추가되는 연속적인 상태 변경 로그입니다

**Paxos의 두 가지 영향**:

1. **로그 압축**: 로그가 무한정 증가하는 것을 방지하기 위해 압축되어야 합니다
2. **로그 저장**: 로그 자체가 어딘가에 저장되어야 합니다

**로그 무한 증가 방지**

- 현재 상태의 스냅샷을 찍으면 됩니다
- 이는 현재 상태로 이어지는 모든 상태 변경 로그 항목을 재생할 필요 없이 상태를 재구성할 수 있음을 의미합니다
- 예시: 로그에 저장된 상태 변경이 "카운터를 1씩 증가"라면, 천 번 반복 후 천 개의 로그 항목을 "카운터를 1,000으로 설정"이라는 스냅샷으로 쉽게 변경할 수 있습니다

**로그 손실 시나리오**

- 로그가 손실된 경우 마지막 스냅샷 이후의 상태만 손실합니다
- 스냅샷은 실제로 가장 중요한 상태입니다:
  - 스냅샷을 잃으면 내부 상태를 잃었기 때문에 본질적으로 처음부터 다시 시작해야 합니다
- 로그를 잃는 것은 제한된 상태 손실만 야기하고 크론 시스템을 최신 스냅샷이 찍힌 시점으로 되돌립니다

**데이터 저장 옵션**

1. **일반적으로 사용 가능한 분산 스토리지에 외부 저장**
2. **크론 서비스 자체의 일부로 소량의 상태를 저장하는 시스템**

시스템을 설계할 때 두 옵션의 요소를 결합했습니다.

**구글의 저장 전략**

**Paxos 로그 저장**:

- 크론 서비스 복제본이 예약된 머신의 로컬 디스크에 Paxos 로그를 저장합니다
- 기본 작동에서 세 개의 복제본을 갖는다는 것은 로그의 세 개 복사본을 갖는다는 의미입니다

**스냅샷 저장**:

- 스냅샷도 로컬 디스크에 저장합니다
- 그러나 중요하기 때문에 분산 파일시스템에도 백업하여 세 머신 모두에 영향을 미치는 장애로부터 보호합니다

**로그를 분산 파일시스템에 저장하지 않는 이유**

- 로그를 잃는 것은 최근 상태 변경의 적은 양을 나타내므로 허용 가능한 위험입니다
- 분산 파일시스템에 로그를 저장하면 빈번한 작은 쓰기로 인한 상당한 성능 저하가 발생할 수 있습니다
- 세 머신의 동시 손실은 가능성이 낮습니다
- 동시 손실이 발생하면 스냅샷에서 자동으로 복원합니다
- 구성 가능한 간격으로 수행하는 마지막 스냅샷 이후에 찍은 로그만 손실합니다
- 물론 이러한 균형은 인프라의 세부사항과 크론 시스템에 부과된 요구사항에 따라 다를 수 있습니다

**추가 복원 메커니즘**

- 로컬 디스크에 저장된 로그 및 스냅샷과 분산 파일시스템의 스냅샷 백업 외에도, 새로 시작된 복제본은 네트워크를 통해 이미 실행 중인 복제본에서 상태 스냅샷과 모든 로그를 가져올 수 있습니다
- 이 기능은 복제본 시작을 로컬 머신의 모든 상태와 독립적으로 만듭니다
- 따라서 재시작 시 (또는 머신 장애 시) 복제본을 다른 머신으로 재스케줄링하는 것은 서비스 신뢰성에 본질적으로 문제가 되지 않습니다

## 대규모 크론 실행

### 썬더링 허드(Thundering Herd) 문제

**문제점**

- 전통적인 크론은 작습니다: 기껏해야 수십 개의 크론 작업을 포함할 것입니다
- 그러나 데이터센터의 수천 대의 머신에 대해 크론 서비스를 실행하면 사용량이 증가하고 문제가 발생할 수 있습니다
- 분산 시스템의 크고 잘 알려진 문제: **썬더링 허드(thundering herd)**

**사용자 구성으로 인한 스파이크**

- 크론 서비스는 사용자 구성에 따라 데이터센터 사용량에 상당한 스파이크를 일으킬 수 있습니다
- 사람들이 "일일 크론 작업"을 생각할 때 일반적으로 자정에 실행되도록 이 작업을 구성합니다
- 크론 작업이 동일한 머신에서 실행되는 경우 이 설정은 잘 작동합니다
- 그러나 크론 작업이 수천 개의 워커가 있는 MapReduce를 생성할 수 있다면?
- 30개의 다른 팀이 이와 같은 일일 크론 작업을 동일한 데이터센터에서 실행하기로 결정한다면?

**해결책: Crontab 형식 확장**

- 일반 crontab에서 사용자는 크론 작업이 실행되어야 하는 분, 시간, 월의 일(또는 주), 월을 지정하거나 모든 값을 지정하려면 별표를 지정합니다
- 자정에 매일 실행하는 것은 "0 0 \* \* \*"의 crontab 사양을 갖습니다 (즉, 0번째 분, 0번째 시간, 매주 매일, 매월, 매주 매일)

**물음표(?) 도입**

- 물음표는 모든 값이 허용되고 크론 시스템이 값을 자유롭게 선택할 수 있음을 의미합니다
- 사용자는 주어진 시간 범위에 대해 크론 작업 구성을 해싱하여 이 값을 선택합니다 (예: 시간의 경우 `0..23`)
- 따라서 해당 실행을 더 고르게 분산합니다

**여전히 남아 있는 부하 스파이크**

- 이러한 변경에도 불구하고 크론 작업으로 인한 부하는 여전히 매우 스파이크가 심합니다
- 그림 24-3의 그래프는 구글에서 크론 작업 실행의 전체 글로벌 수를 보여줍니다
- 이 그래프는 크론 작업 실행의 빈번한 스파이크를 강조합니다
- 이는 종종 특정 시간에 실행되어야 하는 크론 작업으로 인해 발생합니다 - 예를 들어 외부 이벤트에 대한 시간적 종속성으로 인해

## 요약

**크론 서비스의 역사와 진화**

- 크론 서비스는 수십 년 동안 UNIX 시스템의 기본 기능이었습니다
- 데이터센터가 가장 작은 효과적인 하드웨어 단위일 수 있는 대규모 분산 시스템으로의 업계 이동은 스택의 많은 부분에서 변경을 요구합니다
- 크론도 이러한 추세의 예외가 아닙니다

**구글의 새로운 설계**

- 크론 서비스의 필수 속성과 크론 작업의 요구사항을 신중하게 살펴보면 구글의 새로운 설계가 도출됩니다

**핵심 내용**

1. **분산 시스템 환경의 새로운 제약사항**: 이 장에서는 분산 시스템 환경에서 요구하는 새로운 제약사항과 구글의 솔루션을 기반으로 한 크론 서비스의 가능한 설계를 논의했습니다

2. **강력한 일관성 보장의 필요성**: 이 솔루션은 분산 환경에서 강력한 일관성 보장을 요구합니다

3. **Paxos의 핵심 역할**: 분산 크론 구현의 핵심은 Paxos입니다 - 신뢰할 수 없는 환경에서 합의에 도달하는 일반적인 알고리즘입니다

4. **견고한 크론 서비스 구축**: Paxos의 사용과 대규모 분산 환경에서 크론 작업의 새로운 실패 모드에 대한 올바른 분석을 통해 구글에서 많이 사용되는 견고한 크론 서비스를 구축할 수 있었습니다

**주요 교훈**

- 단순해 보이는 서비스도 대규모 분산 환경에서는 복잡한 설계가 필요합니다
- 신뢰성을 위해서는 실패 모드에 대한 철저한 분석과 이에 대한 적절한 대응이 필수적입니다
- 분산 합의 알고리즘(Paxos)은 대규모 시스템에서 일관성을 보장하는 핵심 도구입니다

# 25장. 데이터 처리 파이프라인

## 개요

- 깊이와 복잡성을 가진 데이터 처리 파이프라인 관리의 실제 과제 집중
- 매우 드물게 실행되는 정기적 파이프라인부터 멈추지 않고 실행되는 연속 파이프라인까지의 빈도 연속체 고려
- 중대한 운영 문제를 야기할 수 있는 불연속성 논의
- 빅데이터 처리를 위한 정기적 파이프라인의 더 신뢰성 있고 확장 가능한 대안으로 리더-팔로워 모델의 새로운 접근 제시

## 파이프라인 디자인 패턴의 기원

### 고전적 데이터 처리 접근법

- 데이터를 읽고, 원하는 방식으로 변환하고, 새 데이터를 출력하는 프로그램 작성
- 일반적으로 cron과 같은 주기적 스케줄링 프로그램의 제어 하에 실행되도록 예약
- 이러한 디자인 패턴을 데이터 파이프라인이라고 명명

### 파이프라인의 역사

- 코루틴(co-routines) 시대부터 시작
- DTSS 통신 파일
- UNIX 파이프
- ETL 파이프라인으로 발전
- "빅데이터"의 부상과 함께 주목도 증가
- 빅데이터 정의: "전통적인 데이터 처리 애플리케이션으로는 부적절할 정도로 크고 복잡한 데이터셋"

## 단순한 파이프라인 패턴을 적용한 빅데이터의 기본적인 효과

### 단순, 단일 단계 파이프라인

- 빅데이터에 대한 주기적 또는 연속적 변환을 수행하는 프로그램
- 일반적으로 "단순, 단일 단계 파이프라인"으로 지칭

### 다단계 파이프라인

- 빅데이터의 규모와 처리 복잡성으로 인해 프로그램은 일반적으로 연쇄 시리즈로 구성
- 한 프로그램의 출력이 다음 프로그램의 입력으로 사용
- 시스템에 대한 추론의 용이성을 위해 설계되며 일반적으로 운영 효율성을 목표로 하지 않음
- 체인의 각 프로그램이 개별 데이터 처리 단계로 작동하기 때문에 다단계 파이프라인으로 명명

### 파이프라인 깊이

- 시리즈로 연결된 프로그램의 수를 측정하는 지표
- 얕은 파이프라인: 하나의 프로그램만 있고 파이프라인 깊이가 1
- 깊은 파이프라인: 수십 또는 수백 개의 프로그램으로 파이프라인 깊이 측정

## 정기적인 파이프라인 패턴의 과제

### 안정성 조건

- 데이터 볼륨과 실행 요구에 대한 충분한 워커가 있을 때 일반적으로 안정적
- 연결된 작업의 수와 작업 간 상대적 처리량이 균일하게 유지될 때 처리 병목 현상과 같은 불안정성 회피

### 구글의 사용 현황

- 정기적 파이프라인은 유용하고 실용적이며 구글에서 정기적으로 실행
- MapReduce, Flume 등의 프레임워크로 작성

### 취약성 발견

- SRE의 집단적 경험: 정기적 파이프라인 모델은 취약함
- 초기 설치 시 워커 크기 조정, 주기성, 청킹 기법 및 기타 매개변수를 신중하게 조정하면 초기에는 성능이 신뢰할 수 있음
- 그러나 유기적 성장과 변화가 불가피하게 시스템에 스트레스를 주기 시작하면 문제 발생

### 발생 문제 사례

- 실행 마감 시간을 초과하는 작업
- 리소스 고갈
- 처리 청크 중단으로 인한 해당 운영 부하

## 작업의 불균형 분산으로 인해 발생하는 문제

### 빅데이터의 핵심 돌파구

- "당황스러울 정도로 병렬적인(embarrassingly parallel)" 알고리즘의 광범위한 적용
- 큰 워크로드를 개별 머신에 맞을 만큼 작은 청크로 분할

### 불균등 리소스 요구 문제

- 때때로 청크는 서로에 비해 불균등한 양의 리소스 필요
- 특정 청크가 서로 다른 양의 리소스를 요구하는 이유가 초기에는 명확하지 않은 경우가 많음
- 예시: 고객별로 분할된 워크로드에서 일부 고객의 데이터 청크가 다른 고객보다 훨씬 클 수 있음
- 고객이 불가분성의 지점이기 때문에 종단 간 런타임이 가장 큰 고객의 런타임으로 제한

### "행잉 청크(hanging chunk)" 문제

- 클러스터의 머신 간 차이 또는 작업에 대한 과다 할당으로 인해 리소스가 할당될 때 발생
- 스트리밍 데이터 정렬과 같은 스트림의 일부 실시간 작업의 어려움으로 인해 발생
- 일반적인 사용자 코드 패턴: 다음 파이프라인 단계로 진행하기 전에 전체 계산이 완료되기를 기다림
- 정렬이 포함될 수 있기 때문에 일반적으로 발생하며, 정렬은 모든 데이터가 진행되어야 함
- 사용 중인 청킹 방법론에 의해 지시된 최악의 경우 성능에 차단되기 때문에 파이프라인 완료 시간을 크게 지연시킬 수 있음

### 대응의 역효과

- 엔지니어나 클러스터 모니터링 인프라가 이 문제를 감지하면 대응이 상황을 악화시킬 수 있음
- 행잉 청크에 대한 "합리적" 또는 "기본" 대응: 즉시 작업을 종료한 다음 작업이 재시작되도록 허용
- 차단이 비결정적 요인의 결과일 수 있기 때문
- 그러나 파이프라인 구현이 설계상 일반적으로 체크포인팅을 포함하지 않기 때문에 모든 청크에 대한 작업이 처음부터 다시 시작
- 결과적으로 이전 주기에 투자한 시간, CPU 사이클 및 인적 노력 낭비

## 분산 환경에서 정기적 파이프라인의 단점

### 배치 작업 스케줄링 메커니즘

- 빅데이터 정기적 파이프라인은 구글에서 광범위하게 사용
- 구글의 클러스터 관리 솔루션에는 이러한 파이프라인을 위한 대체 스케줄링 메커니즘 포함
- 이 메커니즘이 필요한 이유: 연속 실행 파이프라인과 달리 정기적 파이프라인은 일반적으로 낮은 우선순위 배치 작업으로 실행

### 낮은 우선순위 지정의 효과

- 배치 작업이 인터넷 대면 웹 서비스와 같은 방식으로 지연 시간에 민감하지 않기 때문에 잘 작동
- 머신 워크로드를 최대화하여 비용을 제어하기 위해 Borg(구글의 클러스터 관리 시스템)는 사용 가능한 머신에 배치 작업 할당
- 이 우선순위는 시작 지연 시간 저하를 초래할 수 있으므로 파이프라인 작업은 잠재적으로 무기한 시작 지연 경험 가능

### 배치 스케줄링의 한계

- 사용자 대면 웹 서비스 작업이 남긴 공백에서 예약된 작업은 다음 측면에서 영향을 받을 수 있음:
  - 저지연 리소스의 가용성
  - 가격
  - 리소스 액세스의 안정성
- 실행 비용은 요청된 시작 지연에 반비례하고 소비된 리소스에 정비례
- 배치 스케줄러의 과도한 사용은 클러스터 부하가 높을 때 다른 사용자가 배치 리소스를 고갈시켜 작업이 선점(preemption) 위험에 처함
- 위험 균형을 고려할 때 잘 조정된 정기적 파이프라인을 성공적으로 실행하는 것은 높은 리소스 비용과 선점 위험 사이의 미묘한 균형

### 지연 시간 문제

- 매일 실행되는 파이프라인의 경우 최대 몇 시간의 지연이 허용될 수 있음
- 그러나 예약된 실행 빈도가 증가함에 따라 실행 사이의 최소 시간이 빠르게 최소 평균 지연 지점에 도달
- 정기적 파이프라인이 달성할 수 있는 지연 시간에 하한선 설정
- 이 효과적인 하한선 아래로 작업 실행 간격을 줄이면 단순히 진행 증가가 아닌 바람직하지 않은 동작 초래
- 특정 실패 모드는 사용 중인 배치 스케줄링 정책에 따라 다름

### 실패 모드 예시

- 이전 실행이 완료되지 않았기 때문에 각 새 실행이 클러스터 스케줄러에 쌓일 수 있음
- 더 나쁜 경우: 다음 실행이 시작되도록 예약되면 현재 실행 중이고 거의 완료된 실행이 종료될 수 있음
- 실행 증가라는 명목으로 모든 진행을 완전히 중단

### 해결책

- 적절한 작동을 위한 충분한 서버 용량 확보
- 그러나 공유 분산 환경에서 리소스 획득은 수요와 공급의 영향을 받음
- 예상대로 개발 팀은 리소스를 공통 풀에 기여하고 공유해야 할 때 리소스 획득 프로세스를 거치는 것을 꺼림
- 이를 해결하기 위해 배치 스케줄링 리소스와 프로덕션 우선순위 리소스를 구분하여 리소스 획득 비용 합리화 필요

### 모니터링 문제

- 실행 기간이 충분한 파이프라인의 경우 런타임 성능 메트릭에 대한 실시간 정보 보유가 전체 메트릭을 아는 것만큼 중요하거나 더 중요할 수 있음
- 실시간 데이터가 긴급 대응을 포함한 운영 지원 제공에 중요하기 때문
- 실제로 표준 모니터링 모델은 작업 실행 중 메트릭을 수집하고 완료 시에만 메트릭을 보고하는 것을 포함
- 실행 중 작업이 실패하면 통계가 제공되지 않음
- 연속 파이프라인은 작업이 지속적으로 실행되고 원격 측정이 일상적으로 실시간 메트릭을 사용할 수 있도록 설계되어 이러한 문제를 공유하지 않음
- 정기적 파이프라인에는 본질적인 모니터링 문제가 없어야 하지만 강한 연관성 관찰

### "썬더링 허드(Thundering Herd)" 문제

- 실행 및 모니터링 과제에 더해 분산 시스템에 고유한 "썬더링 허드" 문제 추가
- 충분히 큰 정기적 파이프라인이 주어지면 각 사이클마다 잠재적으로 수천 명의 워커가 즉시 작업 시작
- 워커가 너무 많거나 워커가 잘못 구성되었거나 결함이 있는 재시도 논리에 의해 호출되면:
  - 실행되는 서버가 압도됨
  - 기본 공유 클러스터 서비스가 압도됨
  - 사용 중인 모든 네트워킹 인프라도 압도됨

### 재시도 논리의 영향

- 재시도 논리가 구현되지 않은 경우: 실패 시 작업이 삭제될 때 정확성 문제 발생 가능, 작업이 재시도되지 않음
- 재시도 논리가 존재하지만 순진하거나 제대로 구현되지 않은 경우: 실패 시 재시도가 문제를 악화시킬 수 있음

### 인적 개입의 영향

- 제한된 경험을 가진 엔지니어는 작업이 원하는 기간 내에 완료되지 않을 때 파이프라인에 더 많은 워커를 추가하여 이 문제를 증폭시키는 경향
- "썬더링 허드" 문제의 출처에 관계없이, 버그가 있는 10,000개의 워커 파이프라인 작업만큼 클러스터 인프라와 클러스터의 다양한 서비스를 담당하는 SRE에게 어려운 것은 없음

### 모아레 부하 패턴(Moiré Load Pattern)

- 때때로 썬더링 허드 문제가 고립되어 발견하기 명확하지 않을 수 있음
- "모아레 부하 패턴"이라고 부르는 관련 문제: 두 개 이상의 파이프라인이 동시에 실행되고 실행 시퀀스가 때때로 겹쳐 공통 공유 리소스를 동시에 소비할 때 발생
- 이 문제는 연속 파이프라인에서도 발생할 수 있지만 부하가 더 고르게 도착할 때는 덜 일반적
- 모아레 부하 패턴은 공유 리소스의 파이프라인 사용 플롯에서 가장 명확
- 예시: 세 개의 정기적 파이프라인의 리소스 사용 식별 가능
- 스택 버전에서 온콜 통증을 유발하는 피크 영향은 집계 부하가 1.2M에 가까워질 때 발생

## 구글 워크플로우 소개

### 개발 배경

- 본질적으로 일회성 배치 파이프라인이 지속적으로 업데이트된 결과에 대한 비즈니스 요구로 인해 압도당할 때 발생
- 파이프라인 개발 팀은 일반적으로 다음을 고려:
  - 현재 요구를 충족하기 위해 원래 설계를 리팩토링
  - 연속 파이프라인 모델로 이동
- 불행히도 비즈니스 요구는 일반적으로 파이프라인 시스템을 온라인 연속 처리 시스템으로 리팩토링하기에 가장 불편한 시기에 발생
- 확장 문제를 강제하는 더 새롭고 더 큰 고객은 일반적으로 새로운 기능 포함을 원하고 이러한 요구사항이 움직일 수 없는 마감일을 준수하기를 기대

### 설계 시 고려사항

- 제안된 데이터 파이프라인을 포함하는 시스템 설계 초기에 다음 세부 사항을 확인하는 것이 중요:
  - 예상 성장 궤적
  - 설계 수정에 대한 요구
  - 예상 추가 리소스
  - 비즈니스의 예상 지연 시간 요구사항

### 워크플로우 시스템 개발

- 이러한 요구에 직면하여 구글은 2003년에 "Workflow"라는 시스템 개발
- Workflow는 대규모로 연속 처리를 가능하게 함
- 리더-팔로워(워커) 분산 시스템 디자인 패턴 사용
- 시스템 프리밸런스(system prevalence) 디자인 패턴 사용
- 이 조합을 통해 매우 대규모 트랜잭션 데이터 파이프라인 가능
- 정확히 한 번(exactly-once) 의미론으로 정확성 보장

### MVC 패턴으로서의 워크플로우

- 시스템 프리밸런스 작동 방식으로 인해 Workflow를 사용자 인터페이스 개발에서 알려진 모델-뷰-컨트롤러 패턴의 분산 시스템 동등물로 생각하는 것이 유용할 수 있음
- 이 디자인 패턴은 주어진 소프트웨어 애플리케이션을 세 개의 상호 연결된 부분으로 나누어 정보의 내부 표현을 정보가 사용자에게 제시되거나 사용자로부터 수용되는 방식과 분리

### MVC 패턴의 워크플로우 적용

**모델(Model)**

- "Task Master"라는 서버에 보관
- Task Master는 시스템 프리밸런스 패턴을 사용하여 빠른 가용성을 위해 모든 작업 상태를 메모리에 보관
- 동시에 영구 디스크에 변경 사항을 동기적으로 저널링

**뷰(View)**

- 파이프라인의 하위 구성 요소로서의 관점에 따라 마스터와 트랜잭션 방식으로 시스템 상태를 지속적으로 업데이트하는 워커
- 모든 파이프라인 데이터가 Task Master에 저장될 수 있지만 최상의 성능은 일반적으로 작업에 대한 포인터만 Task Master에 저장되고 실제 입력 및 출력 데이터는 공통 파일시스템 또는 기타 스토리지에 저장될 때 달성
- 이 비유를 지원하여 워커는 완전히 무상태이며 언제든지 폐기 가능

**컨트롤러(Controller)**

- 선택적으로 세 번째 시스템 구성 요소로 추가 가능
- 파이프라인에 영향을 미치는 여러 보조 시스템 활동을 효율적으로 지원:
  - 파이프라인의 런타임 확장
  - 스냅샷
  - 작업 주기 상태 제어
  - 파이프라인 상태 롤백
  - 비즈니스 연속성을 위한 글로벌 차단 수행

## 워크플로우의 실행 단계들

### 파이프라인 깊이 증가

- Task Master에 보관된 작업 그룹으로 처리를 세분화하여 Workflow 내부에서 파이프라인 깊이를 임의의 수준으로 증가 가능
- 각 작업 그룹은 일부 데이터에 대해 임의의 작업을 수행할 수 있는 파이프라인 단계에 해당하는 작업 보유
- 모든 단계에서 매핑, 셔플링, 정렬, 분할, 병합 또는 기타 작업을 수행하는 것이 비교적 간단

### 워커 유형 및 스케줄링

- 단계에는 일반적으로 연관된 워커 유형이 있음
- 주어진 워커 유형의 여러 동시 인스턴스 가능
- 워커는 다양한 유형의 작업을 찾고 수행할 유형을 선택할 수 있다는 점에서 자체 스케줄링 가능

### 작업 처리 과정

- 워커는 이전 단계의 작업 단위를 소비하고 출력 단위 생성
- 출력은 종료점이거나 다른 처리 단계의 입력일 수 있음
- 시스템 내에서 모든 작업이 실행되거나 최소한 영구 상태에 반영되도록 정확히 한 번 보장하는 것이 쉬움

### 정확성 보장

**첫 번째 보장: 이중 정확성**

- Task Master 내부에 파이프라인 상태의 모든 세부 사항을 저장하는 것은 실용적이지 않음 (Task Master가 RAM 크기로 제한되기 때문)
- 그러나 이중 정확성 보장이 지속:
  - 마스터가 고유하게 명명된 데이터에 대한 포인터 모음 보유
  - 각 작업 단위가 고유하게 보유된 리스(lease) 보유
- 워커는 리스로 작업을 획득하고 현재 유효한 리스를 소유한 작업에서만 작업을 커밋 가능

**고아 워커(orphaned worker) 문제 해결**

- 고아 워커가 작업 단위에서 계속 작업하여 현재 워커의 작업을 파괴하는 상황 방지
- 워커가 여는 각 출력 파일에 고유한 이름 부여
- 이러한 방식으로 고아 워커도 커밋을 시도할 때까지 마스터와 독립적으로 계속 쓰기 가능
- 커밋을 시도하면 다른 워커가 해당 작업 단위에 대한 리스를 보유하고 있기 때문에 커밋 불가
- 고유한 파일 이름 체계로 모든 워커가 별개의 파일에 쓰고 있음을 보장하므로 고아 워커가 유효한 워커가 생성한 작업을 파괴할 수 없음
- 이러한 방식으로 이중 정확성 보장 유지: 출력 파일은 항상 고유하고 파이프라인 상태는 리스가 있는 작업 덕분에 항상 정확

**두 번째 보장: 작업 버전 관리**

- 이중 정확성 보장으로도 충분하지 않은 것처럼 Workflow는 모든 작업의 버전도 관리
- 작업 업데이트 또는 작업 리스 변경 시 각 작업은 새로운 고유 작업을 생성하여 이전 작업을 대체하고 새 ID가 작업에 할당
- Workflow의 모든 파이프라인 구성이 작업 단위 자체와 동일한 형식으로 Task Master 내부에 저장되기 때문에 작업을 커밋하려면:
  - 워커가 활성 리스를 소유해야 함
  - 결과를 생성하는 데 사용한 구성의 작업 ID 번호를 참조해야 함
- 작업 단위가 진행 중일 때 구성이 변경되면 해당 유형의 모든 워커가 현재 리스를 소유하고 있음에도 불구하고 커밋 불가
- 따라서 구성 변경 후 수행된 모든 작업은 새 구성과 일치하며, 이전 리스를 보유하기에 불운한 워커가 수행한 작업이 버려지는 비용 발생

**세 번째 보장: 삼중 정확성**

- 이러한 조치는 삼중 정확성 보장 제공:
  - 구성
  - 리스 소유권
  - 파일 이름 고유성
- 그러나 이것조차도 모든 경우에 충분하지 않음
  **네 번째 보장: 서버 토큰** (계속)

  - 더 일반적으로 누군가가 독립적인 Task Master 세트 앞에 로드 밸런서를 삽입하여 Task Master 설정을 (잘못) 구성한 경우

- Workflow는 각 작업의 메타데이터에 서버 토큰(이 특정 Task Master의 고유 식별자)을 포함하여 비정상 또는 잘못 구성된 Task Master가 파이프라인을 손상시키는 것을 방지
- 클라이언트와 서버 모두 각 작업에서 토큰을 확인하여 작업 식별자 충돌이 발생할 때까지 모든 작업이 원활하게 실행되는 매우 미묘한 잘못된 구성 방지

**네 가지 워크플로우 정확성 보장 요약**

1. 구성 작업을 통한 워커 출력이 작업을 예측하는 장벽 생성
2. 커밋된 모든 작업은 워커가 보유한 현재 유효한 리스 필요
3. 출력 파일은 워커에 의해 고유하게 명명
4. 클라이언트와 서버가 모든 작업에서 서버 토큰을 확인하여 Task Master 자체를 검증

### 데이터베이스 사용과의 비교

- 특수 Task Master를 포기하고 Spanner 또는 다른 데이터베이스를 사용하는 것이 더 간단할 수 있다고 생각할 수 있음
- 그러나 Workflow는 각 작업이 고유하고 불변이기 때문에 특별함
- 이러한 쌍둥이 속성은 광범위한 작업 분산에서 발생할 수 있는 많은 잠재적으로 미묘한 문제 방지

**데이터베이스 직접 사용의 비효율성**

- 예시: 워커가 획득한 리스는 작업 자체의 일부이므로 리스 변경에도 새로운 작업 필요
- 데이터베이스를 직접 사용하고 트랜잭션 로그가 "저널"처럼 작동하는 경우:
  - 모든 읽기가 장기 실행 트랜잭션의 일부여야 함
  - 이 구성은 확실히 가능하지만 매우 비효율적

## 비즈니스의 지속성 보장하기

### 빅데이터 파이프라인의 연속성 요구

- 빅데이터 파이프라인은 다음을 포함한 모든 유형의 장애에도 불구하고 처리를 계속해야 함:
  - 광섬유 절단
  - 기상 이벤트
  - 연쇄 전력망 장애
- 이러한 유형의 장애는 전체 데이터센터를 비활성화할 수 있음
- 작업 완료에 대한 강력한 보장을 얻기 위해 시스템 프리밸런스를 사용하지 않는 파이프라인은 종종 비활성화되고 정의되지 않은 상태로 진입
- 이러한 아키텍처 격차는 취약한 비즈니스 연속성 전략을 만들고 파이프라인과 데이터를 복원하기 위한 비용이 많이 드는 대량 노력 복제를 수반

### Workflow의 해결책

- Workflow는 연속 처리 파이프라인에 대해 이 문제를 결정적으로 해결
- 글로벌 일관성을 얻기 위해 Task Master는 Spanner에 저널을 저장하여 글로벌하게 사용 가능하고 일관성 있지만 낮은 처리량의 파일시스템으로 사용

### Task Master 선출 메커니즘

- 어떤 Task Master가 쓸 수 있는지 결정하기 위해:
  - 각 Task Master는 Chubby라는 분산 잠금 서비스를 사용하여 작성자 선출
  - 결과는 Spanner에 유지
- 클라이언트는 내부 네이밍 서비스를 사용하여 현재 Task Master 조회

### 글로벌 분산 Workflow의 구조

- Spanner는 높은 처리량의 파일시스템이 아니기 때문에 글로벌 분산 Workflow는 다음을 사용:
  - 별개의 클러스터에서 실행되는 두 개 이상의 로컬 Workflow
  - 글로벌 Workflow에 저장된 참조 작업 개념

### 작업 처리 흐름

- 작업 단위(작업)가 파이프라인을 통해 소비되면:
  - "stage 1"로 표시된 바이너리에 의해 동등한 참조 작업이 글로벌 Workflow에 삽입
- 작업이 완료되면:
  - "stage n"에 묘사된 대로 참조 작업이 글로벌 Workflow에서 트랜잭션 방식으로 제거
- 글로벌 Workflow에서 작업을 제거할 수 없는 경우:
  - 로컬 Workflow는 글로벌 Workflow가 다시 사용 가능해질 때까지 차단
  - 트랜잭션 정확성 보장

### 자동 장애 조치(Failover)

- 장애 조치를 자동화하기 위해:
  - "stage 1"로 표시된 헬퍼 바이너리가 각 로컬 Workflow 내부에서 실행
- 로컬 Workflow는 "작업 수행" 상자에서 설명한 대로 달리 변경되지 않음

**헬퍼 바이너리의 역할**

- MVC 의미에서 "컨트롤러" 역할 수행
- 다음을 담당:
  - 참조 작업 생성
  - 글로벌 Workflow 내부의 특수 하트비트 작업 업데이트

**장애 조치 프로세스**

- 하트비트 작업이 타임아웃 기간 내에 업데이트되지 않으면:
  - 원격 Workflow의 헬퍼 바이너리가 참조 작업에 의해 문서화된 진행 중인 작업을 점유
  - 환경이 작업에 무엇을 하든 방해받지 않고 파이프라인 계속 진행

## 요약 및 결론

### 정기적 파이프라인의 가치와 한계

- 정기적 파이프라인은 가치가 있음
- 그러나 데이터 처리 문제가 연속적이거나 유기적으로 성장하여 연속적이 될 경우:
  - 정기적 파이프라인을 사용하지 말 것
  - 대신 Workflow와 유사한 특성을 가진 기술 사용

### Workflow의 장점

- Workflow가 제공하는 강력한 보장을 통한 연속 데이터 처리:
  - 분산 클러스터 인프라에서 성능이 우수하고 확장성이 좋음
  - 사용자가 신뢰할 수 있는 결과를 일상적으로 생성
  - 사이트 신뢰성 엔지니어링 팀이 관리 및 유지보수하기에 안정적이고 신뢰할 수 있는 시스템

### 핵심 교훈

- 데이터 처리 파이프라인은 깊이와 복잡성 관리에 실제적인 과제 존재
- 정기적 파이프라인은 초기에는 안정적이지만 유기적 성장으로 인한 취약성 내재
- 불균등한 작업 분산과 행잉 청크 문제는 파이프라인 성능에 심각한 영향
- 분산 환경에서 배치 스케줄링은 지연 시간과 리소스 제약의 균형 필요
- 썬더링 허드와 모아레 부하 패턴은 클러스터 인프라에 심각한 부담
- Workflow의 리더-팔로워 모델과 시스템 프리밸런스 패턴은 대규모 연속 처리에 효과적인 대안 제공
- 다중 정확성 보장(구성, 리스, 파일명, 서버 토큰)을 통해 분산 환경에서도 정확한 작업 실행 보장
- 글로벌 분산 Workflow와 자동 장애 조치를 통해 비즈니스 연속성 확보

# 26장 데이터 무결성: 내가 기록한 그대로 읽을 수 있어야 한다

## 1. 데이터 무결성의 중요한 조건

### 1.1 데이터 무결성의 정의

- **사용자 중심 정의**: 사용자가 생각하는 것이 곧 데이터 무결성
- **기술적 정의**: 사용자에게 적절한 수준의 서비스를 제공하는 데 필요한 데이터 저장소의 접근성과 정확성을 측정하는 지표
- **실질적 의미**: 클라우드 서비스가 사용자에게 접근 가능한 상태로 유지되며, 데이터 접근이 완벽한 상태를 유지하는 것

### 1.2 데이터 무결성의 엄격한 요구사항

#### 가용성 vs 데이터 무결성 비교

- **가용성 SLO**: 99.99% = 연간 약 1시간의 다운타임 허용
  - 대부분의 인터넷 및 엔터프라이즈 사용자의 기대치를 초과하는 높은 기준
- **데이터 무결성 SLO**: 99.99%의 정확도
  - 2GB 파일에서 최대 200KB의 손상 의미
  - 실행 파일의 무작위 opcode, 완전히 로드 불가능한 데이터베이스 등 치명적 결과 초래
  - 대부분의 경우 재앙적인 수준

#### 사용자 관점의 독립적 요구사항

- 모든 서비스는 독립적인 가용성 및 데이터 무결성 요구사항 보유
- 이러한 요구사항은 암묵적일 수 있음
- 데이터 소멸 후에 요구사항에 대해 논의하는 것이 최악의 시나리오

### 1.3 데이터 무결성의 재정의

- **핵심 원칙**: 클라우드 서비스가 사용자에게 계속 접근 가능해야 함
- **사용자 데이터 접근의 중요성**: 특히 완벽한 상태 유지 필수

#### 시나리오 분석

- **시나리오 1 - 복구 불가능한 손실**:
  - 연 1회 손상/손실 발생 시, 해당 연도의 가용성 손실
  - 유일한 해결책: 사전 탐지 + 신속한 복구
- **시나리오 2 - 사전 탐지 및 복구**:
  - 사용자 영향 전 손상 즉시 탐지
  - 30분 내 제거, 수정, 서비스 복구
  - 30분의 다운타임만 고려 시: 99.99% 가용성 달성
  - 놀랍게도 객체의 접근 가능 수명 동안 데이터 무결성은 100% (또는 거의 100%)

#### 우수한 데이터 무결성의 비밀

**사전 탐지(Proactive Detection) + 신속한 복구(Rapid Repair) + 빠른 회복(Recovery)**

### 1.4 클라우드 환경의 독특한 기술적 도전과제

#### 복잡한 API 조합

- 대부분의 클라우드 애플리케이션은 ACID와 BASE API의 혼합 위에서 지속적으로 진화
  - **ACID**: Atomicity, Consistency, Isolation, Durability (MySQL, PostgreSQL 등)
  - **BASE**: Basically Available, Soft state, Eventual consistency (Bigtable, Megastore 등)
- BASE는 ACID보다 높은 가용성 제공, 대신 분산 일관성 보장이 약함
- BASE는 데이터 업데이트가 중단되면 결국 모든 저장 위치에서 값이 일관되게 됨을 보장

#### 속도(Velocity)가 우선시될 때의 예시

- 개발자에게 가장 익숙한 API의 임의 조합에 의존
- **Blobstore 예시**:
  - 효율적인 BLOB 저장 API 활용
  - 분산 일관성보다 확장성, 높은 가용성, 낮은 지연시간, 저비용 우선
  - 보상책: 소량의 권위 있는 메타데이터를 Paxos 기반 Megastore에 저장
  - 특정 클라이언트는 메타데이터를 로컬 캐시하고 blob에 직접 접근하여 지연시간 단축
  - 다른 애플리케이션은 개발자 친숙도로 인해 메타데이터를 Bigtable에 보관

#### 런타임 데이터 무결성 도전과제

- **참조 무결성 문제**: 데이터 저장소 간 (Blobstore, Megastore, 클라이언트 측 캐시)
- **고속 개발의 부작용**:
  - 스키마 변경
  - 데이터 마이그레이션
  - 구 기능 위에 신 기능 적층
  - 재작성
  - 다른 애플리케이션과의 통합 지점 진화
- **결과**: 어떤 단일 엔지니어도 완전히 이해하지 못하는 복잡한 데이터 관계망 생성

#### 필요한 해결책

- 데이터 저장소 내부 및 사이의 out-of-band 검사 및 균형 시스템
- 독립적이고 조정되지 않은 여러 데이터 저장소 백업 사용 시 복구 복잡성 증가

### 1.5 우수한 데이터 무결성을 위한 전략 선택

#### 클라우드 컴퓨팅의 최적화 요소 5가지

1. **Uptime (가용성)**: 서비스를 사용자가 사용할 수 있는 시간의 비율
2. **Latency (지연시간)**: 서비스가 사용자에게 얼마나 빠르게 응답하는지
3. **Scale (규모)**: 지연시간 악화 또는 서비스 붕괴 전까지 처리 가능한 사용자 볼륨과 워크로드 혼합
4. **Velocity (속도)**: 합리적 비용으로 사용자에게 우수한 가치를 제공하기 위한 혁신 속도
5. **Privacy (개인정보보호)**: 사용자가 삭제한 데이터를 합리적 시간 내에 파기

#### 클라우드 환경의 기술적 도전과제

- 트랜잭션 및 비트랜잭션 백업/복원 솔루션 혼합 시 복구 데이터가 정확하지 않을 수 있음
- 유지보수 중단 없이 서비스 진화 시 다른 버전의 비즈니스 로직이 데이터에 병렬 작용
- 독립적으로 버전 관리되는 서비스 간 상호작용 시 일시적으로 호환되지 않는 버전이 상호작용하여 우발적 손상/손실 위험 증가

#### API 설계 요구사항

서비스 제공자는 규모의 경제 유지를 위해 제한된 수의 API만 제공해야 하며, 이 API들은:

- 대다수 애플리케이션에 간단하고 사용하기 쉬워야 함
- 동시에 다음을 이해할 만큼 견고해야 함:
  - 데이터 지역성 및 캐싱
  - 로컬 및 글로벌 데이터 분산
  - 강한 일관성 및/또는 최종 일관성
  - 데이터 내구성, 백업, 복구

## 2. 데이터 무결성과 가용성을 유지하기 위한 구글 SRE의 목표

### 2.1 측정 가능한 구체적 목표

#### 비전과 지표

- **비전**: "영구 데이터의 무결성 유지"
- **실행 방식**: 측정 가능한 지표가 있는 구체적 목표 설정
- **핵심 지표 정의**: 시스템과 프로세스의 기능에 대한 기대치 설정 및 실제 이벤트 중 성능 추적

### 2.2 데이터 무결성은 수단, 데이터 가용성이 목표

#### 데이터 무결성의 의미

- 데이터의 전체 수명 동안 정확성과 일관성 유지
- 사용자는 처음 기록된 시점부터 마지막 관찰 시점까지 정보가 정확하고 예상치 못한 변경이 없음을 알아야 함

#### 실제 사례: 이메일 제공업체의 10일 간 데이터 중단

- **초기 대응**: 사용자들이 임시 방법으로 업무 처리, 곧 복구될 것으로 기대
- **최악의 소식**: 과거 이메일과 연락처가 사라졌으며 영구적으로 복구 불가
- **원인**: 데이터 무결성 관리의 일련의 실수로 사용 가능한 백업 없음
- **사용자 반응**: 분노한 사용자들이 임시 계정 유지하거나 새 계정 생성, 이전 이메일 제공업체 포기
- **반전**: 며칠 후 사용자 개인정보 복구 가능하다고 발표
- **실제 결과**: 데이터 손실은 없었지만 데이터가 필요한 사람들이 너무 오랫동안 접근 불가

#### 교훈

**사용자 관점에서 기대되는 정기적인 데이터 접근 없는 데이터 무결성 = 데이터 없음과 동일**

### 2.3 백업 시스템이 아닌 복구 시스템 제공

#### 백업의 전통적 문제점

- 전통적으로 소홀히 취급되고 위임되며 연기되는 시스템 관리 작업
- 누구에게도 높은 우선순위가 아님
- 시간과 리소스를 지속적으로 소모하지만 즉각적이고 가시적인 이익 없음
- 백업 전략 구현의 부실함에 대해 동정적인 시선으로 바라봄
- 일부는 이를 실용적 태도로 주장

#### 근본적 문제

- 위험은 낮지만 영향은 크다 (low-risk but high-impact)
- 서비스 데이터 접근 불가 시 대응이 서비스, 제품, 심지어 회사를 좌우할 수 있음

#### 구글의 접근 방식: 복구에 집중

- **동기 부여 전환**: 백업이라는 감사받지 못하는 작업 대신 가시적 성과가 있는 복구에 집중
- **세금 비유**: 백업은 보장된 데이터 가용성이라는 공공 서비스를 위해 지속적으로 지불하는 세금
- **강조점 전환**: 세금보다는 세금이 제공하는 서비스(데이터 가용성)에 주목

#### 실행 방법

팀들이 백업을 "연습"하도록 하는 대신:

1. **SLO 정의**: 다양한 장애 모드에서 데이터 가용성에 대한 서비스 수준 목표 정의
2. **능력 입증**: 팀이 해당 SLO를 충족할 수 있는 능력을 연습하고 시연

## 3. 구글이 데이터 무결성의 문제를 해결하는 방법

### 3.1 백업 대 아카이브

#### 핵심 질문

전통적으로 기업들은 백업 전략에 투자하여 데이터 손실을 "방어"하지만, 실제 초점은 **데이터 복구**여야 함

- **진실**: "아무도 실제로 백업을 만들고 싶어하지 않음; 사람들이 진정 원하는 것은 복원"

#### 백업 vs 아카이브 구분

**질문**: 당신의 "백업"이 실제로는 재해 복구용이 아닌 아카이브인가?

**가장 중요한 차이점**:

- **백업**: 애플리케이션에 다시 로드 가능
- **아카이브**: 애플리케이션에 로드 불가

**사용 사례 차이**:

- **아카이브**:

  - 감사, 발견, 규정 준수 필요를 위해 장기간 데이터 보호
  - 데이터 복구가 서비스 가동 시간 요구사항 내에 완료될 필요 없음
  - 예: 7년간 금융 거래 데이터 보관, 월 1회 장기 아카이브 저장소로 이동, 월간 감사 중 1주일 소요 복구 허용

- **백업**:
  - 재해 발생 시 신속한 데이터 복구 (서비스 가동 시간 요구사항 내)
  - 그렇지 않으면 사용자가 데이터 무결성 문제 시작부터 복구 완료까지 유용한 애플리케이션 접근 불가
  - 가장 최신 데이터가 안전하게 백업되기 전까지 위험에 노출
  - 최적 스케줄: 일일, 시간별 또는 더 자주 (전체/증분 또는 연속 스트리밍 방식)

#### 백업 전략 수립 시 고려사항

1. 문제로부터 얼마나 빨리 복구해야 하는가?
2. 얼마나 많은 최신 데이터 손실을 감당할 수 있는가?

### 3.2 데이터 손실로 이어지는 장애 유형

#### 24가지 데이터 무결성 장애 모드

3가지 요소가 모든 조합으로 발생 가능 → 매우 높은 수준에서 24가지 구별되는 장애 유형

**요소 1: 근본 원인 (Root Cause)**
복구 불가능한 데이터 손실의 원인:

- 사용자 행동 (User action)
- 운영자 실수 (Operator error)
- 애플리케이션 버그 (Application bugs)
- 인프라 결함 (Infrastructure defects)
- 하드웨어 고장 (Faulty hardware)
- 사이트 재해 (Site catastrophes)

**요소 2: 범위 (Scope)**

- **광범위 (Widespread)**: 많은 엔티티에 영향
- **협소 (Narrow and directed)**: 작은 사용자 하위 집합의 특정 데이터 삭제/손상

**요소 3: 속도 (Rate)**

- **빅뱅 이벤트**: 예: 1분 내 100만 행이 10행으로 대체
- **점진적 (Creeping)**: 예: 수주에 걸쳐 매분 10행의 데이터 삭제

#### 효과적인 복원 계획의 필요성

- 이러한 장애 모드가 어떤 조합으로든 발생할 수 있음을 고려한 복원 계획 필요
- 점진적 애플리케이션 버그로 인한 데이터 손실에 완벽하게 효과적인 전략이 데이터센터 화재에는 전혀 도움이 안 될 수 있음

#### 구글의 19건 데이터 복구 연구 결과

**가장 흔한 사용자 가시적 데이터 손실 시나리오**:

- 소프트웨어 버그로 인한 데이터 삭제 또는 참조 무결성 손실
- **가장 어려운 변형**: 버그가 프로덕션 환경에 처음 릴리스된 후 수주~수개월 후 발견되는 저등급 손상이나 삭제

**필요한 보호장치**:

- 이러한 유형의 손실 방지 또는 복구에 적합한 보호장치
- 대규모 성공적 애플리케이션의 복구 요구사항:
  - 수백만 사용자의 데이터를 며칠, 수주, 수개월에 걸쳐 검색
  - 각 영향받은 객체를 고유한 시점으로 복구
- 이를 **"Point-in-time Recovery"** (구글 외부) 또는 **"Time-travel"** (구글 내부)라고 함

#### Point-in-time Recovery의 현실

**현재 상황**: ACID 및 BASE 데이터 저장소 전반에 걸쳐 엄격한 가동 시간, 지연시간, 확장성, 속도, 비용 목표를 충족하면서 point-in-time 복구를 제공하는 백업 및 복구 솔루션은 **오늘날 존재하지 않음 (chimera)**

**절충안**:

- 자체 엔지니어로 문제 해결 시 속도(velocity) 희생 필요
- 많은 프로젝트가 point-in-time 복구 없는 계층화된 백업 전략으로 타협

**계층화된 전략 예시**:

1. **로컬 "스냅샷"**:
   - 애플리케이션 버그로부터 제한적 보호 제공
   - 빠른 복원 기능
   - 수일간 보존, 수 시간 간격으로 생성
2. **전체 및 증분 복사본**:
   - 비용 효율적
   - 2일마다 생성
   - 더 오래 보존

**권장사항**:

- 사용하려는 클라우드 API가 제공하는 데이터 복구 옵션 고려
- 필요 시 point-in-time 복구를 계층화 전략과 교환하되, 둘 다 사용하지 않는 것은 피할 것
- 두 기능 모두 사용 가능하면 둘 다 사용 (각 기능이 어느 시점에는 가치 있을 것)

### 3.3 깊고 넓게 데이터 무결성 유지의 도전과제

#### 핵심 인식

**복제와 중복성은 복구 가능성이 아니다**

#### 확장 문제: 전체 백업, 증분 백업, 그리고 백업과 복원의 경쟁력

**잘못된 대응의 예**:

- 질문: "백업이 있나요?"
- 결함 있는 답변: "백업보다 더 나은 것이 있습니다—복제입니다!"

**복제의 한계**:

- 복제는 많은 이점 제공 (데이터 지역성, 사이트별 재해 보호)
- 하지만 많은 데이터 손실 원인으로부터 보호 불가
- 자동으로 여러 복제본을 동기화하는 데이터 저장소는 손상된 데이터베이스 행이나 잘못된 삭제를 문제 격리 전에 모든 복사본으로 푸시

**추가 보호 조치의 문제점**:

- 비서비스 데이터 복사본을 다른 형식으로 생성 (예: 데이터베이스를 네이티브 파일로 자주 내보내기)
- 복제가 보호하지 못하는 오류(사용자 실수, 애플리케이션 계층 버그)로부터 보호 추가
- 하지만 하위 계층에서 발생한 손실로부터는 보호하지 못함
- 추가 위험 요소:
  - 데이터 변환 중 버그 (양방향)
  - 네이티브 파일 저장 중 버그
  - 두 형식 간 의미론적 불일치

**제로데이 공격 시나리오**:

- 파일시스템이나 장치 드라이버 같은 스택의 낮은 수준에서 제로데이 공격 발생
- 손상된 소프트웨어 구성 요소에 의존하는 모든 복사본(데이터베이스를 지원하는 동일한 파일시스템에 작성된 데이터베이스 내보내기 포함) 취약

**핵심 원칙: 다양성이 핵심**

- X 계층의 장애 방지를 위해 해당 계층의 다양한 구성 요소에 데이터 저장 필요
- **미디어 격리**: 미디어 결함으로부터 보호
  - 디스크 장치 드라이버의 버그나 공격이 테이프 드라이브에 영향을 미칠 가능성 낮음
  - 가능하다면 점토판에 귀중한 데이터의 백업 복사본 만들고 싶을 정도

**경쟁하는 힘: 데이터 신선도 vs 복원 완료**
스택 아래로 스냅샷을 푸시할수록:

- 복사본 생성 시간 증가
- 복사본 빈도 감소
- 예시:
  - **데이터베이스 수준**: 트랜잭션 복제에 수초
  - **파일시스템으로 스냅샷 내보내기**: 40분 소요
  - **기본 파일시스템의 전체 백업**: 수 시간 소요

**복원 시의 문제**:

- 최신 스냅샷 복원 시 최대 40분의 최신 데이터 손실 가능
- 파일시스템 백업에서 복원 시 수 시간의 트랜잭션 누락 가능
- 복원 자체도 백업만큼 시간 소요 → 실제 데이터 로드에 수 시간
- **딜레마**: 가장 신선한 데이터를 가능한 빨리 복구하고 싶지만, 장애 유형에 따라 가장 신선하고 즉시 사용 가능한 복사본이 선택지가 아닐 수 있음

#### 보존 기간 (Retention)

데이터 복구 계획에서 고려해야 할 또 다른 요소

**시나리오 차이**:

- **급격한 손실**: 전체 데이터베이스가 갑자기 비워지면 빠르게 인지 가능
- **점진적 손실**: 더 점진적인 데이터 손실은 적절한 사람의 주의를 끌기까지 며칠 소요 가능

**점진적 손실의 복원 요구사항**:

- 더 과거 시점의 스냅샷 필요
- 과거로 거슬러 올라갈 때 복원된 데이터를 현재 상태와 병합하고 싶을 가능성
- 복원 프로세스 대폭 복잡화

### 3.4 다층 방어 전략 (Defense in Depth)

#### 24가지 데이터 무결성 장애 모드 조합

주어진 많은 데이터 손실 방식을 고려할 때, 여러 장애 모드 조합을 방어하는 만병통치약은 없음
→ **다층 방어(Defense in Depth)** 필요

**다층 방어의 구성**:

- 여러 계층으로 구성
- 각각의 연속적인 방어 계층이 점진적으로 덜 흔한 데이터 손실 시나리오로부터 보호 제공

**객체의 여정**: 소프트 삭제부터 파괴까지

1. **제1계층**: 소프트 삭제 (개발자 API 제공의 경우 "lazy deletion")
   - 우발적 데이터 삭제 시나리오에 대한 효과적 방어로 입증
2. **제2계층**: 백업 및 관련 복구 방법
3. **제3계층**: 정기적인 데이터 검증

**모든 계층에 걸친 추가 요소**:

- 복제(replication)의 존재가 특정 시나리오에서 데이터 복구에 가끔 유용
- 단, 데이터 복구 계획이 복제에만 의존해서는 안 됨

#### 제1계층: 소프트 삭제 (Soft Deletion)

**배경 및 필요성**:

- 속도가 빠르고 개인정보가 중요한 환경에서 애플리케이션의 버그가 대부분의 데이터 손실 및 손상 이벤트 원인
- 실제로 데이터 삭제 버그가 너무 흔해져서 제한된 시간 동안 데이터를 삭제 취소하는 기능이 대부분의 영구적이고 우발적인 데이터 손실에 대한 주요 방어선이 됨

**개인정보 보호와의 균형**:

- 사용자 개인정보를 지키는 제품은 사용자가 선택한 데이터 하위 집합 및/또는 전체 데이터 삭제 허용 필요
- 이러한 제품은 우발적 삭제로 인한 지원 부담 발생
- 사용자에게 데이터 삭제 취소 기능 제공(예: 휴지통 폴더)은 지원 부담 감소하지만 완전히 제거하지는 못함
  - 특히 서비스가 데이터를 삭제할 수 있는 서드파티 애드온도 지원하는 경우

# 사이트 신뢰성 엔지니어링 26장: 데이터 무결성 - 내가 기록한 그대로 읽을 수 있어야 한다

## 데이터 무결성 방어 계층 구조

- 여러 계층으로 구성
- 각각의 연속적인 방어 계층이 점진적으로 덜 흔한 데이터 손실 시나리오로부터 보호 제공

### 객체의 여정: 소프트 삭제부터 파괴까지

1. **제1계층**: 소프트 삭제 (개발자 API 제공의 경우 "lazy deletion")
   - 우발적 데이터 삭제 시나리오에 대한 효과적 방어로 입증
2. **제2계층**: 백업 및 관련 복구 방법
3. **제3계층**: 정기적인 데이터 검증

**모든 계층에 걸친 추가 요소**:

- 복제(replication)의 존재가 특정 시나리오에서 데이터 복구에 가끔 유용
- 단, 데이터 복구 계획이 복제에만 의존해서는 안 됨

## 제1계층: 소프트 삭제 (Soft Deletion)

### 배경 및 필요성

- 속도가 빠르고 개인정보가 중요한 환경에서 애플리케이션의 버그가 대부분의 데이터 손실 및 손상 이벤트 원인
- 실제로 데이터 삭제 버그가 너무 흔해져서 제한된 시간 동안 데이터를 삭제 취소하는 기능이 대부분의 영구적이고 우발적인 데이터 손실에 대한 주요 방어선이 됨

### 개인정보 보호와의 균형

- 사용자 개인정보를 지키는 제품은 사용자가 선택한 데이터 하위 집합 및/또는 전체 데이터 삭제 허용 필요
- 이러한 제품은 우발적 삭제로 인한 지원 부담 발생
- 사용자에게 데이터 삭제 취소 기능 제공(예: 휴지통 폴더)은 지원 부담 감소하지만 완전히 제거하지는 못함
  - 특히 서비스가 데이터를 삭제할 수 있는 서드파티 애드온도 지원하는 경우

### 소프트 삭제의 작동 방식

- 삭제된 데이터는 즉시 표시되어 애플리케이션의 관리 코드 경로를 제외한 모든 곳에서 사용 불가능하게 됨
- 관리 코드 경로 포함 항목:
  - 법적 발견(Legal discovery)
  - 계정 해킹 복구
  - 기업 관리
  - 사용자 지원
  - 문제 해결 및 관련 기능
- 사용자가 휴지통을 비울 때 소프트 삭제 수행
- 승인된 관리자가 실수로 삭제한 항목을 복원할 수 있는 사용자 지원 도구 제공

### 구글의 소프트 삭제 구현 사례

- 구글은 가장 인기 있는 생산성 애플리케이션에 이 전략 구현
- 그렇지 않으면 사용자 지원 엔지니어링 부담이 감당 불가능
- Gmail 휴지통: 30일 이전에 삭제된 메시지에 대한 사용자 액세스 허용

### 계정 해킹과 소프트 삭제

- 계정 해킹은 또 다른 일반적인 원치 않는 데이터 삭제 원인
- 해커는 일반적으로 스팸 및 기타 불법 목적으로 계정을 사용하기 전에 원래 사용자의 데이터 삭제
- 우발적 사용자 삭제의 빈도와 해커의 데이터 삭제 위험을 결합하면, 애플리케이션 내부 및/또는 하위에 프로그래밍 방식의 소프트 삭제 및 복원 취소 인터페이스 필요성이 명확해짐

### 소프트 삭제 지연 기간

- 데이터가 표시된 후 적절한 지연 후 파괴됨
- 지연 기간은 다음에 따라 결정:
  - 조직의 정책 및 적용 법률
  - 사용 가능한 스토리지 리소스 및 비용
  - 제품 가격 책정 및 시장 포지셔닝(특히 수명이 짧은 데이터가 많은 경우)
- 일반적인 선택: 15, 30, 45, 또는 60일
- 구글의 경험:
  - 대부분의 계정 해킹 및 데이터 무결성 문제는 60일 이내에 보고 또는 감지
  - 따라서 60일 이상 소프트 삭제 데이터를 유지하는 경우가 강하지 않을 수 있음

### 가장 심각한 데이터 삭제 사례

- 가장 치명적인 급성 데이터 삭제 사례는 기존 코드에 익숙하지 않지만 삭제 관련 코드를 작업하는 애플리케이션 개발자가 원인
- 특히 배치 처리 파이프라인(예: 오프라인 MapReduce 또는 Hadoop 파이프라인)에서 발생
- 익숙하지 않은 개발자가 새 코드로 소프트 삭제 기능을 우회하지 못하도록 인터페이스 설계가 유리
- 효과적인 방법: 내장 소프트 삭제 및 복원 취소 API를 포함하는 클라우드 컴퓨팅 제품 구현 및 해당 기능 활성화

### 클라우드 컴퓨팅 제품에서의 소프트 삭제

- 클라우드 컴퓨팅 제품이 합리적인 기본값으로 프로그래밍 방식의 소프트 삭제 및 복원 취소 기능을 이미 지원한다고 가정하면, 나머지 우발적 데이터 삭제 시나리오는 자체 내부 개발자 또는 개발자 고객의 실수에서 비롯됨

### Lazy Deletion (게으른 삭제)

- 추가 소프트 삭제 계층 도입 가능
- "Lazy deletion"은 스토리지 시스템이 제어하는 백그라운드 제거(소프트 삭제는 클라이언트 애플리케이션 또는 서비스가 제어)
- Lazy deletion 시나리오:
  - 클라우드 애플리케이션에 의해 삭제된 데이터는 애플리케이션에 즉시 액세스 불가능해짐
  - 하지만 파괴 전 최대 몇 주 동안 클라우드 서비스 제공자에 의해 보존됨
- Lazy deletion이 모든 심층 방어 전략에서 권장되지 않는 경우:
  - 수명이 짧은 데이터가 많은 시스템에서는 긴 lazy deletion 기간이 비용이 많이 듦
  - 합리적인 시간 내에 삭제된 데이터의 파괴를 보장해야 하는 시스템(즉, 개인정보 보호 보장을 제공하는 시스템)에서는 비실용적

### 소프트 삭제 계층 요약

1. **휴지통 폴더**: 사용자가 데이터를 복원 취소할 수 있도록 하는 것이 사용자 오류에 대한 주요 방어
2. **소프트 삭제**: 개발자 오류에 대한 주요 방어이자 사용자 오류에 대한 보조 방어
3. **Lazy deletion**: 개발자 제품에서 내부 개발자 오류에 대한 주요 방어이자 외부 개발자 오류에 대한 보조 방어

### 리비전 히스토리와 소프트 삭제

- 일부 제품은 항목을 이전 상태로 되돌릴 수 있는 기능 제공
- 사용자가 이 기능을 사용할 수 있으면 휴지통의 한 형태
- 개발자가 사용할 수 있는 경우, 구현에 따라 소프트 삭제를 대체할 수도 있고 아닐 수도 있음
- 구글에서의 리비전 히스토리:
  - 특정 데이터 손상 시나리오 복구에 유용한 것으로 입증
  - 우발적 삭제와 관련된 대부분의 데이터 손실 시나리오 복구에는 유용하지 않음
- 이유: 일부 리비전 히스토리 구현은 삭제를 특수한 경우로 취급하여 이전 상태를 제거해야 함(항목의 기록을 특정 기간 동안 유지하는 항목 변경과 대조적)
- 원치 않는 삭제에 대한 적절한 보호를 제공하려면 리비전 히스토리에도 lazy 및/또는 소프트 삭제 원칙 적용 필요

## 포괄 계층: 복제 (Replication)

### 복제의 이상적 적용

- 이상적인 세계에서는 백업을 포함한 모든 스토리지 인스턴스가 복제되어야 함
- 데이터 복구 노력 중 마지막으로 발견하고 싶은 것:
  - 백업 자체가 필요한 데이터를 잃어버렸다는 것
  - 가장 유용한 백업을 포함하는 데이터센터가 유지보수 중이라는 것

### 대규모 데이터에서의 복제

- 데이터 양이 증가하면 모든 스토리지 인스턴스 복제가 항상 가능하지는 않음
- 이러한 경우 합리적인 접근:
  - 각각 독립적으로 실패할 수 있는 다른 사이트에 연속적인 백업을 분산
  - RAID, Reed-Solomon 소거 코드 또는 GFS 스타일 복제와 같은 중복성 방법을 사용하여 백업 작성

### 중복성 체계 선택

- 자신의 드문 데이터 복구 시도만이 효능의 유일한 "테스트"인 드물게 사용되는 체계에 의존하지 말 것
- 대신 많은 사용자가 일반적이고 지속적으로 사용하는 인기 있는 체계 선택

## 제2계층: 백업 및 복구

### 1T vs 1E: "단순히" 더 큰 백업이 아님

**테라바이트(T) 규모의 데이터**:

- T(테라바이트)로 측정되는 데이터 볼륨에 적용되는 프로세스 및 관행은 E(엑사바이트)로 측정되는 데이터에 잘 확장되지 않음
- 몇 기가바이트의 구조화된 데이터 검증, 복사 및 왕복 테스트 수행은 흥미로운 문제
- 스키마 및 트랜잭션 모델에 대한 충분한 지식이 있다고 가정하면 특별한 어려움 없음
- 일반적으로 데이터를 반복하고, 일부 검증 논리를 수행하고, 데이터의 몇 개 복사본을 보관할 충분한 스토리지를 위임할 머신 리소스를 조달하기만 하면 됨

**엑사바이트(E) 규모의 데이터**:

- 700페타바이트의 구조화된 데이터를 보호하고 검증하는 경우를 가정
- 이상적인 SATA 2.0 성능인 300 MB/s를 가정하면:
  - 모든 데이터를 반복하고 가장 기본적인 검증 검사를 수행하는 단일 작업에 **8년(80년)** 소요
  - 미디어가 있다고 가정할 때 몇 개의 전체 백업 만들기에 최소 그만큼 시간 소요
  - 일부 사후 처리를 포함한 복원 시간은 더 오래 걸림
- 거의 전체 세기를 보내 최대 80년 된 백업을 복원하는 것은 명백히 재고가 필요한 전략

### 대규모 백업 전략

#### 신뢰 지점(Trust Points) 설정

- 대량의 데이터를 백업하는 데 사용되는 가장 일반적이고 대체로 효과적인 기술
- 데이터에 "신뢰 지점" 설정: 일반적으로 시간 경과에 의해 불변으로 렌더링된 후 검증되는 저장된 데이터의 일부
- 주어진 사용자 프로필 또는 트랜잭션이 고정되어 더 이상 변경되지 않는다는 것을 알게 되면:
  - 내부 상태를 검증할 수 있음
  - 복구 목적으로 적합한 복사본을 만들 수 있음
- 마지막 백업 이후 수정되거나 추가된 데이터만 포함하는 증분 백업 가능
- 이 기술은 백업 시간을 "주류" 처리 시간에 맞춤
  - 빈번한 증분 백업으로 80년의 모놀리식 검증 및 복사 작업에서 벗어날 수 있음

#### 복원 시 고려사항

- 백업이 아닌 복원에 관심이 있음을 기억
- 3년 전에 전체 백업을 수행하고 그 이후 매일 증분 백업을 했다고 가정:
  - 데이터의 전체 복원은 1,000개 이상의 고도로 상호 의존적인 백업 체인을 순차적으로 처리
  - 각 독립적인 백업은 추가 실패 위험을 초래
  - 이러한 작업의 스케줄링 물류 부담 및 런타임 비용 발생

#### 부하 분산

- 복사 및 검증 작업의 벽시계 시간을 줄이는 또 다른 방법은 부하 분산
- 데이터를 잘 샤딩하면 N개의 작업을 병렬로 실행 가능
  - 각 작업은 데이터의 1/N을 복사하고 검증하는 책임
- 다음을 위해 스키마 설계 및 데이터의 물리적 배포에 사전 고려 및 계획 필요:
  - 데이터를 올바르게 균형화
  - 각 샤드의 독립성 보장
  - 동시 형제 작업 간 경합 방지
- 부하를 수평으로 분산하고 시간으로 구분된 데이터의 수직 슬라이스로 작업을 제한하면:
  - 80년의 벽시계 시간을 여러 자릿수 줄일 수 있음
  - 복원을 관련성 있게 만듦

### 백업 전략 최적화

#### 계층형 백업 전략

- 24가지 데이터 무결성 실패 모드 조합을 방어하기 위한 조언 요약:
  - 합리적인 비용으로 광범위한 시나리오를 해결하려면 계층형 백업 전략 필요

**제1계층 백업**:

- 라이브 데이터스토어에 가장 가까이 저장된 많은 빈번하고 빠르게 복원되는 백업으로 구성
- 데이터 소스와 동일하거나 유사한 스토리지 기술 사용 가능
- 소프트웨어 버그 및 개발자 오류와 관련된 대부분의 시나리오로부터 보호

**제2계층 백업**:

- 덜 빈번하지만 더 광범위한 백업
- 다른 스토리지 미디어 또는 위치 사용
- 인프라 결함 및 사이트 재해로부터 보호

#### 백업 보존 기간

- 구글은 많은 서비스에 대해 30~90일 사이의 백업 보존 선택
- 이 범위 내에서 서비스가 속하는 위치는 다음에 따라 결정:
  - 데이터 손실에 대한 허용 범위
  - 조기 감지에 대한 상대적 투자

#### 복제의 한계

- 복제가 일부 시나리오에서 데이터 복구에 유용할 수 있지만, 복제만으로는 충분하지 않음
- 자동으로 여러 복제본을 동기화하는 데이터스토어는 손상된 데이터베이스 행 또는 잘못된 삭제를 모든 복사본에 푸시할 수 있음
  - 문제를 격리하기 전에 발생할 가능성이 높음

### 백업의 다양성

#### 계층별 다양성의 필요성

- 다양성이 핵심: 계층 X에서의 실패로부터 보호하려면 해당 계층의 다양한 구성 요소에 데이터 저장 필요
- **미디어 격리**는 미디어 결함으로부터 보호:
  - 디스크 장치 드라이버의 버그 또는 공격은 테이프 드라이브에 영향을 미칠 가능성이 낮음
  - 가능하다면 귀중한 데이터의 백업 복사본을 점토판에 만들고 싶음(가장 오래된 기록 보존 방법)

#### 백업과 복원의 경쟁 요소

- **데이터 신선도와 복원 완료**가 포괄적 보호와 경쟁:
  - 스택 아래로 데이터 스냅샷을 푸시할수록 복사 시간이 오래 걸림
  - 즉, 복사 빈도가 감소

**예시**:

- 데이터베이스 수준: 트랜잭션 복제에 초 단위 소요
- 데이터베이스 스냅샷을 하위 파일시스템으로 내보내기: 40분 소요
- 하위 파일시스템의 전체 백업: 몇 시간 소요

**복원 시 데이터 손실**:

- 최신 스냅샷 복원 시 최대 40분의 최근 데이터 손실 가능
- 파일시스템 백업에서 복원 시 몇 시간의 누락된 트랜잭션 발생 가능
- 또한 복원은 백업만큼 오래 걸릴 수 있으므로 실제 데이터 로드에 몇 시간 소요 가능

#### 백업 보존 기간의 중요성

- 전체 데이터베이스의 갑작스러운 비우기는 빠르게 알아차릴 가능성이 높음
- 더 점진적인 데이터 손실은 적절한 사람의 주의를 끌기까지 며칠이 걸릴 수 있음
- 후자의 시나리오에서 손실된 데이터 복원에는 시간적으로 더 먼 스냅샷 필요
- 이렇게 먼 과거로 돌아갈 때는 복원된 데이터를 현재 상태와 병합하고자 할 가능성이 높음
- 이렇게 하면 복원 프로세스가 상당히 복잡해짐

## 제3계층: 조기 감지 (Early Detection)

### 조기 감지의 중요성

**"나쁜" 데이터는 가만히 있지 않고 전파됨**:

- 누락되거나 손상된 데이터에 대한 참조가 복사됨
- 링크가 확산됨
- 모든 업데이트마다 데이터스토어의 전반적인 품질이 저하됨
- 후속 종속 트랜잭션 및 잠재적 데이터 형식 변경으로 인해 주어진 백업에서 복원하기가 시간이 지날수록 더 어려워짐
- **데이터 손실에 대해 빨리 알수록 복구가 더 쉽고 완전할 수 있음**

### 클라우드 개발자가 직면한 과제

고속 환경에서 클라우드 애플리케이션 및 인프라 서비스가 직면하는 많은 데이터 무결성 과제:

- 데이터스토어 간 참조 무결성
- 스키마 변경
- 오래된 코드
- 무중단 데이터 마이그레이션
- 다른 서비스와의 진화하는 통합 지점

**데이터 품질 저하**:

- 데이터의 새로운 관계를 추적하기 위한 의식적인 엔지니어링 노력 없이는 성공적이고 성장하는 서비스의 데이터 품질이 시간이 지남에 따라 저하됨

### 분산 일관성 API의 한계

**초보 클라우드 개발자의 오해**:

- 분산 일관성 스토리지 API(예: Megastore)를 선택하는 초보 개발자는 애플리케이션 데이터의 무결성을 API 아래에 구현된 분산 일관성 알고리즘(예: Paxos)에 위임
- 선택한 API만으로 애플리케이션의 데이터를 좋은 상태로 유지할 것이라고 추론
- 결과적으로 분산 일관성을 보장하는 단일 스토리지 솔루션으로 모든 애플리케이션 데이터를 통합
- 성능 및/또는 규모 감소를 대가로 참조 무결성 문제 회피

**이론과 실제의 차이**:

- 이론적으로 이러한 알고리즘은 완벽하지만, 구현에는 종종 해킹, 최적화, 버그 및 추측이 가득함
- **Paxos 예시**:
  - 이론: Paxos는 실패한 컴퓨트 노드를 무시하고 기능하는 노드의 정족수가 유지되는 한 진행 가능
  - 실제: 실패한 노드를 무시하는 것은 특정 Paxos 구현 아래의 타임아웃, 재시도 및 기타 실패 처리 접근 방식에 해당할 수 있음
  - 특정 방식으로 특정 타이밍으로 특정 데이터센터에서 특정 머신이 실패하면 예측할 수 없는 동작 발생
- 애플리케이션 규모가 클수록 이러한 불일치의 영향을 모르는 사이에 더 자주 받음
- **신뢰하되 검증하라**: 스토리지 시스템을 신뢰하되 검증해야 함!

### Out-of-Band 데이터 검증

#### 필요성

- 데이터 품질이 사용자 눈앞에서 저하되는 것을 방지하고, 저등급 데이터 손상 또는 데이터 손실 시나리오가 복구 불가능해지기 전에 감지하려면:
  - 애플리케이션의 데이터스토어 내부 및 사이에 대역 외 검사 및 균형 시스템 필요

#### 구현

- 가장 자주 이러한 데이터 검증 파이프라인은 MapReduce 또는 Hadoop 작업 모음으로 구현
- 다음과 같은 상황에서 파이프라인이 추가되는 경우가 많음:
  - 이미 인기 있고 성공적인 서비스에 사후적으로 추가
  - 서비스가 확장성 한계에 도달하고 처음부터 다시 구축될 때

#### 개발 속도와의 관계

- 일부 개발자를 데이터 검증 파이프라인 작업에 투입하면 단기적으로 엔지니어링 속도가 느려질 수 있음
- 그러나 데이터 검증에 엔지니어링 리소스를 투입하면 장기적으로 다른 개발자들이 더 빠르게 움직일 수 있는 용기를 얻게 됨
  - 엔지니어들은 데이터 손상 버그가 프로덕션에 눈에 띄지 않게 들어갈 가능성이 낮다는 것을 알게 됨
- 단위 테스트가 프로젝트 수명 주기 초기에 도입될 때 누리는 효과와 유사하게, 데이터 검증 파이프라인은 소프트웨어 개발 프로젝트의 전반적인 가속화를 가져옴

#### Gmail의 사례

- Gmail은 여러 데이터 검증기를 보유하며, 각각 프로덕션에서 실제 데이터 무결성 문제를 감지
- Gmail 개발자들은 다음을 통해 위안을 얻음:
  - 프로덕션 데이터에 불일치를 도입하는 버그가 24시간 이내에 감지된다는 지식
  - 일일 검증보다 덜 자주 데이터 검증기를 실행한다는 생각에 소름이 돋음
- 이러한 검증기는 단위 및 회귀 테스트 문화 및 기타 모범 사례와 함께:
  - Gmail 개발자들에게 일주일에 한 번 이상 Gmail의 프로덕션 스토리지 구현에 코드 변경을 도입할 수 있는 용기를 부여

### Out-of-Band 검증 구현의 복잡성

#### 균형 맞추기

- Out-of-band 데이터 검증은 올바르게 구현하기 어려움
- **너무 엄격한 경우**: 단순하고 적절한 변경도 검증 실패를 일으킴
  - 결과: 엔지니어들이 데이터 검증을 완전히 포기
- **충분히 엄격하지 않은 경우**: 사용자 경험에 영향을 미치는 데이터 손상이 감지되지 않고 통과할 수 있음
- **올바른 균형 찾기**: 사용자에게 치명적인 불변성만 검증

#### Google Drive의 사례

- Google Drive는 파일 내용이 Drive 폴더의 목록과 일치하는지 주기적으로 검증
- 이 두 요소가 일치하지 않으면 일부 파일에 데이터가 누락됨 - 재앙적인 결과
- Drive 인프라 개발자들의 데이터 무결성에 대한 투자:
  - 검증기를 향상시켜 이러한 불일치를 자동으로 수정하도록 함
  - 2013년의 잠재적 긴급 "모든 손을 모아야 하는 파일이 사라지고 있다!" 데이터 손실 상황을 평범한 업무 "집에 가서 월요일에 근본 원인을 수정하자" 상황으로 전환
- **긴급 상황을 평범한 업무로 전환**함으로써:
  - 검증기는 엔지니어링 사기, 삶의 질 및 예측 가능성을 향상시킴

### 대규모 검증의 비용과 최적화

#### 비용 고려사항

- Out-of-band 검증기는 대규모에서 비용이 많이 들 수 있음
- Gmail의 컴퓨트 리소스 사용량의 상당 부분이 일일 검증기 모음 지원
- 추가 비용: 검증기들이 서버 측 캐시 적중률을 낮춰 사용자가 경험하는 서버 측 응답성 감소

#### 완화 전략

- Gmail은 검증기의 속도 제한을 위한 다양한 노브(knob) 제공
- 디스크 경합을 줄이기 위해 주기적으로 검증기 리팩토링
- **리팩토링 성과 사례**:
  - 한 리팩토링 노력에서 다루는 불변성의 범위를 크게 줄이지 않고 디스크 스핀들에 대한 경합을 60% 감소
- Gmail 검증기의 대부분은 매일 실행되지만, 가장 큰 검증기의 워크로드는 규모상의 이유로 10-14개 샤드로 나뉘어 하루에 하나의 샤드만 검증

#### Google Compute Storage의 사례

- Out-of-band 검증기가 하루 내에 완료할 수 없게 되었을 때:
  - Compute Storage 엔지니어들은 무차별 대입만으로 메타데이터를 검증하는 것보다 더 효율적인 방법을 고안해야 했음
  - 데이터 복구에서의 적용과 유사하게, **계층형 전략**이 out-of-band 데이터 검증에도 유용할 수 있음
- **서비스 확장 시**: 일일 검증기의 엄격함을 희생
  - 일일 검증기가 24시간 이내에 가장 재앙적인 시나리오를 계속 포착하도록 보장
  - 비용과 지연 시간을 억제하기 위해 빈도를 줄여 더 엄격한 검증 계속 수행

### 검증 실패 문제 해결

#### 과제

- 실패한 검증 문제 해결에는 상당한 노력이 필요할 수 있음
- 간헐적 검증 실패의 원인은 몇 분, 몇 시간 또는 며칠 내에 사라질 수 있음
- 따라서 검증 감사 로그를 빠르게 드릴다운할 수 있는 능력이 필수적

#### 성숙한 Google 서비스의 제공사항

Gmail의 온콜 엔지니어들에게 제공되는 것:

1. **플레이북 항목 모음**: 검증 실패 경고에 대응하는 방법을 설명
2. **BigQuery와 유사한 조사 도구**
3. **데이터 검증 대시보드**

#### 효과적인 Out-of-Band 데이터 검증의 요구사항

다음 모두를 요구함:

- 검증 작업 관리
- 모니터링, 경고 및 대시보드
- 속도 제한 기능
- 문제 해결 도구
- 프로덕션 플레이북
- 검증기를 쉽게 추가하고 리팩토링할 수 있게 하는 데이터 검증 API

#### 조직 구조 제안

- 대부분의 소규모 엔지니어링 팀은 고속으로 운영되면서 이 모든 시스템을 설계, 구축 및 유지할 여유가 없음
- 그렇게 하도록 압력을 받으면 결과는 종종 취약하고 제한적이며 낭비적인 일회용 솔루션이 되어 빠르게 사용 불능 상태가 됨
- **권장 구조**:
  - 중앙 인프라 팀이 여러 제품 엔지니어링 팀을 위한 데이터 검증 프레임워크 제공
  - 중앙 인프라 팀: out-of-band 데이터 검증 프레임워크 유지
  - 제품 엔지니어링 팀: 진화하는 제품에 맞춰 검증기의 핵심인 맞춤 비즈니스 논리 유지

## 데이터 복구가 작동할 것임을 알기

### 전구의 비유

- **전구는 언제 고장나는가?**
  - 스위치를 켰을 때 불이 켜지지 않을 때? 항상 그런 것은 아님
  - 종종 전구는 이미 고장났고, 스위치를 켜는 무반응한 순간에 실패를 알아차릴 뿐
  - 그때쯤이면 방은 어둡고 발가락을 부딪힘

### 복구 의존성의 잠재적 고장

- 복구 의존성(주로 백업이지만 그것만은 아님)이 잠재적으로 고장난 상태일 수 있음
- 데이터 복구를 시도할 때까지는 알지 못함
- **사전 발견의 이점**:
  - 의존하기 전에 복원 프로세스가 고장났음을 발견하면, 피해를 입기 전에 취약점을 해결할 수 있음
  - 다른 백업을 수행하고, 추가 리소스를 제공하고, SLO를 변경할 수 있음
  - 하지만 이러한 조치를 사전에 취하려면 먼저 필요하다는 것을 알아야 함

### 취약점 감지 방법

- 정상 운영의 일부로 복구 프로세스를 지속적으로 테스트
- 복구 프로세스가 성공의 하트비트 표시를 제공하지 못할 때 발화되는 경고 설정

### 복구 프로세스에서 잘못될 수 있는 것

- **모든 것이 잘못될 수 있음** - 그래서 밤에 편안히 잠들게 해줄 유일한 테스트는 **완전한 end-to-end 테스트**
- 증명은 실제로 해봐야 함(pudding에 있어야 함)
- 최근에 성공적인 복구를 실행했더라도 복구 프로세스의 일부가 여전히 고장날 수 있음
- **이 장에서 얻어야 할 단 하나의 교훈**:
  - 최근 상태를 복구할 수 있다는 것을 알 수 있는 유일한 방법은 실제로 그렇게 하는 것

### 복구 테스트 자동화

- 복구 테스트가 수동적이고 단계적인 이벤트라면:
  - 테스트는 신뢰를 얻을 만큼 깊거나 자주 수행되지 않는 환영받지 못하는 지루한 일이 됨
- **따라서**: 가능할 때마다 이러한 테스트를 자동화하고 지속적으로 실행

### 확인해야 할 복구 계획의 측면

- 백업이 유효하고 완전한가, 아니면 비어 있는가?
- 복구를 구성하는 모든 설정, 복원 및 사후 처리 작업을 실행할 충분한 머신 리소스가 있는가?
- 복구 프로세스가 합리적인 벽시계 시간 내에 완료되는가?
- 복구 프로세스가 진행됨에 따라 상태를 모니터링할 수 있는가?
- 24/7 사용할 수 없는 오프사이트 미디어 스토리지 금고에 대한 액세스와 같이 통제할 수 없는 리소스에 대한 중요한 종속성이 없는가?

### Google의 테스트 경험

- 테스트를 통해 앞서 언급한 실패 및 성공적인 데이터 복구의 다른 많은 구성 요소의 실패를 발견
- 이러한 실패를 정기적인 테스트에서 발견하지 못했다면(즉, 실제 긴급 상황에서 사용자 데이터를 복구해야 할 때만 실패를 발견했다면):
  - 오늘날 Google의 가장 성공적인 일부 제품이 시간의 시험을 견뎌내지 못했을 가능성이 높음

### 실패의 불가피성

- **실패는 불가피함**
- 실제 데이터 손실에 직면했을 때, 압박을 받을 때까지 기다려 실패를 발견한다면 불장난을 하는 것
- 테스트가 실제 재앙이 닥치기 전에 실패를 강제로 발생시키면:
  - 피해가 발생하기 전에 문제를 해결할 수 있음

## 사례 연구

### 사례 1: Gmail - 2011년 2월: GTape에서 복원

#### 사건의 고유성

- 두 가지 면에서 독특했던 첫 번째 복구 사례 연구:
  1. 데이터 손실을 초래하기 위해 동시에 발생한 실패의 수
  2. 마지막 방어선인 GTape 오프라인 백업 시스템의 가장 큰 사용

#### 사건 경과

**2011년 2월 27일 일요일, 늦은 저녁**:

- Gmail 백업 시스템 호출기 작동, 전화회의 참여 번호 표시
- 오랫동안 두려워했던 사건 - 백업 시스템 존재의 이유 - 발생:
  - Gmail이 상당량의 사용자 데이터 손실
  - 시스템의 많은 안전장치와 내부 검사 및 중복성에도 불구하고 Gmail에서 데이터가 사라짐

#### 복구 과정

**GTape의 첫 대규모 사용**:

- 라이브 고객 데이터를 복원하기 위한 GTape(Gmail용 글로벌 백업 시스템)의 첫 대규모 사용
- 다행히 처음이 아니었음 - 유사한 상황이 이전에 여러 번 시뮬레이션됨
- **결과적으로 가능했던 것**:
  - 영향을 받은 대부분의 사용자 계정을 복원하는 데 걸리는 시간 추정치 제공
  - 초기 추정치의 몇 시간 이내에 모든 계정 복원
  - 추정 완료 시간 전에 99% 이상의 데이터 복구

#### 성공 요인

- 이러한 추정을 공식화할 수 있는 능력이 행운이었나? **아니오**
- 성공은 다음의 결실:
  - 계획
  - 모범 사례 준수
  - 노력
  - 협력
- Google은 심층 방어 및 비상 대비의 모범 사례에 따라 설계된 계획을 실행하여 적시에 손실된 데이터를 복원할 수 있었음

#### 공개 반응

- Google이 이전에 공개되지 않은 테이프 백업 시스템에서 이 데이터를 복구했다고 공개했을 때:
  - 대중의 반응은 놀라움과 재미의 혼합
  - "테이프? Google에는 이렇게 중요한 데이터를 복제할 많은 디스크와 빠른 네트워크가 없나요?"
- 물론 Google은 그러한 리소스를 보유
- 하지만 **심층 방어 원칙**은 단일 보호 메커니즘의 고장 또는 손상에 대비한 여러 보호 계층 제공을 요구

#### Gmail 백업의 심층 방어

Gmail과 같은 온라인 시스템 백업은 두 계층에서 심층 방어 제공:

1. 내부 Gmail 중복성 및 백업 하위 시스템의 실패
2. 기본 스토리지 미디어(디스크)에 영향을 미치는 장치 드라이버 또는 파일시스템의 광범위한 실패 또는 제로데이 취약점

**이번 실패**: 첫 번째 시나리오로 인한 것

- Gmail에는 손실된 데이터를 복구하는 내부 수단이 있었지만, 이 손실은 내부 수단이 복구할 수 있는 범위를 넘어섰음

#### 내부적으로 가장 찬사를 받은 측면

- Gmail 데이터 복구에서 내부적으로 가장 찬사를 받은 측면:
  - 복구를 구성한 협력의 정도와 원활한 조정
- Gmail 또는 데이터 복구와 완전히 무관한 팀을 포함한 많은 팀이 도움을 주기 위해 협력
- 이렇게 광범위하게 분산된 헤라클레스적 노력을 조율할 중앙 계획 없이는 복구가 그렇게 원활하게 성공할 수 없었음
  - 이 계획은 정기적인 예행 연습과 드라이 런의 산물

#### 비상 대비의 중요성

- Google의 비상 대비에 대한 헌신:
  - 이러한 실패를 불가피한 것으로 간주
- 이러한 불가피성을 받아들이고:
  - 이러한 재난을 피하기를 희망하거나 내기하지 않고 발생할 것으로 예상
- 따라서 예측 가능한 실패뿐만 아니라 일정량의 무작위적인 미분화된 고장을 처리하기 위한 계획 필요

**결론**: 모범 사례 준수가 중요하다는 것을 항상 알고 있었고, 그 격언이 사실로 입증된 것을 보게 되어 좋았음

### 사례 2: Google Music - 2012년 3월: 폭주 삭제 감지

#### 사건의 독특한 과제

- 복구 중인 데이터스토어의 규모에 고유한 물류 과제:
  - 5,000개 이상의 테이프를 어디에 보관할 것인가?
  - 합리적인 시간 내에 오프라인 미디어에서 그렇게 많은 데이터를 효율적으로(또는 실행 가능하게라도) 읽는 방법은?

#### 사건 발생

**2012년 3월 6일 화요일, 오후 중반**:

- **문제 발견**:
  - Google Music 사용자가 이전에 문제없던 트랙이 건너뛰어진다고 보고
  - Google Music 사용자와 인터페이스하는 팀이 Google Music 엔지니어에게 알림
  - 미디어 스트리밍 문제일 가능성으로 조사

**3월 7일**:

- 조사 엔지니어가 재생 불가능한 트랙의 메타데이터가 실제 오디오 데이터를 가리켜야 하는 참조가 누락되었음을 발견
- 놀라움: 명백한 해결책은 오디오 데이터를 찾아 데이터에 대한 참조를 복원하는 것
- 그러나 Google 엔지니어링은 근본적으로 문제를 해결하는 문화를 자랑하므로 엔지니어는 더 깊이 파고듦

**근본 원인 발견**:

- 데이터 무결성 실수의 원인을 발견했을 때 거의 심장마비를 일으킬 뻔함:
  - 오디오 참조가 개인정보 보호 데이터 삭제 파이프라인에 의해 제거됨
  - Google Music의 이 부분은 기록적인 시간 내에 매우 많은 수의 오디오 트랙을 삭제하도록 설계됨

#### 피해 평가

**Google의 개인정보 보호 정책**:

- 사용자의 개인 데이터 보호
- Google Music에 특별히 적용: 사용자가 삭제한 후 합리적인 시간 내에 음악 파일 및 관련 메타데이터 제거
- Google Music의 인기가 치솟으면서 데이터 양이 빠르게 증가
  - 원래 삭제 구현을 2012년에 더 효율적으로 재설계 필요

**첫 실행**:

- 2월 6일, 업데이트된 데이터 삭제 파이프라인이 관련 메타데이터를 제거하는 첫 실행 수행
- 당시 아무것도 잘못된 것처럼 보이지 않아 파이프라인의 두 번째 단계가 관련 오디오 데이터도 제거하도록 허용

**엔지니어의 조치**:

- 엔지니어의 최악의 악몽이 사실일 수 있는가?
- 즉시 경보를 울림:
  - 지원 사례의 우선순위를 Google의 가장 긴급한 분류로 높임
  - 엔지니어링 관리 및 사이트 신뢰성 엔지니어링에 문제 보고
- Google Music 개발자 및 SRE의 소규모 팀이 문제를 해결하기 위해 모임
- 문제가 있는 파이프라인을 일시적으로 비활성화하여 외부 사용자 피해의 물결을 막음

**피해 규모 확인**:

- 수백만에서 수십억 개의 파일에 대한 메타데이터를 여러 데이터센터에 분산되어 수동으로 확인하는 것은 상상할 수 없음
- 팀은 피해를 평가하기 위해 급히 MapReduce 작업을 작성하고 작업이 완료되기를 필사적으로 기다림
- **3월 8일 결과 도착**:
  - 리팩토링된 데이터 삭제 파이프라인이 제거하지 말았어야 할 약 600,000개의 오디오 참조 제거
  - 21,000명의 사용자에게 영향을 미치는 오디오 파일
  - 급한 진단 파이프라인이 몇 가지 단순화를 했기 때문에 실제 피해 범위가 더 나쁠 수 있음

**시간적 압박**:

- 버그가 있는 데이터 삭제 파이프라인이 처음 실행된 지 한 달 이상 경과
- 그 첫 실행 자체가 제거하지 말았어야 할 수십만 개의 오디오 트랙 제거
- 데이터를 복구할 희망이 있는가?
- 트랙이 복구되지 않거나 충분히 빠르게 복구되지 않으면 Google은 사용자로부터 비난을 받아야 함
- **어떻게 이 결함을 알아차리지 못했는가?**

#### 문제 해결

**병렬 버그 식별 및 복구 노력**:

- 문제 해결의 첫 번째 단계: 실제 버그 식별, 버그가 어떻게 그리고 왜 발생했는지 결정
- 근본 원인이 식별되고 수정되지 않는 한 모든 복구 노력은 헛될 것
- 사용자가 삭제한 오디오 트랙의 요청을 존중하기 위해 파이프라인을 다시 활성화해야 하는 압박
  - 그렇게 하면 계속해서 저장 구매 음악 또는 더 나쁘게는 자신이 힘들게 녹음한 오디오 파일을 잃게 될 무고한 사용자에게 피해
- **Catch-22에서 벗어나는 유일한 방법**:
  - 근본 원인에서 문제를 해결하고 빠르게 수정

**시간 낭비 불가**:

- 복구 노력을 시작하기 전에 낭비할 시간 없음
- 오디오 트랙 자체는 테이프에 백업되었지만:
  - Gmail 사례 연구와 달리 Google Music의 암호화된 백업 테이프는 트럭으로 오프사이트 스토리지 위치로 운반됨
  - 이 옵션은 사용자 오디오 데이터의 방대한 백업에 더 많은 공간 제공
- 영향을 받은 사용자의 경험을 빠르게 복원하기 위해:
  - 팀은 근본 원인을 해결하면서 병렬로 오프사이트 백업 테이프 검색(상당히 시간이 소요되는 복원 옵션) 결정

**팀 분할**:

- 엔지니어들이 두 그룹으로 분할:
  1. **가장 경험 많은 SRE**: 복구 노력 작업
  2. **개발자**: 데이터 삭제 코드 분석 및 근본 원인에서 데이터 손실 버그 수정 시도
- 근본 문제에 대한 불완전한 지식으로 인해 복구는 여러 단계로 나누어야 함
- 첫 번째 배치의 거의 50만 개의 오디오 트랙이 식별됨
- 테이프 백업 시스템을 유지하는 팀에 **2012년 3월 8일 오후 4:34(태평양 시간)**에 긴급 복구 노력 통지

**유리한 요인**:

- 복구 팀에게 유리하게 작용한 한 가지 요인:
  - 이 복구 노력은 회사의 연례 재해 복구 테스트 연습(DiRT) 몇 주 후에 발생
- 테이프 백업 팀은 이미 DiRT 테스트의 대상이었던 하위 시스템의 기능
- 테이프 백업 팀은 이미 DiRT 테스트의 대상이었던 하위 시스템의 기능과 한계를 알고 있었음
- DiRT 연습 중에 테스트했던 새 도구를 사용하기 시작
- 새 도구를 사용하여 결합된 복구 팀은 수십만 개의 오디오 파일을 테이프 백업 시스템에 등록된 백업에 매핑하는 고된 작업 시작
- 그런 다음 백업에서 실제 테이프로 파일 매핑

**초기 복구 계획**:

- 이러한 방식으로 팀은 초기 복구 노력에 트럭으로 5,000개 이상의 백업 테이프 회수 포함 결정
- 그 후 데이터센터 기술자들은 테이프 라이브러리에 테이프를 위한 공간 확보 필요
- 테이프를 등록하고 테이프에서 데이터를 추출하는 길고 복잡한 프로세스가 이어질 것:
  - 불량 테이프, 불량 드라이브 및 예상치 못한 시스템 상호 작용 시 해결 방법 및 완화 조치 포함

**누락된 트랙**:

- 불행히도 약 600,000개의 손실된 오디오 트랙 중 436,223개만 테이프 백업에서 발견됨
  - 약 161,000개의 다른 오디오 트랙은 백업되기 전에 삭제됨
- 복구 팀은 테이프 백업이 있는 트랙의 복구 프로세스를 시작한 후 161,000개의 누락된 트랙을 복구하는 방법을 찾기로 결정

**근본 원인 팀의 진행**:

- 한편 근본 원인 팀은 엉뚱한 단서를 추적하고 포기:
  - 처음에는 Google Music이 의존하는 스토리지 서비스가 데이터 삭제 파이프라인을 잘못된 오디오 데이터를 제거하도록 오도한 버그가 있는 데이터를 제공했다고 생각
  - 더 자세한 조사를 통해 그 이론이 거짓으로 입증됨
- 근본 원인 팀은 머리를 긁적이며 찾기 어려운 버그 검색 계속

#### 첫 번째 복구 물결

**복구 시작**:

- 복구 팀이 백업 테이프를 식별하자 **3월 8일** 첫 번째 복구 물결 시작
- 수천 개의 테이프에 분산된 1.5페타바이트의 데이터를 오프사이트 스토리지에서 요청하는 것은 한 가지 문제
- 테이프에서 데이터를 추출하는 것은 또 다른 문제

**맞춤 제작 테이프 백업 소프트웨어 스택의 한계**:

- 이렇게 큰 규모의 단일 복원 작업을 처리하도록 설계되지 않음
- 따라서 초기 복구는 5,475개의 복원 작업으로 분할됨
- 사람 운영자가 분당 하나의 복원 명령을 입력하면 그 많은 복원을 요청하는 데 3일 이상 소요
  - 사람 운영자는 의심할 여지없이 많은 실수를 할 것
- 테이프 백업 시스템에서 복원을 요청하는 것만으로도 SRE가 프로그래밍 방식 솔루션을 개발해야 함

**복원 요청 완료**:

- **3월 9일 자정**까지 Music SRE가 모든 5,475개의 복원 요청 완료
- 테이프 백업 시스템이 마법을 시작
- 4시간 후, 오프사이트 위치에서 회수할 5,337개의 백업 테이프 목록 출력
- 8시간 후, 일련의 트럭 배송으로 테이프가 데이터센터에 도착

**테이프 로딩 준비**:

- 트럭이 이동하는 동안:
  - 데이터센터 기술자들이 여러 테이프 라이브러리를 유지보수를 위해 중단
  - 대규모 데이터 복구 작업을 위한 공간을 만들기 위해 수천 개의 테이프 제거
- 그런 다음 기술자들은 새벽에 도착한 수천 개의 테이프를 손으로 고되게 로딩하기 시작
- 과거 DiRT 연습에서 이 수동 프로세스는 대규모 복원을 위해 테이프 라이브러리 공급업체가 제공하는 로봇 기반 방법보다 수백 배 빠른 것으로 입증
- 3시간 이내에 라이브러리가 다시 가동되어 테이프를 스캔하고 분산 컴퓨트 스토리지에 수천 개의 복원 작업 수행

#### 복원 과정의 지연

**예상보다 긴 복구 시간**:

- 팀의 DiRT 경험에도 불구하고 대규모 1.5페타바이트 복구는 추정된 2일보다 오래 걸림
- **3월 10일 아침**까지:
  - 436,223개의 오디오 파일 중 74%만 3,475개의 회수된 백업 테이프에서 인근 컴퓨트 클러스터의 분산 파일시스템 스토리지로 성공적으로 전송됨
  - 다른 1,862개의 백업 테이프는 공급업체에 의해 테이프 회수 프로세스에서 누락됨
  - 또한 17개의 불량 테이프로 인해 복구 프로세스가 지연됨

**중복성 테이프 활용**:

- 불량 테이프로 인한 실패를 예상하여 백업 파일을 작성할 때 중복 인코딩 사용
- 첫 번째 오프사이트 회수에서 누락된 다른 1,862개의 테이프와 함께 중복성 테이프를 회수하기 위해 추가 트럭 배송 시작

**복구 완료**:

- **3월 11일 아침**까지:
  - 복원 작업의 99.95% 이상 완료
  - 나머지 파일에 대한 추가 중복성 테이프 회수 진행 중
- 데이터가 분산 파일시스템에 안전하게 있었지만:
  - 사용자가 액세스할 수 있도록 하기 위해 추가 데이터 복구 단계 필요
- Google Music 팀은 복구된 오디오 파일의 작은 샘플에 대해 데이터 복구 프로세스의 이러한 최종 단계를 병렬로 실행하여 프로세스가 여전히 예상대로 작동하는지 확인

**예상치 못한 중단**:

- 바로 그 순간, 관련 없지만 중요한 사용자에게 영향을 미치는 프로덕션 장애로 인해 Google Music 프로덕션 호출기가 울림
  - Google Music 팀을 이틀 동안 완전히 참여시킨 장애
- **3월 13일**에 데이터 복구 노력 재개:
  - 436,223개의 오디오 트랙이 모두 다시 사용자에게 액세스 가능하게 됨
- **7일 이내**에 1.5페타바이트의 오디오 데이터가 오프사이트 테이프 백업의 도움으로 사용자에게 복원됨
  - 7일 중 5일이 실제 데이터 복구 노력 구성

#### 두 번째 복구 물결

**나머지 트랙 복구**:

- 첫 번째 복구 물결을 마친 후:
  - 팀은 백업되기 전에 버그에 의해 삭제된 다른 161,000개의 누락된 오디오 파일에 초점 전환
- 이러한 파일의 대부분은 스토어에서 구매한 프로모션 트랙:
  - 원래 스토어 사본은 버그의 영향을 받지 않음
  - 이러한 트랙은 빠르게 복원되어 영향을 받은 사용자가 다시 음악을 즐길 수 있음

**사용자 업로드 파일**:

- 그러나 161,000개의 오디오 파일 중 작은 부분은 사용자가 직접 업로드한 것
- Google Music 팀은 서버가 영향을 받은 사용자의 Google Music 클라이언트에 **3월 14일** 이후의 파일을 다시 업로드하도록 요청하게 함
- 이 프로세스는 일주일 이상 지속
- **이로써 사건에 대한 완전한 복구 노력이 종료됨**

#### 근본 원인 해결

**결함 식별**:

- 결국 Google Music 팀이 리팩토링된 데이터 삭제 파이프라인의 결함 식별
- 이 결함을 이해하려면 먼저 대규모 오프라인 데이터 처리 시스템이 어떻게 진화하는지에 대한 컨텍스트 필요

**대규모 복잡한 서비스의 데이터 삭제**:

- 여러 하위 시스템과 스토리지 서비스로 구성된 크고 복잡한 서비스의 경우:
  - 삭제된 데이터를 제거하는 것처럼 간단한 작업조차도 단계적으로 수행 필요
  - 각 단계마다 다른 데이터스토어 포함

**병렬 처리의 필요성과 위험**:

- 데이터 처리가 빠르게 완료되려면:
  - 처리는 다양한 하위 시스템에 큰 부하를 가하는 수만 대의 머신에서 실행되도록 병렬화됨
  - 이 분산은 사용자를 위해 서비스를 느리게 하거나 심한 부하로 인해 서비스가 충돌할 수 있음

**보조 스토리지 사용의 경합 조건**:

- 이러한 바람직하지 않은 시나리오를 피하기 위해:
  - 클라우드 컴퓨팅 엔지니어는 종종 보조 스토리지에 데이터의 수명이 짧은 복사본을 만듦
  - 그런 다음 데이터 처리가 수행됨
- 보조 데이터 복사본의 상대적 연령이 신중하게 조정되지 않으면 이 관행은 경합 조건을 도입

**파이프라인 설계의 가정**:

- 예를 들어, 파이프라인의 두 단계가 엄격하게 3시간 간격으로 순차적으로 실행되도록 설계될 수 있음
  - 두 번째 단계가 입력의 정확성에 대해 단순화 가정을 할 수 있도록
- 이 단순화 가정이 없으면 두 번째 단계의 논리를 병렬화하기 어려울 수 있음
- 그러나 데이터 볼륨이 증가함에 따라 단계를 완료하는 데 시간이 더 오래 걸릴 수 있음
- 결국 원래 설계 가정이 두 번째 단계에 필요한 특정 데이터 조각에 대해 더 이상 유효하지 않을 수 있음

**경합 조건의 발생**:

- 처음에는 이 경합 조건이 데이터의 아주 작은 부분에서만 발생할 수 있음
- 그러나 데이터 볼륨이 증가함에 따라 점점 더 많은 부분의 데이터가 경합 조건을 트리거할 위험에 처함
- 이러한 시나리오는 확률적:
  - 파이프라인은 대부분의 데이터와 대부분의 시간에 올바르게 작동
- 데이터 삭제 파이프라인에서 이러한 경합 조건이 발생하면:
  - 잘못된 데이터가 비결정적으로 삭제될 수 있음

**Google Music의 경우**:

- Google Music의 데이터 삭제 파이프라인은 조정 및 오류에 대한 큰 여유를 두고 설계됨
- 그러나 서비스가 성장함에 따라 파이프라인의 업스트림 단계에 증가된 시간이 필요하기 시작:
  - Google Music이 개인정보 보호 요구 사항을 계속 충족할 수 있도록 성능 최적화 도입
- 결과적으로 이 파이프라인에서 우발적인 데이터 삭제 경합 조건의 확률이 증가하기 시작
- 파이프라인이 리팩토링되었을 때:
  - 이 확률이 다시 크게 증가하여 경합 조건이 더 정기적으로 발생하는 지점까지 도달

**개선 조치**:

- 복구 노력의 여파로:
  - Google Music은 이러한 유형의 경합 조건을 제거하도록 데이터 삭제 파이프라인 재설계
- 또한 프로덕션 모니터링 및 경고 시스템을 향상:
  - 사용자가 문제를 알아차리기 전에 유사한 대규모 폭주 삭제 버그를 감지하고 수정하는 것을 목표로 함

## 데이터 무결성에 적용된 SRE의 일반 원칙

### 초심자의 마음 (Beginner's Mind)

- 대규모의 복잡한 서비스에는 완전히 이해할 수 없는 고유한 버그가 있음
- 복잡한 시스템을 충분히 이해하여 특정 방식으로 실패하지 않을 것이라고 말할 수 없다고 생각하지 말 것
- **신뢰하되 검증하고, 심층 방어를 적용**
- 참고: "초심자의 마음"은 신입 사원을 데이터 삭제 파이프라인 담당자로 배치하는 것을 제안하지 않음!

### 신뢰하되 검증 (Trust but Verify)

- 의존하는 모든 API가 항상 완벽하게 작동하지는 않음
- 엔지니어링 품질이나 테스트의 엄격함에 관계없이 API에 결함이 있을 것이 확실함
- API 의미론이 그럴 필요가 없다고 제안하더라도:
  - Out-of-band 데이터 검증기를 사용하여 데이터의 가장 중요한 요소의 정확성 확인
- **완벽한 알고리즘이 완벽한 구현을 갖지 못할 수 있음**

### 희망은 전략이 아니다 (Hope Is Not a Strategy)

- 지속적으로 실행되지 않는 시스템 구성 요소는 가장 필요할 때 실패함
- **정기적인 실행으로 데이터 복구가 작동함을 증명**하거나, 데이터 복구가 작동하지 않을 것
- 인간은 시스템 구성 요소를 지속적으로 실행할 규율이 부족하므로 **자동화가 친구**
- 그러나 경쟁 우선순위를 가진 엔지니어로 이러한 자동화 노력을 구성하면 임시 방편으로 끝날 수 있음

### 심층 방어 (Defense in Depth)

- 가장 완벽한 시스템조차도 버그와 운영자 오류에 취약함
- 데이터 무결성 문제를 수정 가능하게 하려면:
  - 서비스가 이러한 문제를 빠르게 감지해야 함
- **모든 전략은 결국 변화하는 환경에서 실패함**
- 최고의 데이터 무결성 전략은 다계층:
  - 서로에게 의존하고 합리적인 비용으로 광범위한 시나리오를 함께 해결하는 여러 전략

## 결론

### 데이터 가용성의 중요성

- **데이터 가용성은 모든 데이터 중심 시스템의 최우선 관심사여야 함**
- 수단보다는 목표에 집중하는 것이 유용:
  - 테스트 주도 개발에서 한 페이지를 빌려와서
  - 예측된 최대 다운타임으로 시스템이 데이터 가용성을 유지할 수 있음을 증명
- 이 최종 목표를 달성하기 위해 사용하는 수단과 메커니즘은 필요악
- **목표에 집중함으로써** 다음의 함정에 빠지는 것을 피함:
  - "수술은 성공했지만 시스템이 죽었다"

### 모든 것이 잘못될 것임을 인식

- 단지 무언가가 잘못될 수 있다는 것뿐만 아니라, **모든 것이 잘못될 것**이라는 것을 인식하는 것:
  - 실제 긴급 상황에 대한 준비를 위한 중요한 단계
- 가능한 모든 재해 조합의 매트릭스와 이러한 각 재해를 해결하기 위한 계획:
  - 최소한 하룻밤은 편안히 잘 수 있게 해줌
- 복구 계획을 최신 상태로 유지하고 실행하면:
  - 일년 중 다른 364일 밤도 잘 수 있게 해줌

### 복구 시간 단축

- 합리적인 시간 N 내에 모든 고장에서 복구하는 데 더 능숙해지면:
  - 더 빠르고 세밀한 손실 감지를 통해 그 시간을 단축하는 방법 찾기
  - **N = 0에 접근하는 것을 목표**로 함
- 그런 다음 복구 계획에서 예방 계획으로 전환 가능:
  - 모든 데이터, 항상(all the data, all the time)이라는 성배를 달성하는 것을 목표로 함
- 이 목표를 달성하면:
  - 마땅히 받아야 할 휴가 때 해변에서 잠을 잘 수 있음

### 주요 교훈 요약

1. **계층형 방어**: 소프트 삭제, 백업 및 복구, 조기 감지의 다중 계층 구현
2. **자동화된 테스트**: 복구 프로세스를 정기적으로 자동 테스트
3. **심층 방어**: 단일 실패 지점에 의존하지 않음
4. **신속한 감지**: 데이터 손상을 빨리 발견할수록 복구가 쉬움
5. **복제는 백업이 아님**: 복제에만 의존하지 말 것
6. **실제 복원 연습**: 백업이 아닌 복원에 집중
7. **대규모 전략**: 규모에 따라 다른 접근 방식 필요 (1T vs 1E)
8. **조직 문화**: 데이터 무결성을 모든 엔지니어링 활동의 핵심으로 만들기

# 27장: 대용량 환경에서의 신뢰할 수 있는 제품 출시

## 서론: 산타를 추적하라

### NORAD Tracks Santa 사례

- **Keyhole 서비스**: Google Maps와 Google Earth의 위성 이미지 제공
  - 일반적인 날: 초당 수천 개의 위성 이미지 제공
  - **2011년 크리스마스 이브**: 평소 피크 트래픽의 25배
    - 초당 100만 건 이상의 요청

### 출시의 어려움

이 프로젝트의 특징:

- **엄격한 마감기한**: 산타가 늦게 올 수 없음
- **많은 홍보**: 수백만 명의 관중
- **급격한 트래픽 증가**: 크리스마스 이브에 집중
- **실패의 대가**: "어린이를 울게 만들 수 있음"

### Launch Coordination Engineers (LCE)의 역할

- 다양한 방식으로 출시가 잘못될 수 있음을 예측
- 출시에 관련된 여러 엔지니어링 그룹 간 조정
- "Make-children-cry switches": 서비스를 보호하기 위한 킬 스위치

## Google의 출시 환경

### 출시의 정의

- **출시(Launch)**: 애플리케이션에 외부적으로 보이는 변경 사항을 도입하는 새로운 코드
- Google은 **주당 최대 70회 출시** 수행

### 전통 기업과의 차이

**전통 기업**:

- 3년마다 한 번 정도 제품 출시
- 상세한 출시 프로세스 불필요
  - 다음 출시 시 대부분의 프로세스가 구식화
- 출시 프로세스 설계 기회 부족
  - 충분한 경험 축적 불가

**인터넷 기업 (Google)**:

- 빠른 반복 출시
- 서버 측에서 새 기능 롤아웃 가능
- 개별 고객 워크스테이션에 소프트웨어 롤아웃 불필요
- **간소화된 출시 프로세스의 근거와 기회 제공**

## Launch Coordination Engineering (LCE)

### LCE의 탄생 배경

**소프트웨어 엔지니어의 강점과 약점**:

- 강점: 코딩 및 설계에 대한 전문 지식, 자체 제품 기술에 대한 깊은 이해
- 약점: 수백만 사용자에게 제품을 출시하는 동시에 중단을 최소화하고 성능을 최대화하는 과제와 함정에 익숙하지 않음

### LCE의 역할

**SRE 내 전담 컨설팅 팀**:

- 소프트웨어 엔지니어 및 시스템 엔지니어로 구성
  - 일부는 다른 SRE 팀 경험 보유
- 새 제품 또는 기능 출시의 기술적 측면 담당
- 개발자가 Google의 견고성, 확장성, 신뢰성 표준을 충족하는 제품 구축하도록 안내

### LCE의 주요 기능

1. **감사(Auditing)**

   - Google의 신뢰성 표준 및 모범 사례 준수 여부 제품 및 서비스 감사
   - 신뢰성 향상을 위한 구체적인 조치 제공

2. **연락 담당자(Liaison)**

   - 출시에 관련된 여러 팀 간 연락 담당

3. **추진력 유지(Driving)**

   - 작업이 추진력을 유지하도록 출시의 기술적 측면 주도

4. **게이트키퍼(Gatekeeper)**

   - "안전"하다고 판단된 출시 승인

5. **교육(Education)**
   - 모범 사례 및 Google 서비스와 통합 방법에 대해 개발자 교육
   - 내부 문서 및 교육 리소스 제공

### LCE 감사 시기

- 대부분의 감사: 새 제품 또는 서비스 출시 전
- SRE 지원 없는 제품 개발 팀 출시: LCE가 원활한 출시를 위한 도메인 지식 제공
- 강력한 SRE 지원이 있는 제품: 중요한 출시 중에도 LCE 팀과 협력
  - 새 제품 출시의 과제는 신뢰할 수 있는 서비스의 일상적인 운영과 상당히 다름
  - LCE 팀은 수백 건의 출시 경험 활용 가능
- 새 서비스가 SRE와 처음 협력할 때도 서비스 감사 촉진

## Launch Coordination Engineer의 역할

### 팀 구성

- 직접 이 역할로 채용된 엔지니어
- Google 서비스 실행 경험이 있는 SRE

### 요구 역량

**기술적 요구사항**:

- 다른 SRE와 동일한 기술 요구사항 보유

**소프트 스킬**:

- 강력한 의사소통 능력
- 리더십 기술
- 서로 다른 당사자를 모아 공동 목표를 향해 일하게 함
- 때때로 발생하는 갈등 중재
- 동료 엔지니어 안내, 코칭 및 교육

### LCE 전담 팀의 장점

#### 1. 경험의 폭 (Breadth of Experience)

- 진정한 제품 간 팀
- Google의 거의 모든 제품 영역에서 활동
- 광범위한 제품 간 지식 및 회사 전체 많은 팀과의 관계
- 지식 전달을 위한 탁월한 수단

#### 2. 다기능 관점 (Cross-functional Perspective)

- 출시에 대한 전체적인 시각 보유
- SRE, 개발, 제품 관리의 서로 다른 팀 간 조정 가능
- 여러 시간대의 6개 이상 팀에 걸쳐 있는 복잡한 출시에 특히 중요

#### 3. 객관성 (Objectivity)

- 비당파적 자문 역할
- 이해 관계자 간 균형 및 중재 역할
  - SRE, 제품 개발자, 제품 관리자, 마케팅 포함

### 인센티브 구조

- LCE는 SRE 역할이므로 다른 우려사항보다 신뢰성을 우선시하도록 인센티브 부여
- Google의 신뢰성 목표를 공유하지 않지만 빠른 변화율을 공유하는 회사는 다른 인센티브 구조 선택 가능

## 출시 프로세스 설정

### 좋은 출시 프로세스의 기준

Google이 10년 이상에 걸쳐 출시 프로세스를 다듬으면서 식별한 기준:

1. **경량 (Lightweight)**

   - 개발자에게 부담이 적음

2. **견고함 (Robust)**

   - 명백한 오류를 포착

3. **철저함 (Thorough)**

   - 중요한 세부 사항을 일관되고 재현 가능하게 처리

4. **확장 가능 (Scalable)**

   - 많은 수의 간단한 출시와 적은 수의 복잡한 출시 모두 수용

5. **적응 가능 (Adaptable)**
   - 일반적인 유형의 출시(예: 제품에 새 UI 언어 추가)와 새로운 유형의 출시(예: Chrome 브라우저 또는 Google Fiber의 초기 출시) 모두에 잘 작동

### 상충하는 요구사항의 균형

- 일부 요구사항은 명백한 충돌 (예: 경량이면서 동시에 철저함)
- 이러한 기준을 서로 균형 있게 유지하려면 지속적인 작업 필요

### 균형을 달성하기 위한 전략

#### 1. 단순성 (Simplicity)

- 기본을 올바르게 수행
- 모든 가능성을 계획하지 않음

#### 2. 높은 접촉 접근법 (High Touch Approach)

- 경험 많은 엔지니어가 각 출시에 맞게 프로세스 맞춤화

#### 3. 빠른 공통 경로 (Fast Common Paths)

- 항상 공통 패턴을 따르는 출시 클래스 식별 (예: 새 국가에서 제품 출시)
- 이 클래스에 대해 단순화된 출시 프로세스 제공

### 프로세스 최적화의 중요성

- 경험상 엔지니어들은 너무 부담스럽거나 가치가 불충분하다고 생각하는 프로세스를 우회하는 경향
  - 특히 팀이 이미 크런치 모드에 있고 출시 프로세스가 출시를 막는 또 다른 항목으로 보일 때
- **LCE는 출시 경험을 지속적으로 최적화**하여 비용과 이점 간 적절한 균형 유지 필요

## 출시 체크리스트

### 체크리스트의 중요성

- 체크리스트는 다양한 분야에서 실패를 줄이고 일관성과 완전성을 보장하는 데 사용
- 일반적인 예: 항공 비행 전 체크리스트, 수술 체크리스트

### LCE 체크리스트의 사용

- 출시 자격 심사를 위해 출시 체크리스트 사용
- 체크리스트의 역할:
  - LCE가 출시 평가
  - 출시 팀에 조치 항목 및 추가 정보 포인터 제공

### 체크리스트 항목 예시

**1. 도메인 이름 관련**

- 질문: 새 도메인 이름이 필요합니까?
- 조치 항목: 원하는 도메인 이름에 대해 마케팅과 조정하고 도메인 등록 요청. 마케팅 양식 링크 제공

**2. 데이터 저장 관련**

- 질문: 영구 데이터를 저장하고 있습니까?
- 조치 항목: 백업 구현 확인. 백업 구현 지침 제공

**3. 남용 방지**

- 질문: 사용자가 서비스를 남용할 가능성이 있습니까?
- 조치 항목: 속도 제한 및 할당량 구현. 공유 서비스 사용

### 체크리스트 관리

#### 크기 관리의 어려움

- 실제로 모든 시스템에 대해 질문할 수 있는 항목은 거의 무한
- 체크리스트가 관리할 수 없는 크기로 쉽게 성장
- 관리 가능한 개발자 부담 유지를 위해 체크리스트의 신중한 큐레이션 필요

#### 체크리스트 성장 억제 전략

- 한때 Google의 출시 체크리스트에 새 질문 추가하려면 부사장 승인 필요
- 현재 LCE가 사용하는 지침:
  1. **모든 질문의 중요성은 입증되어야 함**
     - 이상적으로는 이전 출시 재난으로 입증
  2. **모든 지침은 구체적이고 실용적이며 개발자가 수행하기에 합리적이어야 함**

#### 지속적인 유지보수

체크리스트는 관련성과 최신 상태를 유지하기 위해 지속적인 관심 필요:

- 권장 사항은 시간이 지남에 따라 변경
- 내부 시스템이 다른 시스템으로 교체됨
- 이전 출시의 우려 영역이 새 정책 및 프로세스로 인해 구식화됨

**유지보수 프로세스**:

- LCE가 체크리스트를 지속적으로 큐레이션
- 팀 구성원이 수정이 필요한 항목을 발견하면 작은 업데이트 수행
- 연 1~2회 팀 구성원이 전체 체크리스트 검토
  - 구식 항목 식별
  - 서비스 소유자 및 주제 전문가와 협력하여 체크리스트 섹션 현대화

## 수렴 및 단순화 추진

### 공통 인프라 사용의 필요성

**대규모 조직의 문제**:

- 엔지니어는 일반적인 작업(예: 속도 제한)에 사용 가능한 인프라를 인식하지 못할 수 있음
- 적절한 안내가 없으면 기존 솔루션을 다시 구현할 가능성이 높음

**공통 인프라 라이브러리 수렴의 이점**:

- 중복 노력 감소
- 서비스 간 지식 전달 용이
- 인프라에 집중된 관심으로 인해 더 높은 수준의 엔지니어링 및 서비스 품질

### 출시 체크리스트를 통한 수렴 추진

- Google의 거의 모든 그룹이 공통 출시 프로세스에 참여
- 출시 체크리스트가 공통 인프라 수렴을 위한 수단
- LCE는 맞춤 솔루션 구현 대신 기존 인프라를 빌딩 블록으로 권장 가능
  - 수년간의 경험을 통해 이미 강화된 인프라
  - 용량, 성능 또는 확장성 위험 완화에 도움

**공통 인프라 예시**:

- 속도 제한 또는 사용자 할당량
- 서버에 새 데이터 푸시
- 바이너리의 새 버전 릴리스

### 체크리스트 단순화 효과

- 표준화가 출시 체크리스트를 근본적으로 단순화하는 데 도움
- 예: 속도 제한 요구 사항을 다루는 체크리스트의 긴 섹션을 "시스템 X를 사용하여 속도 제한 구현"이라는 한 줄로 교체 가능

### LCE의 단순화 기회 식별

**LCE의 고유한 위치**:

- Google의 모든 제품에 걸친 경험의 폭으로 인해 단순화 기회 식별에 유리
- 출시 작업 중 직접 장애물 목격:
  - 출시의 어떤 부분이 가장 많은 어려움을 일으키는지
  - 어떤 단계가 불균형적으로 많은 시간을 소요하는지
  - 어떤 문제가 유사한 방식으로 독립적으로 반복해서 해결되는지
  - 공통 인프라가 부족한 곳
  - 공통 인프라에 중복이 존재하는 곳

**출시 경험 간소화 방법**:

- 출시 팀의 옹호자 역할
- 특히 힘든 승인 프로세스의 소유자와 협력하여 기준 단순화 및 일반적인 경우에 대한 자동 승인 구현
- 공통 인프라 소유자에게 문제점 에스컬레이션 및 고객과 대화 생성
- 여러 이전 출시에서 얻은 경험을 활용하여 개별 우려 사항 및 제안에 더 많은 관심 집중

## 예상치 못한 출시

### 새로운 제품 공간 진입 시

- 프로젝트가 새로운 제품 공간 또는 수직 시장에 진입할 때
- LCE는 처음부터 적절한 체크리스트를 만들어야 할 수 있음
- 관련 도메인 전문가의 경험을 종합하는 것이 포함되는 경우가 많음

### 새 체크리스트 작성 시 유용한 구조

광범위한 주제를 중심으로 체크리스트 구조화:

- 신뢰성
- 실패 모드
- 프로세스

### Android 출시 사례

**과제**:

- Android 이전에 Google은 직접 제어하지 않는 클라이언트 측 논리를 가진 대량 소비자 장치를 거의 다루지 않음
- Gmail의 버그는 브라우저에 JavaScript의 새 버전을 푸시하여 몇 시간 또는 며칠 내에 수정 가능
- 모바일 장치에서는 이러한 수정이 옵션이 아님

**접근 방법**:

- 모바일 출시 작업 LCE가 모바일 도메인 전문가와 협력
- 기존 체크리스트의 어떤 섹션이 적용되거나 적용되지 않는지 결정
- 새로운 체크리스트 질문이 필요한 곳 파악

### 체크리스트 적용 시 중요한 고려사항

- 각 질문의 의도를 염두에 두는 것이 중요
- 출시되는 고유한 제품 설계와 관련이 없는 구체적인 질문이나 조치 항목을 무심코 적용하는 것을 피함
- 비정상적인 출시에 직면한 LCE:
  - 안전한 출시를 실행하는 방법에 대한 추상적인 제1원칙으로 돌아가야 함
  - 그런 다음 체크리스트를 개발자에게 구체적이고 유용하게 만들기 위해 다시 전문화

## 출시 체크리스트 개발

### 체크리스트의 중요성

- 재현 가능한 신뢰성으로 새 서비스 및 제품을 출시하는 데 필수적
- Google의 출시 체크리스트는 시간이 지남에 따라 성장
- Launch Coordination Engineering 팀 구성원이 주기적으로 큐레이션

### 회사별 맞춤화

- 출시 체크리스트의 세부 사항은 모든 회사마다 다름
- 세부 사항은 회사의 내부 서비스 및 인프라에 맞춤화되어야 함

### 체크리스트의 주요 주제

#### 1. 아키텍처 및 종속성 (Architecture and Dependencies)

**목적**:

- 서비스가 공유 인프라를 올바르게 사용하는지 확인
- 공유 인프라 소유자를 출시의 추가 이해 관계자로 식별

**Google의 상황**:

- 새 제품의 빌딩 블록으로 사용되는 많은 수의 내부 서비스 보유
- 용량 계획의 후기 단계에서 이 섹션에서 식별된 종속성 목록을 사용하여 모든 종속성이 올바르게 프로비저닝되었는지 확인

**체크리스트 질문 예시**:

- 사용자에서 프론트엔드, 백엔드로의 요청 흐름은 어떻게 됩니까?
- 지연 시간 요구 사항이 다른 여러 유형의 요청이 있습니까?

**조치 항목 예시**:

- 사용자 대면 요청을 비사용자 대면 요청과 격리
- 요청 볼륨 가정 검증 (한 페이지 뷰가 많은 요청으로 전환될 수 있음)

#### 2. 통합 (Integration)

**내부 에코시스템**:

- 많은 회사의 서비스는 다음에 대한 지침을 수반하는 내부 에코시스템에서 실행:
  - 머신 설정 방법
  - 새 서비스 구성
  - 모니터링 설정
  - 로드 밸런싱과 통합
  - DNS 주소 설정 등

**특성**:

- 이러한 내부 에코시스템은 일반적으로 시간이 지남에 따라 성장
- 종종 자체 특성 및 탐색해야 할 함정 보유
- 따라서 체크리스트의 이 섹션은 회사마다 크게 다름

**조치 항목 예시**:

- 서비스에 대한 새 DNS 이름 설정
- 서비스와 통신하도록 로드 밸런서 설정
- 새 서비스에 대한 모니터링 설정

#### 3. 용량 계획 (Capacity Planning)

**출시 시 트래픽 특성**:

- 새 기능은 출시 시 일시적인 사용 증가를 보일 수 있으며 며칠 내에 감소
- 출시 스파이크의 워크로드 또는 트래픽 혼합 유형은 안정 상태와 상당히 다를 수 있음
  - 부하 테스트 결과를 혼란스럽게 만듦
- 대중의 관심은 예측하기 매우 어려움
  - 일부 Google 제품은 초기 추정치보다 최대 15배 높은 출시 스파이크 수용 필요
- 처음에 한 번에 하나의 지역 또는 국가에서 출시하면 더 큰 출시를 처리할 신뢰 개발에 도움

**용량과 중복성 및 가용성의 상호작용**:

- 피크 시 100% 트래픽을 제공하기 위해 3개의 복제된 배포가 필요한 경우
- 유지보수 및 예상치 못한 오작동으로부터 사용자를 보호하기 위해 4~5개의 배포(그 중 1~2개는 중복) 유지 필요
- 데이터센터 및 네트워크 리소스는 종종 리드 타임이 길어 회사가 획득할 수 있도록 충분히 미리 요청 필요

**체크리스트 질문 예시**:

- 이 출시가 보도 자료, 광고, 블로그 게시물 또는 기타 형태의 프로모션과 연결되어 있습니까?
- 출시 중 및 출시 후 얼마나 많은 트래픽과 성장률을 예상합니까?
- 트래픽을 지원하는 데 필요한 모든 컴퓨팅 리소스를 획득했습니까?

#### 4. 실패 모드 (Failure Modes)

**목적**:

- 새 서비스의 가능한 실패 모드에 대한 체계적인 검토
- 처음부터 높은 신뢰성 보장

**검토 사항**:

- 각 구성 요소 및 종속성 검사
- 실패의 영향 식별

**질문 예시**:

- 서비스가 개별 머신 실패를 처리할 수 있습니까?
- 데이터센터 중단?
- 네트워크 실패?
- 잘못된 입력 데이터를 어떻게 처리합니까?
- 서비스 거부(DoS) 공격 가능성에 대비했습니까?
- 종속성 중 하나가 실패하면 서비스가 저하된 모드로 계속 제공할 수 있습니까?
- 서비스 시작 시 종속성의 불가용성을 어떻게 처리합니까?
- 런타임 중에는?

**체크리스트 질문 예시**:

- 설계에 단일 실패 지점이 있습니까?
- 종속성의 불가용성을 어떻게 완화합니까?

**조치 항목 예시**:

- 장기 실행 요청에 대한 리소스 부족을 피하기 위해 요청 마감 시간 구현
- 과부하 상황에서 새 요청을 조기에 거부하기 위해 부하 차단 구현

#### 5. 클라이언트 동작 (Client Behavior)

**전통적인 웹사이트**:

- 합법적인 사용자의 남용 동작을 고려할 필요가 거의 없음
- 모든 요청이 링크 클릭과 같은 사용자 동작에 의해 트리거될 때
- 요청 속도는 사용자가 클릭할 수 있는 속도로 제한됨
- 부하를 두 배로 늘리려면 사용자 수가 두 배가 되어야 함

- 예시:
  - 주기적으로 데이터를 클라우드에 동기화하는 휴대폰 앱
  - 주기적으로 새로 고침하는 웹사이트
- 이러한 시나리오에서 남용 클라이언트 동작은 서비스 안정성을 매우 쉽게 위협할 수 있음

**보호 필요성**:

- 스크레이퍼 및 서비스 거부 공격과 같은 남용 트래픽으로부터 서비스 보호 (제1자 클라이언트의 안전한 동작 설계와는 다름)

**체크리스트 질문 예시**:

- 자동 저장/자동 완성/하트비트 기능이 있습니까?

**조치 항목 예시**:

- 클라이언트가 실패 시 지수적으로 백오프하도록 보장
- 자동 요청을 지터링(jittering)하도록 보장

#### 6. 프로세스 및 자동화 (Processes and Automation)

**Google의 자동화 권장**:

- Google은 엔지니어가 표준 도구를 사용하여 일반적인 프로세스를 자동화하도록 권장
- 그러나 자동화는 결코 완벽하지 않음
- 모든 서비스에는 사람이 실행해야 하는 프로세스가 있음:
  - 새 릴리스 생성
  - 다른 데이터센터로 서비스 이동
  - 백업에서 데이터 복원 등

**단일 실패 지점 최소화**:

- 신뢰성 이유로 단일 실패 지점 최소화 노력
- 단일 실패 지점에는 사람도 포함

**프로세스 문서화**:

- 나머지 프로세스는 출시 전에 문서화되어야 함
- 목적:
  - 정보가 아직 신선할 때 엔지니어의 마음에서 종이로 번역 보장
  - 긴급 상황에서 사용 가능하도록 보장
- 프로세스는 모든 팀 구성원이 긴급 상황에서 주어진 프로세스를 실행할 수 있도록 문서화되어야 함

**체크리스트 질문 예시**:

- 서비스를 계속 실행하는 데 필요한 수동 프로세스가 있습니까?

**조치 항목 예시**:

- 모든 수동 프로세스 문서화
- 서비스를 새 데이터센터로 이동하는 프로세스 문서화
- 새 버전을 빌드하고 릴리스하는 프로세스 자동화

#### 7. 개발 프로세스 (Development Process)

**Google의 버전 관리 사용**:

- Google은 버전 관리를 광범위하게 사용
- 거의 모든 개발 프로세스가 버전 관리 시스템과 깊이 통합
- 많은 모범 사례가 버전 관리 시스템을 효과적으로 사용하는 방법을 중심으로 진행

**브랜치 전략**:

- 대부분의 개발은 메인라인 브랜치에서 수행
- 릴리스는 릴리스별 별도 브랜치에서 빌드
- 이 설정으로 메인라인의 관련 없는 변경 사항을 가져오지 않고 릴리스에서 버그 수정 용이

**구성 파일의 버전 관리**:

- Google은 구성 파일 저장과 같은 다른 목적으로도 버전 관리 사용
- 버전 관리의 많은 장점이 구성 파일에도 적용:
  - 기록 추적
  - 개인에게 변경 사항 귀속
  - 코드 리뷰
- 일부 경우 버전 관리 시스템에서 라이브 서버로 변경 사항을 자동으로 전파
  - 엔지니어가 변경 사항을 제출하기만 하면 라이브로 전환

**조치 항목 예시**:

- 모든 코드 및 구성 파일을 버전 관리 시스템에 체크인
- 새 릴리스 브랜치에서 각 릴리스 컷

#### 8. 외부 종속성 (External Dependencies)

**회사 통제를 벗어난 요인**:

- 때때로 출시가 회사 통제를 벗어난 요인에 의존
- 이러한 요인 식별으로 수반하는 예측 불가능성 완화 가능

**외부 종속성 예시**:

- 제3자가 유지 관리하는 코드 라이브러리
- 다른 회사가 제공하는 서비스 또는 데이터

**발생 가능한 문제**:

- 공급업체 중단
- 버그
- 체계적 오류
- 보안 문제
- 예상치 못한 확장성 한계

**완화 전략**:

- 사전 계획으로 사용자에게 피해 방지 또는 완화 가능
- Google의 출시 기록에서 사용한 방법:
  - 필터링 및/또는 재작성 프록시
  - 데이터 트랜스코딩 파이프라인
  - 캐시

**체크리스트 질문 예시**:

- 서비스 또는 출시가 어떤 제3자 코드, 데이터, 서비스 또는 이벤트에 의존합니까?
- 귀하의 서비스에 의존하는 파트너가 있습니까? 그렇다면 출시를 알려야 합니까?
- 귀하 또는 공급업체가 엄격한 출시 마감일을 맞출 수 없으면 어떻게 됩니까?

#### 9. 롤아웃 계획 (Rollout Planning)

**대규모 분산 시스템의 특성**:

- 대규모 분산 시스템에서는 즉각적으로 발생하는 이벤트가 거의 없음
- 신뢰성 이유로 이러한 즉각성은 일반적으로 이상적이지도 않음

**복잡한 출시의 요구사항**:

- 여러 하위 시스템에서 개별 기능을 활성화해야 할 수 있음
- 각 구성 변경이 완료되는 데 몇 시간이 걸릴 수 있음
- 테스트 인스턴스에서 작동하는 구성이 라이브 인스턴스에 롤아웃될 수 있다고 보장하지 않음
- 때때로 모든 구성 요소가 깨끗하게 올바른 순서로 시작되도록 복잡한 과정이나 특수 기능 필요

**외부 요구사항의 복잡성**:

- 마케팅 및 PR과 같은 팀의 외부 요구사항이 추가 복잡성을 더할 수 있음
- 예: 컨퍼런스의 기조연설을 위해 제때 기능을 사용할 수 있어야 하지만 기조연설 전에는 기능을 보이지 않게 유지해야 함

**비상 조치**:

- 롤아웃 계획의 또 다른 부분
- 기조연설을 위해 제때 기능을 활성화하지 못하면?
- 때때로 이러한 비상 조치는 백업 슬라이드 덱 준비만큼 간단:
  - "우리가 이 기능을 출시했습니다" 대신 "향후 며칠 내에 이 기능을 출시할 예정입니다"

**조치 항목 예시**:

- 서비스를 출시하기 위해 취할 조치를 식별하는 출시 계획 설정. 각 항목에 대한 책임자 식별
- 개별 출시 단계의 위험 식별 및 비상 조치 구현

## 신뢰할 수 있는 출시를 위한 선택된 기법

### 점진적 및 단계적 롤아웃 (Gradual and Staged Rollouts)

#### 시스템 관리의 격언

- "실행 중인 시스템을 절대 변경하지 마라"
- 모든 변경은 위험을 나타냄
- 시스템의 신뢰성을 보장하기 위해 위험 최소화 필요
- 작은 시스템에 적용되는 것은 Google이 운영하는 것과 같은 고도로 복제되고 전 세계적으로 분산된 시스템에 대해 두 배로 사실

#### Google의 출시 패턴

- Google에서 매우 적은 수의 출시가 "푸시 버튼" 방식
  - 특정 시간에 전 세계가 사용할 수 있도록 새 제품 출시
- 시간이 지남에 따라 Google은 제품 및 기능을 점진적으로 출시하여 위험을 최소화할 수 있는 여러 패턴 개발

#### 점진적 롤아웃 프로세스

**단계별 진행**:

1. 하나의 데이터센터의 몇 대 머신에 새 서버 설치
2. 정의된 기간 동안 관찰
3. 모든 것이 잘되면 하나의 데이터센터의 모든 머신에 설치
4. 다시 관찰
5. 전 세계 모든 머신에 설치

**카나리 (Canaries)**:

- 롤아웃의 첫 단계를 "카나리"라고 함
- 광부가 위험한 가스를 감지하기 위해 탄광으로 가져간 카나리에 대한 비유
- 카나리 서버는 실제 사용자 트래픽 하에서 새 소프트웨어 동작으로 인한 위험한 영향 감지

#### 자동화된 도구의 카나리 테스트

- 자동 변경을 수행하는 데 사용되는 Google의 많은 내부 도구에 카나리 테스트 개념이 내장됨
- 구성 파일을 변경하는 시스템에도 적용
- 새 소프트웨어 설치를 관리하는 도구:
  - 새로 시작된 서버를 잠시 관찰
  - 서버가 충돌하거나 오작동하지 않는지 확인
  - 변경이 검증 기간을 통과하지 못하면 자동으로 롤백

#### Google 서버에서 실행되지 않는 소프트웨어

- 점진적 롤아웃 개념은 Google의 서버에서 실행되지 않는 소프트웨어에도 적용
- **Android 앱 예시**:
  - 새 버전을 점진적 방식으로 롤아웃 가능
  - 업데이트된 버전이 업그레이드를 위해 설치의 하위 집합에 제공됨
  - 업그레이드된 인스턴스의 비율이 시간이 지남에 따라 점진적으로 증가하여 100%에 도달
  - 새 버전이 Google의 데이터센터의 백엔드 서버에 추가 트래픽을 초래하는 경우 특히 유용
  - 새 버전을 점진적으로 롤아웃하면서 서버의 영향을 관찰하고 조기에 문제 감지 가능

#### 초대 시스템 (Invite System)

- 또 다른 유형의 점진적 롤아웃
- 종종 새 서비스에 무료 가입을 허용하는 대신 하루에 제한된 수의 사용자만 가입 허용
- 속도 제한 가입은 종종 초대 시스템과 결합:
  - 사용자가 친구에게 제한된 수의 초대 보낼 수 있음

### 기능 플래그 프레임워크 (Feature Flag Frameworks)

#### 출시 전 테스트 보완

- Google은 종종 중단 위험을 완화하는 전략으로 출시 전 테스트 보완
- 변경 사항을 천천히 롤아웃하여 실제 워크로드 하에서 전체 시스템 동작을 관찰할 수 있는 메커니즘:
  - 신뢰성, 엔지니어링 속도 및 시장 출시 시간에 대한 엔지니어링 투자에 대한 대가 지불 가능
- 이러한 메커니즘은 현실적인 테스트 환경이 비실용적인 경우 또는 영향을 예측하기 어려운 특히 복잡한 출시에 특히 유용한 것으로 입증

#### 모든 변경이 동등하지 않음

- 때때로 사용자 인터페이스에 대한 작은 조정이 사용자 경험을 개선하는지 확인하고 싶을 뿐
- 이러한 작은 변경에는 수천 줄의 코드나 무거운 출시 프로세스가 포함되어서는 안 됨
- 이러한 변경을 병렬로 수백 개 테스트하고 싶을 수 있음

#### 프로토타입 테스트

- 때때로 소수의 사용자가 구현하기 어려운 새 기능의 초기 프로토타입 사용을 좋아하는지 알고 싶음
- 기능이 실패작임을 알게 되기 위해서만 수백만 사용자에게 제공하도록 새 기능을 강화하는 데 몇 달의 엔지니어링 노력을 투자하고 싶지 않음

#### 기능 플래그 프레임워크 설계

여러 Google 제품이 기능 플래그 프레임워크를 고안:

- 일부 프레임워크는 0%에서 100% 사용자로 새 기능을 점진적으로 롤아웃하도록 설계
- 제품이 이러한 프레임워크를 도입할 때마다 프레임워크 자체를 가능한 한 강화
  - 대부분의 애플리케이션에 LCE 개입이 필요하지 않도록

#### 프레임워크 요구사항

이러한 프레임워크는 일반적으로 다음 요구사항 충족:

- 병렬로 많은 변경 사항 롤아웃, 각각 몇 개의 서버, 사용자, 엔티티 또는 데이터센터에
- 일반적으로 1~10% 사이의 더 크지만 제한된 사용자 그룹으로 점진적으로 증가
- 사용자, 세션, 객체 및/또는 위치에 따라 다른 서버를 통해 트래픽 지시
- 설계상 새 코드 경로의 실패를 사용자에게 영향을 주지 않고 자동 처리
- 심각한 버그 또는 부작용 발생 시 각 변경 사항을 즉시 독립적으로 되돌림
- 각 변경 사항이 사용자 경험을 개선하는 정도 측정

#### 기능 플래그 프레임워크의 두 가지 일반 클래스

**1. 사용자 인터페이스 개선을 주로 촉진하는 것**

**2. 임의의 서버 측 및 비즈니스 논리 변경을 지원하는 것**

#### UI 변경을 위한 가장 간단한 프레임워크

- 상태 비저장 서비스의 사용자 인터페이스 변경을 위한 가장 간단한 기능 플래그 프레임워크:
  - 프론트엔드 애플리케이션 서버의 HTTP 페이로드 재작성기
  - 쿠키 또는 유사한 HTTP 요청/응답 속성의 하위 집합으로 제한
- 구성 메커니즘:
  - 새 코드 경로와 연관된 식별자 지정 가능
  - 변경 범위 (예: 쿠키 해시 모드 범위), 화이트리스트 및 블랙리스트

#### 상태 저장 서비스

- 상태 저장 서비스는 기능 플래그를 다음으로 제한하는 경향:
  - 고유한 로그인 사용자 식별자의 하위 집합
  - 문서, 스프레드시트 또는 스토리지 객체의 ID와 같이 액세스되는 실제 제품 엔티티
- HTTP 페이로드를 재작성하는 대신:
  - 변경에 따라 다른 서버로 요청을 프록시 또는 재라우팅할 가능성이 높음
  - 개선된 비즈니스 논리 및 더 복잡한 새 기능을 테스트할 수 있는 능력 부여

### 남용 클라이언트 동작 처리 (Dealing with Abusive Client Behavior)

#### 업데이트 속도의 잘못된 판단

- 남용 클라이언트 동작의 가장 간단한 예: 업데이트 속도의 잘못된 판단
- 600초마다가 아닌 60초마다 동기화하는 새 클라이언트는 서비스에 10배의 부하를 일으킴

#### 재시도 동작의 함정

- 재시도 동작에는 사용자가 시작한 요청과 클라이언트가 시작한 요청 모두에 영향을 미치는 여러 함정이 있음
- **과부하 서비스 예시**:
  - 과부하되어 일부 요청이 실패하는 서비스
  - 클라이언트가 실패한 요청을 재시도하면 이미 과부하된 서비스에 부하 추가
  - 결과: 더 많은 재시도 및 더 많은 요청

#### 올바른 재시도 전략

클라이언트가 해야 할 것:

- 재시도 빈도를 줄여야 함
- 일반적으로 재시도 사이에 지수적으로 증가하는 지연 추가
- 재시도를 보증하는 오류 유형을 신중하게 고려
  - 네트워크 오류: 일반적으로 재시도 보증
  - 4xx HTTP 오류 (클라이언트 측 오류 표시): 일반적으로 재시도하지 않음

#### Thundering Herd (집중 폭주)

- 의도적이거나 우발적인 자동 요청의 동기화
- 남용 클라이언트 동작의 또 다른 일반적인 예
- **휴대폰 앱 개발자 예시**:
  - 사용자가 잠들어 있을 가능성이 가장 높기 때문에 오전 2시를 업데이트 다운로드에 좋은 시간으로 결정
  - 그러한 설계는 매일 밤 오전 2시에 다운로드 서버에 요청 집중을 초래
  - 다른 시간에는 거의 요청이 없음
- **올바른 접근**: 모든 클라이언트가 이러한 유형의 요청 시간을 무작위로 선택해야 함

#### 무작위성의 필요성

- 무작위성은 다른 주기적 프로세스에도 주입되어야 함
- **재시도 예시 재검토**:
  - 요청을 보내고 실패를 만났을 때 1초 후, 그다음 2초 후, 그다음 4초 후 등으로 재시도하는 클라이언트
  - 무작위성이 없으면 증가된 오류율로 이어지는 짧은 요청 스파이크가 1초 후, 2초 후, 4초 후 재시도로 인해 반복될 수 있음
- **지터링 (Jittering)**: 이러한 동기화된 이벤트를 고르게 하기 위해 각 지연을 무작위 양만큼 조정해야 함

#### 서버 측에서 클라이언트 제어

- 서버 측에서 클라이언트의 동작을 제어할 수 있는 능력은 과거에 중요한 도구로 입증
- **장치의 앱에 대한 제어**:
  - 클라이언트가 주기적으로 서버와 체크인하고 구성 파일을 다운로드하도록 지시
  - 파일은 특정 기능을 활성화 또는 비활성화하거나 매개변수 설정 가능:
    - 클라이언트가 동기화하는 빈도
    - 재시도 빈도

#### 휴면 기능 (Dormant Functionality)

- 클라이언트 구성이 완전히 새로운 사용자 대면 기능을 활성화할 수도 있음
- **이점**:
  - 해당 기능을 활성화하기 전에 클라이언트 애플리케이션에서 새 기능을 지원하는 코드를 호스팅
  - 출시와 관련된 위험을 크게 줄임
  - 새 버전 릴리스가 훨씬 쉬워짐
    - 기능이 있는 버전과 없는 버전에 대해 병렬 릴리스 트랙을 유지할 필요 없음
  - 서로 다른 일정에 릴리스될 수 있는 독립적인 기능 세트를 다루는 경우 특히 사실
    - 다양한 버전의 조합 폭발 유지 필요성 제거

#### 출시 중단 용이성

- 이러한 종류의 휴면 기능은 롤아웃 중 부작용이 발견될 때 출시 중단을 더 쉽게 만듦
- 이러한 경우:
  - 기능을 끄고, 반복하고, 앱의 업데이트된 버전을 릴리스하기만 하면 됨
- 이러한 유형의 클라이언트 구성이 없으면:
  - 기능이 없는 앱의 새 버전을 제공해야 함
  - 모든 사용자의 휴대폰에서 앱을 업데이트해야 함

### 과부하 동작 및 부하 테스트 (Overload Behavior and Load Tests)

#### 과부하 상황의 복잡성

- 과부하 상황은 특히 복잡한 실패 모드
- 따라서 추가 주의가 필요

#### 과부하의 원인

**새 서비스 출시 시**:

- 폭주 성공은 일반적으로 가장 환영받는 과부하 원인
- 그러나 다음을 포함한 무수한 다른 원인 존재:
  - 로드 밸런싱 실패
  - 머신 중단
  - 동기화된 클라이언트 동작
  - 외부 공격

#### 순진한 모델

**가정**:

- 특정 서비스를 제공하는 머신의 CPU 사용량이 부하(예: 요청 수 또는 처리된 데이터 양)와 선형적으로 확장
- 사용 가능한 CPU가 소진되면 처리가 단순히 느려짐

**현실**:

- 서비스가 현실 세계에서 이러한 이상적인 방식으로 거의 동작하지 않음
- 많은 서비스는 로드되지 않을 때 훨씬 느림
  - 일반적으로 다양한 종류의 캐시 효과 때문:
    - CPU 캐시
    - JIT 캐시
    - 서비스별 데이터 캐시

#### 로드 증가에 따른 동작

**선형 구간**:

- 부하가 증가하면 일반적으로 CPU 사용량과 서비스 부하가 선형적으로 대응하는 구간 존재
- 응답 시간은 대부분 일정하게 유지

**비선형성**:

- 많은 서비스가 과부하에 접근하면서 비선형성 지점에 도달
- **가장 양성인 경우**: 응답 시간이 단순히 증가하기 시작
  - 사용자 경험은 저하되지만 반드시 중단을 일으키지는 않음
  - 느린 종속성이 초과된 RPC 마감 시간으로 인해 스택 위쪽에서 사용자에게 보이는 오류를 일으킬 수 있음
- **가장 극단적인 경우**: 서비스가 과부하에 대응하여 완전히 잠김

#### 과부하 동작의 구체적 예시

**디버깅 정보 로깅 서비스**:

- 백엔드 오류에 대응하여 디버깅 정보를 로깅
- 디버깅 정보 로깅이 정상적인 경우 백엔드 응답 처리보다 비용이 많이 듦
- **결과**:
  - 서비스가 과부하되어 자체 RPC 스택 내에서 백엔드 응답을 시간 초과하면서
  - 서비스가 이러한 응답을 로깅하는 데 더 많은 CPU 시간 소비
  - 그동안 더 많은 요청 시간 초과
  - 서비스가 완전히 멈출 때까지

**GC Thrashing (가비지 컬렉션 스래싱)**:

- Java Virtual Machine (JVM)에서 실행되는 서비스에서 멈추는 것과 유사한 효과
- 이 시나리오에서:
  - 가상 머신의 내부 메모리 관리가 점점 더 가까운 주기로 실행
  - 메모리를 확보하려고 시도
  - 대부분의 CPU 시간이 메모리 관리에 의해 소비될 때까지

#### 부하 테스트의 필요성

- 불행히도 제1원칙으로부터 서비스가 과부하에 어떻게 반응할지 예측하기 매우 어려움
- **따라서 부하 테스트는 귀중한 도구**:
  - 신뢰성 이유
  - 용량 계획
- 부하 테스트는 대부분의 출시에 필요

## LCE의 발전

### Google 초기 (2002-2004년)

#### 배경

- Google의 형성기에 엔지니어링 팀 규모가 여러 해 동안 매년 두 배로 증가
- 엔지니어링 부서가 많은 실험적인 새 제품 및 기능을 작업하는 많은 소규모 팀으로 분열
- 이러한 환경에서 초보 엔지니어가 전임자의 실수를 반복할 위험
  - 특히 새 기능과 제품을 성공적으로 출시하는 것과 관련하여

#### Launch Engineers 팀 탄생 (자원봉사)

- 이러한 실수의 반복을 완화하기 위해 과거 출시에서 배운 교훈을 포착
- "Launch Engineers"라고 불리는 소수의 경험 많은 엔지니어가 컨설팅 팀 역할을 자원
- Launch Engineers가 개발한 새 제품 출시를 위한 체크리스트:
  - 법무 부서와 상담해야 하는 시기
  - 도메인 이름 선택 방법
  - DNS를 잘못 구성하지 않고 새 도메인을 등록하는 방법
  - 일반적인 엔지니어링 설계 및 프로덕션 배포 함정

#### Launch Reviews

- Launch Engineers의 컨설팅 세션을 "Launch Reviews"라고 부름
- 많은 새 제품 출시 며칠에서 몇 주 전에 일반적인 관행이 됨

### 2004년: LCE 전담 팀 구성

#### 복잡성 증가

**2년 이내**:

- 출시 체크리스트의 제품 배포 요구사항이 길고 복잡해짐
- Google의 배포 환경의 복잡성 증가와 결합
- 제품 엔지니어가 안전하게 변경하는 방법에 대해 최신 정보를 유지하는 것이 점점 더 어려워짐

**SRE 조직의 성장**:

- 동시에 SRE 조직이 빠르게 성장
- 경험이 없는 SRE가 때때로 지나치게 신중하고 변경을 꺼림
- Google은 이 두 당사자 간의 협상이 제품/기능 출시 속도를 줄일 위험 직면

#### LCE 팀의 공식화

**2004년 SRE가 소규모 전담 LCE 팀 구성**:

- 새 제품 및 기능 출시를 가속화하는 책임
- 동시에 SRE 전문 지식을 적용하여 Google이 높은 가용성과 낮은 지연 시간으로 신뢰할 수 있는 제품을 출시하도록 보장

**LCE의 책임**:

- 출시가 서비스가 넘어지지 않고 빠르게 실행되도록 보장
- 출시가 실패하면 다른 제품을 다운시키지 않도록 보장
- 시장 출시 시간을 가속화하기 위해 코너를 자를 때마다 이러한 실패의 성격과 가능성을 이해 관계자에게 알리는 책임
- 컨설팅 세션이 **Production Reviews**로 공식화됨

### LCE 체크리스트의 진화

#### 체크리스트 성장

- Google의 환경이 더 복잡해짐에 따라:
  - Launch Coordination Engineering 체크리스트 성장
  - 출시 볼륨 증가
- **3.5년 동안 한 LCE가 350개의 출시를 LCE 체크리스트를 통해 실행**
- 이 기간 동안 팀 평균 5명의 엔지니어
- **Google 출시 처리량: 3.5년 동안 1,500개 이상의 출시!**

#### 복잡성 관리

- LCE 체크리스트의 각 질문은 간단
- 그러나 질문을 유발한 것과 답변의 의미에 많은 복잡성이 내장됨
- **이 정도의 복잡성을 완전히 이해하기 위해 새 LCE 채용은 약 6개월의 교육 필요**

### 2008년: 프로세스 간소화

#### 저위험 출시 범주 식별

출시 볼륨이 증가함에 따라(Google의 엔지니어링 팀의 연간 두 배 증가와 보조):

- LCE가 리뷰를 간소화하는 방법 모색
- 사고 또는 사고를 일으킬 가능성이 매우 낮은 저위험 출시 범주 식별

**저위험 출시 예시**:

- 새 서버 실행 파일이 없는 기능 출시
- 10% 미만의 트래픽 증가
- 이러한 출시는 저위험으로 간주

**차등 프로세스**:

- 저위험 출시: 거의 사소한 체크리스트
- 고위험 출시: 전체 검사 및 균형 범위
- **2008년까지 리뷰의 30%가 저위험으로 간주**

#### 환경 확장의 이점

동시에 Google의 환경이 확장되어 많은 출시의 제약 제거:

- **YouTube 인수 사례**:

  - Google이 네트워크를 구축하고 대역폭을 더 효율적으로 활용하도록 강제
  - 많은 소규모 제품이 "틈새에 맞춤"을 의미
  - 복잡한 네트워크 용량 계획 및 프로비저닝 프로세스 회피
  - 출시 가속화

- **대규모 데이터센터 구축**:
  - 한 지붕 아래 여러 종속 서비스를 호스팅할 수 있음
  - 의존하는 여러 기존 서비스에서 대량의 용량이 필요한 새 제품 출시 단순화

## LCE가 해결하지 못한 문제

### 관료주의 최소화의 한계

- LCE가 리뷰의 관료주의를 최소한으로 유지하려고 노력했지만 그러한 노력은 불충분
- **2009년까지**: Google에서 소규모 새 서비스를 출시하는 어려움이 전설이 됨
- 더 큰 규모로 성장한 서비스는 Launch Coordination이 해결할 수 없는 자체 문제 세트에 직면

### 1. 확장성 변경 (Scalability Changes)

**성공의 역설**:

- 제품이 초기 추정치를 훨씬 넘어 성공할 때
- 사용량이 2자릿수 이상 증가할 때
- 부하를 따라잡기 위해 많은 설계 변경 필요

**결과**:

- 이러한 확장성 변경과 지속적인 기능 추가의 결합:
  - 제품을 더 복잡하고, 취약하고, 운영하기 어렵게 만듦
- 어느 시점에서 원래 제품 아키텍처가 관리할 수 없게 됨
- 제품을 완전히 재설계해야 함

**재설계의 비용**:

- 제품을 재설계한 다음 모든 사용자를 이전 아키텍처에서 새 아키텍처로 마이그레이션:
  - 개발자와 SRE 모두로부터 많은 시간과 리소스 투자 필요
  - 해당 기간 동안 새 기능 개발 속도 저하

### 2. 운영 부하 증가 (Growing Operational Load)

**운영 부하의 정의**:

- 시스템이 기능하도록 유지하는 데 필요한 수동 및 반복적인 엔지니어링 양

**시간 경과에 따른 증가**:

- 서비스를 출시한 후 실행할 때:
  - 부하를 제어하기 위한 노력을 기울이지 않는 한 운영 부하는 시간이 지남에 따라 증가하는 경향
  - 자동화된 알림의 노이즈
  - 배포 절차의 복잡성
  - 수동 유지보수 작업의 오버헤드
  - 시간이 지남에 따라 증가하고 서비스 소유자의 대역폭을 점점 더 많이 소비
  - 팀이 기능 개발에 사용할 시간 감소

**SRE의 목표**:

- SRE는 운영 작업을 최대 50% 이하로 유지하는 내부 광고 목표 보유
- 이 최대치 이하로 유지하려면:
  - 운영 작업 소스의 지속적인 추적 필요
  - 이러한 소스를 제거하기 위한 집중된 노력 필요

### 3. 인프라 변동 (Infrastructure Churn)

**문제**:

- 기본 인프라(클러스터 관리, 스토리지, 모니터링, 로드 밸런싱 및 데이터 전송을 위한 시스템)가 인프라 팀의 활발한 개발로 인해 변경되는 경우
- 인프라에서 실행되는 서비스 소유자:
  - 단순히 인프라 변경 사항을 따라잡기 위해 많은 양의 작업 투자 필요

**역호환성 문제**:

- 서비스가 의존하는 인프라 기능이 더 이상 사용되지 않고 새 기능으로 대체됨
- 서비스 소유자는 구성을 지속적으로 수정하고 실행 파일을 재빌드해야 함
- 결과적으로 "같은 장소에 머물기 위해 빠르게 달리기"

**해결책**:

- 일종의 변동 감소 정책 시행 필요
- 인프라 엔지니어가 역호환되지 않는 기능을 릴리스하는 것을 금지:
  - 클라이언트의 새 기능으로의 마이그레이션도 자동화할 때까지
- 새 기능과 함께 자동화된 마이그레이션 도구 생성:
  - 인프라 변동을 따라잡기 위해 서비스 소유자에게 부과되는 작업 최소화

### LCE 범위를 벗어난 문제

이러한 문제를 해결하려면 LCE의 범위를 훨씬 벗어난 회사 전체의 노력 필요:

- 더 나은 플랫폼 API 및 프레임워크의 조합
- 지속적인 빌드 및 테스트 자동화
- Google의 프로덕션 서비스 전반에 걸친 더 나은 표준화 및 자동화

## 결론

### LCE 팀이 적합한 회사

**빠른 성장과 높은 변화율을 겪는 회사**가 Launch Coordination Engineering 역할에 해당하는 것으로부터 이익을 얻을 수 있음:

**이러한 팀이 특히 가치 있는 경우**:

1. 회사가 1~2년마다 제품 개발자를 두 배로 늘릴 계획인 경우
2. 수억 명의 사용자로 서비스를 확장해야 하는 경우
3. 높은 변화율에도 불구하고 신뢰성이 사용자에게 중요한 경우

### Google의 해결책

- **LCE 팀은 변경을 방해하지 않고 안전을 달성하는 문제에 대한 Google의 해결책**
- 이 장에서는 정확히 그러한 상황에서 10년에 걸쳐 Google의 고유한 LCE 역할이 축적한 경험 중 일부를 소개

### 향후 전망

- Google의 접근 방식이 각자의 조직에서 유사한 과제에 직면한 다른 사람들에게 영감을 주는 데 도움이 되기를 희망

## 주요 교훈 요약

### 1. 출시 프로세스의 핵심 원칙

- **경량성과 철저함의 균형**: 개발자 부담을 최소화하면서 중요한 세부사항 포착
- **적응성**: 간단한 출시와 복잡한 출시 모두 수용
- **지속적 개선**: 프로세스를 계속 최적화하여 비용과 이점의 균형 유지

### 2. 체크리스트 관리

- **증거 기반**: 모든 질문은 실제 출시 재난으로 입증되어야 함
- **실용성**: 모든 지침은 구체적이고 개발자가 수행하기에 합리적이어야 함
- **지속적 큐레이션**: 정기적인 검토와 업데이트로 관련성 유지

### 3. 기술적 안전장치

- **점진적 롤아웃**: 카나리 배포를 통한 위험 최소화
- **기능 플래그**: 빠른 활성화/비활성화 및 A/B 테스트 지원
- **클라이언트 제어**: 서버 측에서 클라이언트 동작 관리
- **부하 테스트**: 과부하 동작을 사전에 이해

### 4. 조직적 접근

- **전담 팀**: 제품 간 경험을 축적하는 전문 LCE 팀
- **객관적 자문**: SRE, 개발, 제품 관리 간 중재자 역할
- **지식 전달**: 모범 사례와 공통 인프라 사용 촉진

### 5. 해결되지 않은 과제

- **확장성**: 급격한 성장 시 아키텍처 재설계 필요
- **운영 부하**: 지속적인 자동화와 최적화 필요
- **인프라 변동**: 자동화된 마이그레이션 도구 필요
