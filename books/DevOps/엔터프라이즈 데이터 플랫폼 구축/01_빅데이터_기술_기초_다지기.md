# 1장 빅데이터 기술 기초 다지기

아파치 하둡은 높은 확장성과 신뢰성을 보유한 분산 스토리지와 분산 처리 기능을 제공하기 위해 다양한 소프트웨어를 밀접하게 통합한 에코시스템이다.

하둡 에코시스템은 2000년대 구글이 발행한 혁신적인 논문에서 영감을 받아 만들어졌다.

논문에는 신뢰도가 잠재거으로 낮은 수백 또는 수천 개의 서버에서 신뢰성있는 스토리지를 만드는 방법(구글 파일 시스템)

데이터를 처리하는 방법 (맵리듀스, 프레겔)

랜덤 액세스 쿼리 응답 속도를 높이는 방법(빅테이블)이 포함돼 있다.

## 하둡 에코시스템 둘러보기

데이터 관점에서는 데이터 읽기와 데이터 쓰기를 중심으로 의존 관계가 구성되고,

제어 관점에서는 메타데이터나 협력 중심으로 의존 관계가 구성된다.

### 핵심 컴포넌트

하둡 에코시스템을 구성하는 기술 중에는 설계나 아키텍처, 용도 등이 비슷한 것들도 많다.

#### HDFS

HDFS(하둡 분산 파일 시스탬)은 확장성과 장애 허용성을 가진 분산 파일 시스템이다.

대규모 데이터셋 분석이라는 하둡의 원래 용도에 맞게, HDFS는 일반적으로 상당히 긴 순차 접근 방식을 통해 디스크에 불변 데이터를 저장하는데 최적화 돼있다.

HDFS는 하둡 스택 내 다른 컴포넌트를 지원하는 핵심 기술이다.

HDFS는 데이터를 설정 가능한 크기의 블록으로 나눠 저장한다.

블록 크기의 기본값은 128메가바이트이며, 데이터 회복성 및 데이터 병렬 처리를 위해 여러 대의 서버에 각 블록의 복제본을 저장한다.

클러스터에 있는 워커 노드는 데이터 노드 데몬을 실행하는데, 이 데몬을 통해 새 블록을 전달 받아서 로컬 디스크에 저장한다.

데이터노드는 데이터의 저장뿐만 아니라 데이터의 제공도 담당한다.

데이터노드는 블록과 블록의 ID만 알고 있으며, 파일이 어느 복제본에 속하는지 알지 못한다.

마스터 서버에서 실행되는 네임노드 프로세스가 파일이 어느 복제본에 속하는지에 대한 정보와 파일과 블록 사이의 매핑 정보, 파일 이름, 권한, 속성, 복제 계수등 파일 자체의 메타데이터를 모두 관리한다.

블록을 저장하려는 클라이언트는 먼저 네임노드를 통해 블록이 저장될 데이터노드의 목록을 받아야한다.

클라이언트는 첫 번째 데이터노드에 블록을 쓰고, 차례로 다음 데이터노드로, 또 그 다음 데이터노드로 파이프라인을 통해 데이터를 흘려 보낸다.

네임노드는 데이터노드의 가용 저장 공간, rack의 지역성을 고려한 물리적 위치 등 여러 요소를 고려해서 파이프라인을 구성하는 데이터노드를 결정한다.

네임노드는 노드 장애 또는 랙 장애에 대비해서 각 블록을 최소한 2개의 서로 다른 랙에 저장한다.

HDFS는 기본적으로 파일의 일부분 수정을 지원하지 않는다.

이와 같은 제약사항으로 인해 필요한 수평적 확장과 데이터 회복력을 비교적 단순한 방법으로 획득할 수 있다.

HDFS에서는 개별 디스크나 데이터노드 또는 심지어 랙에 장애가 발생하더라도데이터의 안정성이 유지되므로 장애 허용성이 있다.

데이터 노드에 장애가 발생하면 네임노드는 장애 데이터 노드에 저장된 데이터 블록 목록을 파악한다.

그래서 클러스터 자체에 충분한 용량과 여분의 노드를 보유하기만 하면, 시스템 일부에서 장애가 밸상히ㅏ더라도 앞에서 설명한 과정을 통해 자체 치유가 가능하다.

#### 얀

데이터를 확장성 있고 회복성 있는 방법도 유용하지만 데이터를 연산하는 것이다.

얀은 각 워커 노드에 노드매니저 데몬을 실행시키는데, 이 데몬이 마스터 프로세스인 리소스매니저에 여러 가지 정보를 보고한다.

노드매니저는 가상 코어의 단위로 얼마나 많은 연산 자원을 사용할 수 있는지와, 해당 노드에 메모리가 얼마나 남아 있는지를 리소스매니저에게 알려준다.

가용 자원은 클러스터 안에서 실행되는 애플리케이션에 컨테이너의 형태로 분할되어 제공되는데, 예를 들어 4개의 가상 코어와 8GB RAM을 가진 컨테이너가 10개 필요하다와 같은 형식으로 필요자원을 정의할 수 있다.

노드 매니저는 로컬 노드에 있는 컨테이너를 기동하고 모니터링하며, 할당 자원을 초과하는 컨테이너는 중단시킨다.

클러스터에서 연산을 수행하고자 하는 애플리케이션은 리소스매니저에게 컨테이너를 1개 요청한다.

이 컨테이너에서는 애플리케이션 조율을 담당하는 애플리케이션마스터 프로세스가 구동된다.

이름과 달리 애플리케이션 마스터 노드는 하나의 워커 노드에서 실행된다.

애플리케이션은 애플리케이션마다 하나씩 각기 다른 워커 노드에서 실행된다.

그래서 하나의 워커 노드에 장애가 발생하면 그 워커 노드에서 실행 중인 애플리케이션마스터에 연동된 애플리케이션만 영향을 받는다.

애플리케이션 마스터는 실제 연산에 필요한 컨테이너를 리소스매니저에게 요청한다.

리소스매니저는 특별한 스레드를 하나 실행한다.

이 스레드는 애플리케이션 요청을 스케줄링하고, 클러스터에서 애플리케이션을 실행하는 사용자와 애플리케이션 사이에 동일한 크기의 컨테이너가 할당되도록 보장한다.

이 스케줄러는 코어와 메모리를 여러 테넌트에 걸쳐 공평하게 배분하기 위해 노력한다.

테넌트와 워크로드는 계층 구조를 가진 풀로 나뉘고, 각 풀은 전체 클러스터 자원읠 일부를 보유하며, 그 비율은 설정 가능하다.

얀은 데이터를 직접 다루는 연산을 수행하지 않고, 클러스터에 분산되어 연산을 수행하는 애플리케이션을 실행하는 프레임워크에 가깝다.

#### 아파치 주키퍼

아파치 주키퍼는 하둡 에코시스템에 사용되는 회복성 있는 분산 설정 서비스다.

회복성을 확보하기 위해 주키퍼 인스턴스는 앙상블이라 불리는 각기 서로 다른 서버에 배포돼야 한다.

주키퍼는 다수결 원칙에 따라 합의를 이끌어내므로 홀수의 서버에 배포된다.

이때 과반수의 서버로 이루어진 그룹을 쿼럼이라고 한다.

그중에서 하나의 서버만 리더 노드로 선출되고, 나머지는 팔로워로 지정된다.

주키퍼는 다수결에 의해 앙상블에서 데이터의 수정이 수행됨을 보장한다.

#### 아파치 하이브 메타스토어

아파치 하이브의 쿼리 기능은 다음에 이어질 분석용 SQL 엔진에서 다루겠지만, 하이브 메타스토어는 하둡 스택의 다른 컴포넌트들이 의존하는 주요 기술이므로 먼저 알아볼 필요가 있다.

하이브 메타스토어는 하둡 내에 존재하는 비정형 바이너리 데이터가 아닌 정형 데이터셋에 대한 정보를 데이터셋, 테이블, 뷰같은 논리적인 위계 구조로 구성해서 관리한다.

하이브 테이블은 관계형 데이터베이스에서 사용되던 거의 모든 데이터 타입을 지원한다.

스토리지 엔진에 저장된 데이터는 테이블 스키마를 따르지만 HDFS 관점에서보면 이 스키마 요건은 런타임에만 검사되며, 이를 **스키마 온 리드**라고 부른다.

메타스토어는 관리형 테이블과 외부 테이블을 모두 지원한다.

##### 더 알아보기

하둡 에코시스템의 핵심을 다루는 좋은 책들이 많은데 그 중에서 다음 3권을 추천한다.

- 하둡 완벽 가이드
- 주키퍼
- 하이브 완벽 가이드

### 연산 프레임워크

데이터를 HDFS에 저장하고 얀을 통해 분산 애플리케이션을 실행하는데 하둡의 핵심 컴포넌트가 사용된다.

대다수 프레임워크는 사용자가 정의하거나 합성한 임의의 연산을 잘게 나눠서 분산실행한다.

이 중 대표적인 두 가지 프레임워크를 알아보자.

#### 하둡 맵리듀스

하둡은 원래 맵리듀스를 위해 만들어졌다.

맵리듀스는 구글 맵리듀스 논문 상의 설계를 자바로 구현한 애플리케이션이다.

처음에는 클러스터에서 실행되는 독립형 프레임워크였는데 하둡 프로젝트가 점점 더 많은 애플리케이션과 다양한 용도에 맞게 진화함에 따라 얀에서 실행되는 애플리케이션으로 바뀌었다.

최근에는 아파치 스파크와 아파치 플링크같은 새로운 엔진이 많이 사용되고 있지만, 아파치 하이브, 아파치 스쿱, 아파치 우지, 아파치 피그를 비롯한 많은 고수준 프레임워크들도 맵리듀스 작업으로 컴파일해서 연산을 실행하기 때문에 맵리듀스는 지금도 알아둘 필요가 있다.

> 맵과 리듀스는 함수형 프로그래밍에서 차용한 용어다. 맵은 컬렉션에 있는 모든 원손에 적용되는 변환 함수며, 리듀스는 리스트의 각 원소에 집계 함수를 적용해서 원래 원소의 개수보다 더 작은 개수의 결과를 산출한다.

맵리듀스는 연산을 맵, 셔플, 리듀스 이렇게 3단계로 나눠서 처리한다.

맵 단계에서는 HDFS에서 읽은 데이터가 다수의 독립적인 맵 Task로 나뉘어서 병렬로 처리된다.

맵 태스크는 이상적으로는 데이터가 위치한 곳에서 실행되며, 일반적으로 HDFS 블록 하나당 하나의 맵 태스크가 배정되는 것이 이상적이다.

사용자는 소스 코드에 map 함수를 정의하는데 이 함수는 파일에 있는 각 레코드를 키-값 형태로 변환한 결과를 사용한다.

셔플 단계에서는 맵 단계의 결과물인 키-값이 맵리듀스에 의해 읽혀서 네트워크를 타고 리듀스 태스크의 입력값으로 전달된다.

사용자가 정의한 reduce()함수는 하나의 키에 대한 여러 값을 집계 또는 결합해서 입력값의 개수보다 더 작은 개수의 결과값을 산출한다.

매퍼가 HDFS에 있는 파일을 레코드 단위로 읽고, 레코드는 ID 칼럼값을 키로 해서 셔플되고, 리듀스가 키 기준으로 레코드의 나머지 내용을 집계해서 결과ㅣ를 다시 HDFS에 저장한다.

이런 단순한 3단계 절차는 조인, 집계 등 임의의 복잡도를 가진 연산으로 합성되거나 결합될 수 있다.

반드시 3단계를 모두 거쳐야하는 것은 아니다.

집계를 필요로하지 않는 단순한 변환에서는 리듀스 단계가 전혀 필요하지 않다.

맵리듀스 작업의 결과는 보통 다른 작업의 입력값으로 사용될 수 있도록 HDFS에 다시 저장된다.

이런 단순함에도 불구하, 맵리듀스는 믿기 어려울 만큼 강력하고, 극도로 견고하며 확장성이 높지만 단점은 사용성과 있다.

자바로 map() 함수와 reduce()함수를 작성하고 컴파일하는 일은 맵리듀스로 복잡한 파이프라인을 조합하는 많은 분석가들에게는 매우 버거운 일이다.

두 번째 단점은 맵리듀스 자체가 특출나게 효율적이지는 않다는 점이다.

맵리듀스는 많은 양의 디스크 기반 I/O를 수행하는데, 복잡한 처리 단계를 결합하거나 반복적인 연산을 할 때는 I/O 비용이 매우 많이 든다.

다중 단계 파이프라인은 HDFS I/O를 수반하는 개별 맵리듀스 작업을 조합해서 구성되는데, 처리 과정에서 찾아낼 수 있는 잠재적인 최적화가 전혀 고려되지 않는다.

이런 단점 때문에, 맵리듀스의 후속작으로 나온 프레임워크는 개발 과정 단순화와 파이프라인 효율화를 목표로 개발되었다.

하지만 서로 다른 장비에서 독립적으로 실행되는 다수의 태스크로 나눠 실행하는 맵, 그리고 맵의 결과를 셔플하고 그룹지어서 서로 다른 장비에서 집계하는 리듀스를 기반으로 데이터를 처리하는 맵리듀스의 개념적 토대는 SQL 기반 프레임워크를 비롯해 모든 분산 데이터 프로세싱 엔진의 근간을 이룬다.

#### 아파치 스파크

아파치 스파크는 효율성과 사용성에 중점을 둔 분산 연산 프레임워크로서, 배치 연산과 스트리밍 연산을 모두 지원한다.

사용자가 데이터 조작 과정에서 map()과 reduce() 함수를 이용해서 표현해야하는 맵리듀스와 달리 스파크는 필터링, 조인, 그룹핑, 집계같은 일반적인 연산을 특정 타입이나 스키마에 부합하는 데이터 셋에 직접 적용할 수 있는 풍부한 API를 제공한다.

이 API를 이용하면 복잡한 프로세싱 파이프라인도 쉽게 구성할 수 있다.

API뿐 아니라 SQL 스타일의 문법을 사용할 수 있는 스파크 SQL을 사용할 수도 있는데 스파크 SQL을 사용하면 파이프라인을 절차적인 프로그래밍 방식이 아니라 선언적인 방법으로도 구성할 수 있다.

이러한 데이터 입출력셋은 HDFS나 쿠두를 사용해 배치 방식으로 수행될 수 있고 카프카를 통해 스트림 방식으로 처리될 수도 있다.

스파크 데이터셋 연산의 주요 특징은 프로세싱 그래프가 표준 쿼리 옵티마이저를 거친 후에 실행된다는 점이다.

이는 관계형 데이터베이스 혹은 대규모 병렬 쿼리 엔진의 쿼리 옵티마이저와 상당히 비슷하다.

프로세싱 그래프를 재배치, 결합, 가지치기 할 수 있다.

이를 통해 데이터셋 연산의 효율을 큰 폭으로 높일 수 있으며, 맵 리듀스의 문제점 중 하나였던 중간 과정의 과도한 I/O 발생도 피할 수 있다.

스파크 설계의 주요 목표 중의 하나는 다수의 저사양 장비로 이루어지는 워커 노드에 있는 메모리를 최대한 활용하는 것이었다.

메인 메모리에서 데이터를 읽고 쓰는 속도는 디스크의 데이터를 읽고 쓰는 것과는 비교할 수 없을 정도로 빠른데, 이런 속도 차이를 통해 특정 유형의 작업 효율을 크게 높일 수 있다.

예를 들어 머신러닝 작업은 맵 리듀스로 실행할 때보다 스파크로 실행할 때 훨씬 큰 성능 이득을 얻을 수 있다.

스파크는 데이터셋에 대한 마이크로배치를 주기적으로 실행하는 방식으로 스트림 프로세싱을 구현했다.

그러나 마이크로배치 방식의 단점은 배치 사이에 대기 시간이 발생한다는 것이다.

따라서 밀리초 이하의 지연시간을 필요로 하는 작업에는 적합하지 않다.

##### 더 알아보기

- 하이 퍼포먼스 스파크
- 스파크 완벽 가이드
- 스파크 프로젝트 공식 문서

### 분석용 SQL 엔진

맵 리듀스와 스파크가 아주 유연하고 강력한 프레임워크이긴 하지만, 이를 사용하려면 자바나 스칼라, 파이썬 언어에 능숙해야 하고 터미널에서 코드를 배포하고 운영하는 데도 익숙해야 한다.

하지만 대부분의 기업에서는 여전하ㅣ SQL이 사실상 표준 분석 언어로 사용되는 것이 현실이고, 대부분의 분석 기법도 SQL을 기반으로 한다.

때로는 애플리케이션 코딩, 컴파일, 배포, 운영같은 복잡한 절차없이 분석해야 할 수도 있다.

이런 기술의 대부분은 내부적으로 맵리듀스나 스파크를 근간으로 하지만 독자적인 연산 엔진을 갖춘 것도 있다.

각 엔진은 스토리지 엔진에 이미 존재하는 데이터를 쿼리하거나 대량 데이터를 저장하는 데 중점을 두며, 소규모의 트랜잭션 처리가 아니라 대규모의 분석에 특화되어 있다.

#### 아파치 하이브

- 하둡에 사용되는 원조 데이터 웨어하우징 기술
- 사용자는 쿼리 서버와 세션을 먼저 맺고 하이브QL 문법에 맞는 쿼리를 쿼리 서버에 보냄
- 이는 다중 단계 컴파일이 되는데 중간 데이터는 HDFS 상의 임시 위치에 저장됨 하이브서버2는 다중 동시 접속을 지원하며, 주키퍼의 공유 락 또는 독점 락을 통해 일관성을 보장함

#### 아파치 임팔라

- 대규모 병렬 처리 엔진
- 하둡이나 클라우드 스토리지에 저장된 대용량 데이터셋에 대한 고속, 대화형 SQL 쿼리를 목적으로 설계되었음
- 테라바이트 단위의 데이터에 대해 다수의 동시 실행 애드혹 쿼리, 보고서 스타일의 쿼리를 수초 내에 처리하는 것이 주요 설계 목표
- 이를 통해 직접 SQL 쿼리를 작성하거나 비즈니스 인텔리전스 도구의 UI를 통해 쿼리를 실행하는 분석가의 업무를 지원할 수 있음
- 속도와 효율에 초점을 맞추므로 실행 모델이 다름

### 스토리지 엔진

하둡의 HDFS는 순차 스캔으로 접근되며 삭제는 안되고 추가만 가능한 대규모 데이터를 저장하는 데 아주 탁월하다.

하지만 임의의 레코드 조회나 수정 등의 접근 방식에서는 어떨까 ?

문서 검색에서도 뛰어날까? 대다수 작업에서 대규모의 다양한 데이터셋을 다루지만, 본질적으로 분석 작업은 아니다.

그래서 분석 용도에 사용할 수 있는 몇 가지 프로젝트가 새로 개발됐으며, 이미 존재하던 프로젝트의 하둡 버전이 나오기도 했다.

#### 아파치 HBase

HBase는 임의 접근 가능한 키-값 구조의 반정형 데이터를 HDFS에 저장한다.

대부분의 하둡 프로젝트와 마찬가지로, HBase 프레임워크의 초기 설계도 구글의 논문인 빅테이블을 기초로 만들어졌디.

HBase는 관계형 데이터 스토어가 아니며, 분산 테이블에 셀이라 불리는 키-값 쌍의 반정형 데이터 형식으로 데이터를 저장한다.

HBase는 셀 키를 위계 구조를 가지도록 세분화해서, 관련된 셀은 함께 효율적으로 접근할 수 있다.

키의 첫 번째 부분은 로우 키라 하는데, 셀의 논리적인 그룹인 **로우**를 정의하는 데 사용된다.

키의 나머지 부분은 **컬럼 패밀리**로 세분화되는데.

디스크에 분리되어 저장되고, 테이블 하나에 컬럼 패밀리는 일반적으로 몇 개밖에 저장되지 않는다. 키 스키마 중에서 컬럼 패밀리는 테이블이 생성될 때 정의돼야 한다.

컬럼 패밀리는 컬럼 한정자로 세분화되는데, 하나의 로우에 수백만 개의 컬럼 한정자가 있을 수 있다.

마지막으로 각 셀은 버전을 나타내는 타임스탬프를 가지고 있다.

동일한 키를 갖지만 각기 다른 타임ㅅ탬프를 가진 여러 셀은 서로 다른 버전으로 저장될 수 있다.

HBase는 타임스탬프 외에 키의 각 구성 요소와 값을 바이트 배열로 취급한다.

그래서 HBase는 셀의 어느 부분에 대해서도 타입 관련 정보를 모르며, 타입 관련 제약사항이 없으므로 결국 반정형 데이터 스토어라고 할 수 있다.

HBase에서는 셀이 키 구성요소에 따라 정렬되어 저장된다.

먼저 로우 키 기준으로 정렬된 컬럼 패밀리, 컬럼 한정자, 마지막으로 타임스탬프 기준으로 정렬된다.

HBase는 수평 파티셔닝을 사용한다.

그래서 테이블 안의 셀들은 파티션되어 클러스터에 분산 저장된다.

테이블의 로우 키를 위한 저장 공간도 **리전**이라는 파티션으로 나뉜다.

각 리전은 정렬된 로우 키를 중복 없이 나눠서 보유한다.

리전 사이의 경계는 **리전 스플릿**이라고 부른다.

예를 들어 로우가 임의의 알파벳 접두어를 가진다면 26개의 리전을 가진 테이블을 만들 수 있다.

a로 시작하는 키를 가진 로우는 첫 번째 리전에 저장도기ㅗ, c로 시작하는 로우 키를 가진 로우는 세 번째 리전에 저장된다.

리전은 나중에 수동으로 추가될 수 있으며, 로우 접근이 많은 리전은 HBase에 의해 자동으로 나뉠 수도 있다.

운영 관점에서 HBase의 학습 곡선은 누구에게나 꽤 가파를 수 있다.

테이블과 셀 키의 설계는 성능에 절대적인 영향을 미치므로 매우 중요하다. 그렇지 않으면 결국 테이블 풀 스캔, 리전 핫스팟, 잦은 압축 발생 같은 좋지 않은 결과로 이어진다.

HBase는 상대적으로 작은 범위 스캔을 거친 작은 그룹의 셀에 대해 적절하게 분산된 읽기/쓰기 요청을 처리하는 임의 접근 I/O 처리에 뛰어나다.

그래서 분석 작업에서 처리하는 수준의 대규모 데이터를 HBase로 스캔하면 실행 시간이 상당히 오래걸릴 수 있어서, 이런 작업은 HDFS 파일을 대상으로 수행하는 편이 낫다.

잘 관리하고 올바르게 사용한다면 HBase는 하둡 에코시스템에서 가장 쓸모있는 도구가 될 수 있ㅇ으며 거대한 데이터셋에 대해서도 놀랄만큼 빠른 성능을 보여줄 수 있다.

적합한 용도에 적합한 방식으로 사용하는 것이 매우 중요하다.

HBase외 에도 HDFS에서 사용되는 반정형 데이터 스토어는 다음과 같다.

- 아파치 카산드라
- 아파치 어큐뮬로

#### 아파치 쿠두

쿠두는 임의 접근과 순차 스캔을 모두 지원하고 기존 데이터 수정까지 허용하는 스토리지 엔진과 쿼리 엔진을 만들기 시작했다.

이를 위해서 어느 정도 성능상의 트레이드오프가 발생하기 마련이지만, 각 네이티브 기술의 성능 수준에 근접하는 것을 목표로 삼았다.

쿠두의 일반적인 용도는 다음과 같다.

- IoT 데이터셋 같은 대규모 시계열 메트릭
- 스타 스키마 테이블에 대한 OLAP 분석 같은 대규모 가변 데이터셋에 대한 리포팅
